{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Welcome to my portfolio! <p>My name is Joe Ganser and I am an award winning data technologist with a passion for solving deep &amp; complex problems. Coming from a strong background in mathematics and physics, I transformed my skill set from academia to technology business applications.   </p> Portfolio <p>Curriculum Vitae <p>My Employment history, Education &amp; Skills</p> <p>Research Projects <p>A collection of assorted Data Science projects I've done over the years, with code examples and github links.</p> <p>Accomplishments <p>Some of the awards &amp; accomplishments I've received in my data career.</p> <p>Teaching <p>My teaching history, with syllabi.</p> <p>Lecture Notes <p>Some lecture notes for my courses. Topics include supervised learning, unsupervised learning, big data computing, statistics, intro to python, et al.</p> <p>As a python &amp; SQL expert level coder, I am skilled in applied math, machine learning, physics and marketing. I have been involved in data science since 2017 and applied mathematics since 2009.</p>I look forward to networking with you! Feel free to connect on Linkedin, follow me on github or just shoot me an email.  I currently live in Hoboken, NJ. When I'm not coding or teaching, I'm probably at the gym, reading a book or doing meditation."},{"location":"awards_CV/","title":"Awards &amp; Accomplishments","text":""},{"location":"awards_CV/#battlefin-alternative-data-science-competition-may-2019","title":"Battlefin Alternative Data Science Competition: May 2019","text":"<p>1st Place winner in a Data Science competition</p> <p>Hosted by Battlefin and sponsored by fin tech firm M Science (divisions of the investment bank Jefferies), this open source data science competition attempted to answer the question: why was Tinder gold a viral product?. After competing against 100 other professional data technologists, my market research came in first place. I was awarded a cash prize  and invited to the 2019 Data Science discovery day at the Plaza Hotel in New York City.</p> <p>MScience &amp; Battlefin presenting the awards.</p> <p> </p> <p>My analysis that won me the award can be read here.</p>"},{"location":"awards_CV/#guest-lecturer-at-nyu-professor-janet-zhengs-data-analytics-course-may-2019","title":"Guest Lecturer at NYU: Professor Janet Zheng's Data Analytics course: May 2019","text":"<p>In response to the award I won for studying Tinder (above), I was invited by professor Janet Zheng of NYU to give a guest lecture at New York University on marketing analytics. This lecture was an explanation on;</p> <ul> <li>How tinder was engineered to be a viral (and addictive) product</li> <li>How data was wrangled &amp; modelled to draw such conclusions</li> <li>How marketing and data work together to influence the social dynamics of dating.</li> </ul> <p>Me presenting my research at New York University. Click the picture for the google slides presentation I showed the class.</p> <p> </p>"},{"location":"awards_CV/#accenture-school-of-ai-2019-healthcare-hackathon-april-2019","title":"Accenture &amp; School of AI 2019 Healthcare Hackathon: April 2019","text":"<p>Leading the 1st Place team in a Machine Learning Hackathon</p> <p>Hosted by Accenture and the School of AI, my team came in first place for the 2019 Health Care global hackathon. Competing against several hundred others, I lead my team to design a machine learing model to predict breathing rate using Smart watch data.</p> <p>My team &amp; I in our group photo</p> <p> </p> <p>The analysis that won first place for our team be read here read here.</p>"},{"location":"awards_CV/#sigma-alpha-pi-national-honor-society-fall-2016","title":"Sigma Alpha Pi National Honor Society: Fall 2016","text":"<p>The National Society of Leadership and success; Inducted during Fall 2016 for my academic performance in my Physics PhD program.</p> <p> </p>"},{"location":"conferences/","title":"Scientific Conferences I've been invited to","text":""},{"location":"conferences/#battlefin-data-science-discovery-day","title":"Battlefin Data Science Discovery Day NYC - Plaza Hotel - 2019      <p>In 2019 I was honored to be invited as a guest scientist to the 2019 Battlefin data science discovery day at the beautiful Plaza hotel in NYC. Battlefin hosts networking events where data buyers connect with data brokers, which included many people from influential companies such as Point 72, Two Sigma, investment banks, hedgefunds and other Wall Street Honchos. Great food, interesting people and lots of business cards. Check out some of the cool pictures I took;</p> <p>    </p>","text":""},{"location":"conferences/#columbia-university-data-science-day","title":"Columbia University Data Science Day NYC - Columbia U. Campus - 2019 <p>     Keeping in touch with your alumni network is really important for professional development. In 2019, Columbia had its first data science day which included talks by leaders at Microsoft, top research scientists and other interesting presentations. I was very impressed by some of the innovative ways people use data to study the natural world and build products. Some cool pictures;</p> <p>      </p>","text":""},{"location":"curriculum_vitae/","title":"Joe Ganser's CV","text":""},{"location":"curriculum_vitae/#employment","title":"Employment","text":"<p>TMCI International/U.S. Department of Defense - Data/Machine Learning EngineerRemote  - Jan. 2023 to Current</p> <ul> <li>Trained &amp; lead a team of five on the usage of unsupervised learning models (Splink/Fellegi Sunter) for the cleaning of multi-million row tabular datasets via distributed cloud computing</li> <li>Automated ETL pipelines of multi-million row datasets using PySpark, AWS EMR, MySQL &amp; Redshift </li> <li>Acquired and maintained a Tier 2.5 secret government security clearance for access to tabular data for military applications </li> <li>Research focused on the maintenance &amp; integrity of healthcare data related to military personnel</li> </ul> <p>Saint Peters University - Data Science/Engineering Adjunct Professor Jersey City, NJ  - Sept. 2022 to Current</p> <ul> <li>Lectured on Python, Spark, Machine Learning &amp; SQL before in person &amp; online audiences of 30+ graduate students</li> <li>Wrote and designed in person &amp; online courses on Big Data Computing, Spark and Hadoop, Databricks, Intro to python for data science.</li> <li>Provided mentorship and guidance to students changing their careers</li> <li>Courses taught:<ul> <li>Python for data science - DS542 - click for the syllabus</li> <li>Big Data computing with Spark &amp; Hadoop - DS610 - click for the syllabus</li> </ul> </li> </ul> <p></p> <p>Berkeley College - Data Science/Machine Learning Adjunct Professor Remote/Online - Aug. 2021 to Dec. 2022</p> <ul> <li>Lectured on Python, supervised Learning &amp; unsupervised learning to an online audience of 10+ senior undergraduates.</li> <li>Wrote and developed course educational content for beginner coders</li> <li>Courses taught: Advanced Programming for AI and Big Data - BDS4440 - click for the syllabus</li> </ul> <p></p> <p>IAC - Data Scientist/Engineer  New York, NY / Hybrid -  Sept. 2019 to May 2022</p> <ul> <li>Designed &amp; implemented cloud based cLTV automation tools used by finance team of 20+ people including executives &amp; stakeholders</li> <li>Lead a team five people to replace out dated financial/marketing forecasting models with cloud automated statistical models</li> <li>Built extract transform load (ETL) Python/SQL frameworks on cloud platforms connecting Google Cloud to Snowflake to Google Sheets </li> <li>Designed &amp; implemented Pyspark ALS collaborative filtering Recommendation Engines for app advertisements using implicit user behavior data </li> <li>Performed causality time series analysis to measure user subscription lift in response to in app advertising</li> <li>Worked with Python, SQL, Snowflake, Pyspark, DataBricks, Amazon Web Service, http/lambda functions, Sagemaker, Google Cloud</li> <li>Promoted to Data Scientist from Senior Data Analyst in August 2020</li> <li>Research focused on subscription user behaviors and the marketing of mobile web apps</li> </ul> <p>Flatiron School - Data Science Assistant Instructor New York, NY  - Sept. 2018 to Apr. 2019</p> <ul> <li>Lectured on intro to Python for data science to in person audience of 30+ adult students</li> <li>Wrote and developed course educational content for beginner Python/Machine learning coders</li> <li>Provided mentorship and guidance to students changing their</li> </ul> <p>Simulmedia - Data Scientist Intern New York, NY June 2018 to Sept. 2018</p> <ul> <li>Analyzed past television marketing campaign data to extract key features used to determine the types of individuals that converted on an advertisement versus the types that didn't</li> <li>Assisted in the design of a user based collaborative filtering recommendation system to find individuals who should be targeted for future advertisement campaigns\u2022 Worked with Python 3x, Amazon Web Service Redshift &amp; Postgres</li> <li>Data science applied to television marketing</li> </ul> <p>PebblePost - Data Scientist Intern New York, NY - Jan. 2018 to May 2018</p> <ul> <li>Used PySpark to automate data cleaning of millions of snail-mail postage addresses</li> <li>Created forecasting models to describe and predict customer conversion rates</li> <li>Worked with cloud based cluster computing to analyze multi-billion row data sets</li> <li>Used Python, PySpark, MySQL &amp; PostgreSQL to analyze big data hosted through Amazon web service</li> <li>Data science applied to mail based marketing</li> </ul> <p>General Assembly - Data Science Fellow New York, NY - June 2017 to Sept. 2017</p> <ul> <li>Coded in Python packages Numpy, Pandas, SciKit Learn, Seaborn, ARIMA, et. al to produce graphical visualizations, calculations and predictions for real world data problems</li> <li>Lead teams to solve data science problems and created presentations weekly</li> <li>Created a predictive model on the price of bitcoin using time series data modeled with ARIMA</li> <li>Solved problems in regression, prediction, classification and clustering</li> </ul> <p>Rutgers Newark - Graduate Physics Teacher's assistant  Newark, NJ - June 2015 to Sept 2015</p> <ul> <li>Teacher assistant for college Physics 1 &amp; 2 for STEM undergraduate classes</li> <li>Lectured on Physics during recitation sessions</li> </ul> <p>New Jersey Institute of Technology/Rutgers Newark - Research Assistant/PhD student in Physics  Newark,NJ - Aug. 2014 to June 2016</p> <ul> <li>Coded in Python and Wolfram Mathematica to create computer models of radiation fields</li> <li>Performed mathematical analysis on electromagnetic equations to calculate WiFi transmission during snow storms</li> <li>Lab work on Multi spectral imaging</li> </ul> <p>City University of New York - Adjunct Lecturer &amp; Physics Lab Technician  New York, NY - Jan. 2012 to Jan 2015</p> <ul> <li>Lecturered on Physics 1, Physics 2, Physics 1 lab and Physics 2 lab courses for undergraduate students</li> <li>Taught courses for both science and non-science majors</li> <li>Six semesters of teaching</li> <li>Taught Physics 110, Physics 111, Physics 210, Physics 211 and their corresponding lab sections</li> </ul> <p></p>"},{"location":"curriculum_vitae/#education","title":"Education","text":"<p>Columbia University 2011 - New York, NY</p> <ul> <li>Masters of Science - Applied Physics/Applied Mathematics<ul> <li>School of Engineering and Applied Science</li> <li>Masters Project - Designed LabVIEW control system for voltage control rod to be put into Nuclear Fusion machine</li> </ul> </li> </ul> <p>Pace University 2008 - Pleasantville, NY</p> <ul> <li>Bachelors of Science - Applied Mathematics/Physics<ul> <li>Dyson School of Arts &amp; Sciences</li> <li>Won the Whose who award in Colleges &amp; Universities for founding a campus science club</li> </ul> </li> </ul>"},{"location":"curriculum_vitae/#skills","title":"Skills","text":"<p>Some of the algorithms &amp; technoligies I've worked with</p> <ul> <li>Python - expert - coding daily since 2016</li> <li>SQL - expert - coding daily since 2016<ul> <li>Snowflake platform</li> <li>AWS Redshift platform</li> </ul> </li> <li>Web development - competent - html/javascript/css et al.<ul> <li>Made my first .com web page in 1999 (\"mechanicalthought.com\"). Dabbled with it since.</li> </ul> </li> <li>Machine Learning:<ul> <li>Supervised Learning:<ul> <li>Linear and Non linear models, ensemble learning</li> <li>Worked with everything from Orinary least squares to XGBoost</li> </ul> </li> <li>Unsupervised Learning:<ul> <li>Kmeans, Hierarchial clustering, Density based scanning, Gaussain Mixture Models, tSNE, PCA, SVD</li> </ul> </li> <li>Neural networks:<ul> <li>Computer Vision Applications</li> <li>Keras framework</li> <li>Transfer learning</li> </ul> </li> <li>Recommendation engines:<ul> <li>Alternating Least Squares, Cosine/Pearson Similarity</li> </ul> </li> </ul> </li> <li>Statistics:<ul> <li>A/B testing in frequentist &amp; Bayesian frameworks</li> <li>Causality inferencing in time series</li> <li>Hypothesis testing</li> </ul> </li> <li>Big Data Technologies:<ul> <li>Spark</li> <li>Map Reduce</li> <li>Databricks</li> </ul> </li> <li>Goole Cloud Platform &amp; Amazon Web Service: <ul> <li>lambda/http functions</li> <li>cloud functions</li> <li>load balancers</li> <li>buckets</li> <li>sagemaker/colab</li> </ul> </li> </ul>"},{"location":"teaching_CV/","title":"Teaching Experiences","text":""},{"location":"teaching_CV/#computer-science-courses","title":"Computer Science Courses","text":"<p>Saint Peters University - Data Science/Engineering Adjunct Professor Jersey City, NJ  - Sept. 2022 to Current</p> <ul> <li>Lectured on Python, Spark, Machine Learning &amp; SQL before in person &amp; online audiences of 30+ graduate students</li> <li>Wrote and designed in person &amp; online courses on Big Data Computing, Spark and Hadoop, Databricks, Intro to python for data science.</li> <li>Provided mentorship and guidance to students changing their careers</li> <li>Courses taught:<ul> <li>Python for data science: DS542 - click for the syllabus</li> <li>Big Data computing with Spark &amp; Hadoop: DS610 - click for the syllabus </li> </ul> </li> </ul> <p>Berkeley College - Data Science/Machine Learning Adjunct Professor Remote/Online - Aug. 2021 to Dec. 2022</p> <ul> <li>Lectured on Python, supervised Learning &amp; unsupervised learning to an online audience of 10+ senior undergraduates.</li> <li>Wrote and developed course educational content for beginner coders</li> <li>Course taught: Advanced Programming for AI and Big Data: BDS4440 - click for the syllabus</li> </ul> <p>Flatiron School - Data Science Assistant Instructor New York, NY  - Sept. 2018 to Apr. 2019</p> <ul> <li>Lectured on intro to Python for data science to in person audience of 30+ adult students</li> <li>Wrote and developed course educational content for beginner Python/Machine learning coders</li> <li>Provided mentorship and guidance to students changing their</li> </ul>"},{"location":"teaching_CV/#college-physics-courses","title":"College Physics Courses","text":"<p>Rutgers Newark - Graduate Physics Teacher's assistant Newark, NJ - June 2015 to Sept 2015</p> <ul> <li>Teacher assistant for college Physics 1 &amp; 2 for STEM undergraduate classes</li> <li>Lectured on Physics during recitation sessions</li> </ul> <p>City University of New York - Adjunct Lecturer New York, NY - Jan. 2012 to Jan 2015</p> <ul> <li>Lecturered on Physics 1, Physics 2, Physics 1 lab and Physics 2 lab courses for undergraduate students</li> <li>Taught courses for both science and non-science majors</li> <li>Six semesters of teaching</li> <li>Taught Physics 110, Physics 111, Physics 210, Physics 211 and their corresponding lab sections</li> </ul> <p></p> My first teaching experience, Spring 2012. I was lecturing on Keppler's laws in Physics 111, that I taught for City University of New York - BMCC campus."},{"location":"lectures/Intro_to_Python/dynamic_programming/dynamic_programming_notebook/","title":"Dynamic Programming","text":"<p>Github Notebook Link</p>"},{"location":"lectures/Intro_to_Python/dynamic_programming/dynamic_programming_notebook/#example-1-factorial-fibonacci-functions","title":"Example 1: Factorial &amp; Fibonacci functions","text":"<p>Evaluate the runtime for the factorial and fibonacci functions for n=50 using <code>%timeit</code></p> <pre><code>def Factorial(n):\nif n&lt;=1:\nreturn 1\nreturn n*Factorial(n-1)\ndef Fibonacci(n):\nif n in [0,1]:\nreturn n\nelse: \nreturn Fibonacci(n-1)+Fibonacci(n-2)\nn=20\n%time print(f'Factorial({n})={Factorial(n)}')\nprint('\\n')\n%time print(f'Fibonacci({n})={Fibonacci(n)}')\n</code></pre> <pre><code>Factorial(20)=2432902008176640000\nCPU times: user 283 \u00b5s, sys: 141 \u00b5s, total: 424 \u00b5s\nWall time: 364 \u00b5s\n\n\nFibonacci(20)=6765\nCPU times: user 3.2 ms, sys: 135 \u00b5s, total: 3.33 ms\nWall time: 3.85 ms\n</code></pre>"},{"location":"lectures/Intro_to_Python/dynamic_programming/dynamic_programming_notebook/#memoization-pseudo-code-top-down","title":"Memoization Pseudo code (top down);","text":"<ol> <li>Define an empty array the same size as the number of steps we intend to iterate</li> <li>Define a function that accepts the array and a discrete number <code>n</code></li> <li>specify the initial conditions, </li> <li>if the value of <code>n</code> meets the initial conditions, return them</li> <li>if theres a saved value for <code>array[n]</code>, return that value</li> <li>other wise, use the recursion relation with the function in step 2 to calculate the specific value, and save it to an array</li> </ol>"},{"location":"lectures/Intro_to_Python/dynamic_programming/dynamic_programming_notebook/#example-2-fibonacci-via-memoization","title":"Example 2: Fibonacci via memoization","text":"<p>Design the Fibonacci function with memoization (top down) in dynamic programming and evaluate the run time for <code>n=20</code>, compare it to the run time WITHOUT memoization</p> <pre><code>def fibonacci_memo(n):\narray = [None]*(n+1)\ndef fibonacci_top_down(n,array):\nif array[n] is not None:\nreturn array[n]\nif n in [1,2]:\nresult=1\nelse:\nresult=fibonacci_top_down(n-1,array)+fibonacci_top_down(n-2,array)\narray[n]=result\nreturn result\nreturn fibonacci_top_down(n,array)\nn=20\n%time print(f'fibonacci_memo({n})={fibonacci_memo(n)}')\nprint('\\n')\n%time print(f'fibonacci({n})={Fibonacci(n)}')\n</code></pre> <pre><code>fibonacci_memo(20)=6765\nCPU times: user 437 \u00b5s, sys: 182 \u00b5s, total: 619 \u00b5s\nWall time: 523 \u00b5s\n\n\nfibonacci(20)=6765\nCPU times: user 3.16 ms, sys: 243 \u00b5s, total: 3.41 ms\nWall time: 3.58 ms\n</code></pre>"},{"location":"lectures/Intro_to_Python/dynamic_programming/dynamic_programming_notebook/#bottom-up-pseudo-code","title":"Bottom up Pseudo Code","text":"<ol> <li>define a function that accepts a discrete number <code>n</code></li> <li>define an empty array of size <code>n+1</code></li> <li>In the empty <code>array</code>, specify the values for initial conditions for <code>n=0</code>, <code>n=1</code>, etc</li> <li>loop through the values between just above the initial condition values (e.g. <code>n=2</code>) and to the value of <code>n+1</code></li> <li>At each iteration of the loop, perform the recursive calculation using the array values and save it to the array.</li> <li>Make the function return the saved value of the array, <code>array[n]</code></li> </ol>"},{"location":"lectures/Intro_to_Python/dynamic_programming/dynamic_programming_notebook/#example-3-factorial-bottom-up","title":"Example 3: Factorial Bottom up","text":"<p>Design the Factorial function with bottom up in dynamic programming and evaluate the run time for  the following values of n; <code>[20,100,1000]</code> and compare it to the original function.</p> <p>What do these results say about using dynamic programming?</p> <pre><code>def factorial_bottom_up(n):\nbottom_up = [None]*(n+1)\nbottom_up[1]=1\nfor i in range(2,n+1):\nbottom_up[i] = i*bottom_up[i-1]\nreturn bottom_up[n]\nfor n in [20,100,1000]:\n%time print(f'factorial_bottom_up({n})={factorial_bottom_up(n)}')\nprint('\\n')\n%time print(f'factorial({n})={Factorial(n)}')\nprint('\\n')\n</code></pre> <pre><code>factorial_bottom_up(20)=2432902008176640000\nCPU times: user 208 \u00b5s, sys: 94 \u00b5s, total: 302 \u00b5s\nWall time: 264 \u00b5s\n\n\nfactorial(20)=2432902008176640000\nCPU times: user 64 \u00b5s, sys: 21 \u00b5s, total: 85 \u00b5s\nWall time: 89.9 \u00b5s\n\n\nfactorial_bottom_up(100)=93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000\nCPU times: user 257 \u00b5s, sys: 100 \u00b5s, total: 357 \u00b5s\nWall time: 313 \u00b5s\n\n\nfactorial(100)=93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000\nCPU times: user 62 \u00b5s, sys: 15 \u00b5s, total: 77 \u00b5s\nWall time: 69.9 \u00b5s\n\n\nfactorial_bottom_up(1000)=402387260077093773543702433923003985719374864210714632543799910429938512398629020592044208486969404800479988610197196058631666872994808558901323829669944590997424504087073759918823627727188732519779505950995276120874975462497043601418278094646496291056393887437886487337119181045825783647849977012476632889835955735432513185323958463075557409114262417474349347553428646576611667797396668820291207379143853719588249808126867838374559731746136085379534524221586593201928090878297308431392844403281231558611036976801357304216168747609675871348312025478589320767169132448426236131412508780208000261683151027341827977704784635868170164365024153691398281264810213092761244896359928705114964975419909342221566832572080821333186116811553615836546984046708975602900950537616475847728421889679646244945160765353408198901385442487984959953319101723355556602139450399736280750137837615307127761926849034352625200015888535147331611702103968175921510907788019393178114194545257223865541461062892187960223838971476088506276862967146674697562911234082439208160153780889893964518263243671616762179168909779911903754031274622289988005195444414282012187361745992642956581746628302955570299024324153181617210465832036786906117260158783520751516284225540265170483304226143974286933061690897968482590125458327168226458066526769958652682272807075781391858178889652208164348344825993266043367660176999612831860788386150279465955131156552036093988180612138558600301435694527224206344631797460594682573103790084024432438465657245014402821885252470935190620929023136493273497565513958720559654228749774011413346962715422845862377387538230483865688976461927383814900140767310446640259899490222221765904339901886018566526485061799702356193897017860040811889729918311021171229845901641921068884387121855646124960798722908519296819372388642614839657382291123125024186649353143970137428531926649875337218940694281434118520158014123344828015051399694290153483077644569099073152433278288269864602789864321139083506217095002597389863554277196742822248757586765752344220207573630569498825087968928162753848863396909959826280956121450994871701244516461260379029309120889086942028510640182154399457156805941872748998094254742173582401063677404595741785160829230135358081840096996372524230560855903700624271243416909004153690105933983835777939410970027753472000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\nCPU times: user 663 \u00b5s, sys: 123 \u00b5s, total: 786 \u00b5s\nWall time: 751 \u00b5s\n\n\nfactorial(1000)=402387260077093773543702433923003985719374864210714632543799910429938512398629020592044208486969404800479988610197196058631666872994808558901323829669944590997424504087073759918823627727188732519779505950995276120874975462497043601418278094646496291056393887437886487337119181045825783647849977012476632889835955735432513185323958463075557409114262417474349347553428646576611667797396668820291207379143853719588249808126867838374559731746136085379534524221586593201928090878297308431392844403281231558611036976801357304216168747609675871348312025478589320767169132448426236131412508780208000261683151027341827977704784635868170164365024153691398281264810213092761244896359928705114964975419909342221566832572080821333186116811553615836546984046708975602900950537616475847728421889679646244945160765353408198901385442487984959953319101723355556602139450399736280750137837615307127761926849034352625200015888535147331611702103968175921510907788019393178114194545257223865541461062892187960223838971476088506276862967146674697562911234082439208160153780889893964518263243671616762179168909779911903754031274622289988005195444414282012187361745992642956581746628302955570299024324153181617210465832036786906117260158783520751516284225540265170483304226143974286933061690897968482590125458327168226458066526769958652682272807075781391858178889652208164348344825993266043367660176999612831860788386150279465955131156552036093988180612138558600301435694527224206344631797460594682573103790084024432438465657245014402821885252470935190620929023136493273497565513958720559654228749774011413346962715422845862377387538230483865688976461927383814900140767310446640259899490222221765904339901886018566526485061799702356193897017860040811889729918311021171229845901641921068884387121855646124960798722908519296819372388642614839657382291123125024186649353143970137428531926649875337218940694281434118520158014123344828015051399694290153483077644569099073152433278288269864602789864321139083506217095002597389863554277196742822248757586765752344220207573630569498825087968928162753848863396909959826280956121450994871701244516461260379029309120889086942028510640182154399457156805941872748998094254742173582401063677404595741785160829230135358081840096996372524230560855903700624271243416909004153690105933983835777939410970027753472000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\nCPU times: user 988 \u00b5s, sys: 101 \u00b5s, total: 1.09 ms\nWall time: 1.02 ms\n</code></pre>"},{"location":"lectures/Intro_to_Python/dynamic_programming/dynamic_programming_notebook/#example-4-runing-sum-bottom-up","title":"Example 4: Runing Sum - bottom up","text":"<p>Given an array nums. We define a running sum of an array as <code>runningSum[i] = sum(nums[0]\u2026nums[i])</code>.</p> <p>Return the running sum of nums.</p> <ul> <li>Reference: https://leetcode.com/problems/running-sum-of-1d-array/</li> </ul> <p>e.g.  <code>Input: nums = [1,2,3,4] Output: [1,3,6,10]</code></p> <p>e.g. <code>Input: nums = [1,1,1,1,1] Output: [1,2,3,4,5]</code></p> <pre><code>from typing import List\ndef runningSum(nums: List[int]) -&gt; List[int]:\n#bottom up approach\nn = len(nums)\nbottom_up = [None]*(n)\nbottom_up[0] = nums[0]\nfor j in range(1,n):\nbottom_up[j] = nums[j]+bottom_up[j-1]\nreturn bottom_up\nrunningSum([1,2,3,4])\n</code></pre> <pre><code>[1, 3, 6, 10]\n</code></pre>"},{"location":"lectures/Intro_to_Python/dynamic_programming/dynamic_programming_notebook/#example-5-running-sum-memoization","title":"Example 5: Running Sum - memoization","text":"<pre><code>def runningSumMemo(nums: List[int]) -&gt; List[int]:\nans = []\ndef cumsum(ind):\nif ind == 0:\nreturn nums[ind]\nreturn nums[ind] + cumsum(ind-1)\nfor i in range(len(nums)):\nans.append(cumsum(i))\nreturn ans\nrunningSumMemo([1,2,3,4])\n</code></pre> <pre><code>[1, 3, 6, 10]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"lectures/Intro_to_Python/dynamic_programming/dynamic_programming_slides/","title":"Slides","text":""},{"location":"lectures/Intro_to_Python/oop/OOP_part1/","title":"Intro to Python: Object Oriented Programming","text":"<p>Notebook Github Repo"},{"location":"lectures/Intro_to_Python/oop/OOP_part1/#example-1-importing-instantiating-a-class","title":"Example 1: Importing &amp; Instantiating a class","text":"<pre><code>from timewithproperties import Time #comment: see source [1]\nwake_up = Time(hour=7,minute=45,second=30)\nprint(wake_up.time)\n</code></pre> timewithproperties.py<pre><code>from itertools import combinations\n\"\"\"Class with time read-write properties\"\"\"\nfrom datetime import datetime,timedelta\nclass Time:\n\"\"\"Class time with read-write properties\"\"\"\ndef __init__(self,hour = 0,minute = 0,second = 0):\n\"\"\"Initialize each attrbiute\"\"\"\nself.hour = int(hour)\nself.minute = int(minute)\nself.second = int(second)\nself.time = (self.hour,self.minute,self.second)\n@property\ndef hour(self):\n\"\"\"Return the hour\"\"\"\nreturn self._hour\n@hour.setter\ndef hour(self,hour):\n\"\"\"Set the hour.\"\"\"\nif not (0&lt;=hour&lt;24):\nraise ValueError(f'Hour ({hour}) must be 0-23')\nself._hour = hour\n@property\ndef minute(self):\n\"\"\"Return the minute\"\"\"\nreturn self._minute\n@minute.setter\ndef minute(self,minute):\n\"\"\"Set minute\"\"\"\nif not (0&lt;=minute&lt;60):\nraise ValueError(f'Minute({minute}) must be 0-59')\nself._minute = minute\n@property\ndef second(self):\n\"\"\"Return the second\"\"\"\nreturn self._second\n@second.setter\ndef second(self,second):\n\"\"\" Set the second\"\"\"\nif not (0&lt;=second&lt;60):\nraise ValueError(f'Second ({second}) must be 0-59')\nself._second = second\ndef set_time(self,hour=0,minute=0,second=0):\n\"\"\"set values of hour, minute and second\"\"\"\nself.hour = hour\nself.minute = minute\nself.second = second\nself._time  = (hour,minute,second)\n@property\ndef time(self):\n\"\"\"Return hour, minute and second as a tuple\"\"\"\nreturn self._time\n@time.setter\ndef time(self,time_tuple):\n\"\"\"Set time form a tuple comntianing hour,minute and second\"\"\"\nself.set_time(time_tuple[0],time_tuple[1],time_tuple[2])\ndef add_time(self,hour,minute,second):\ncurrent_time = datetime.strptime('{0}:{1}:{2}'.format(self.hour,self.minute,self.second), '%H:%M:%S')\nnew_time = current_time + timedelta(days=0,hours=hour,minutes=minute,seconds=second)\nnew_time_print = new_time.strftime('%H:%M:%S')\nprint('new time: ',new_time_print)\nhour,minute,second = [int(j) for j in new_time_print.split(':')]\nself.set_time(hour,minute,second)\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part1/#example-2-using-a-function-within-a-class","title":"Example 2: Using a function within a class","text":"<pre><code>from timewithproperties import Time\nwake_up2 = Time(hour=7,minute=45,second=30)\nwake_up2.add_time(hour=4,minute=5,second=10)\nprint(wake_up2.time)\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part1/#example-3-using-class-doc-strings","title":"Example 3: Using Class doc strings","text":"<p>Show the doc strings associated with; * Time class * the <code>add_time</code> function in the time class</p> <pre><code>Time?\n</code></pre> <pre><code>Time.add_time?\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part1/#example-4-building-a-class-doc-string","title":"Example 4: Building a class doc string","text":"<pre><code>class demo:\n\"\"\"This is the class doc string.\"\"\"\ndef __init__(self,variable1):\nself.variable1 =variable1\ndef print_variables(self):\n\"\"\"This is the doc string for this function in this class\"\"\"\nprint(self.variable1)\nd = demo('hello')\nd.print_variables()\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part1/#example-5-public-vs-private-attributes","title":"Example 5: Public vs Private Attributes","text":"<ul> <li>The <code>__</code> underscore at the beginning of the class is what defines the private vs public property; e.g. <code>self.__private_data</code></li> </ul> <pre><code>class Private:\ndef __init__(self):\nself.public_data=\"public\"\nself.__private_data=\"private\"\np = Private()\nprint(p.public_data)\nprint(p.__private_data)\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part1/#example-6-decorators","title":"Example 6: Decorators","text":"<p>In this example we create a decorator function that takes an arbitrary python function and multiplies it's output by a factor of 10.</p> <pre><code>def multiply_by10(func):\ndef inner(*args,**kwargs):\nreturn 10*func(*args,**kwargs)\nreturn inner\n@multiply_by10\ndef add_two_numbers(a,b):\nreturn a+b\nadd_two_numbers(3,5)\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part1/#example-7-read-only-vs-changeable-properties","title":"Example 7: Read only vs changeable properties","text":"<ul> <li>Class with a property setter</li> </ul> <pre><code>class property_setter:\ndef __init__(self,alpha):\nself._a = alpha\n@property\ndef a(self):\nreturn self._a\n@a.setter\ndef a(self,alpha):\nself._a = alpha\np7 = property_setter('hello')\nprint(p7._a)\np7._a='good bye'\nprint(p7._a)\n</code></pre> <p>Class **without a property setter, for mutability**</p> <pre><code>class property_no_setter:\ndef __init__(self,alpha):\nself._a = alpha\n@property\ndef a(self):\nreturn self._a\np71 = property_no_setter('hello')\nprint(p71.a)\np71.a='good bye'\nprint(p71.a)\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part1/#example-8-representing-a-class","title":"Example 8: Representing a class","text":"<pre><code>class demo_repr_method:\ndef __init__(self,alpha):\nself.alpha = alpha\ndef __repr__(self):\n\"\"\"Return initialization strin for the class\"\"\"\nreturn (f'demo_repr_method(alpha={self.alpha})')\n</code></pre> <pre><code>d = demo_repr_method(45)\nprint(d.__repr__())\n</code></pre> <p>What happens if we change the parameters?</p> <pre><code>d.alpha = 19\nprint(d.__repr__())\n</code></pre> <pre><code>\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part1/#sources","title":"Sources","text":"<ul> <li>[1] Timewithproperties.py - this file taken from the textbook \"Intro to Python for Computer Science and Data Science\" By Deitel &amp; Deitel, Pearson Publications 2020</li> </ul>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/","title":"Object Oriented Programming Part 2","text":"<p>Link to Notebook on Github</p>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-1-building-class-inheritence","title":"Example 1 Building class inheritence","text":"<pre><code>class loan:\n#parent class\ndef statement(self):\nprint('I am a bank loan')\nclass mortgage(loan):\n#child class\ndef announcement(self):\nprint('My interest rate')\nm = mortgage()\nm.announcement()\nm.statement()\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-2-parent-child-classes","title":"Example 2: Parent-child classes","text":"<p>Parent-child classes that have methods of the same name.</p> <pre><code>class animal:\ndef statement(self):\nprint('animal class')\nclass fish(animal):\ndef statement(self):\nprint('fish class')\nf = fish()\nf.statement()\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-3-overriding-the-child-classes-method","title":"Example 3: Overriding the child classes' method","text":"<p>Overriding the child classes' method that shares the same name as the parent via the <code>super()</code> clause</p> <pre><code>class animal:\ndef statement(self):\nreturn 'animal class'\nclass fish(animal):\ndef statement(self):\nif super().statement() is not None:\nreturn super().statement()\nelse:\nreturn 'fish class'\nf = fish()\nf.statement()\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-4-using-the-super-method","title":"Example 4: Using the <code>super()</code> method","text":"<p>class to extract the constructor from the parent class into the child class</p> <pre><code>class loan:\ndef __init__(self,rate):\nself.rate=rate\ndef statement(self):\nreturn 'the rate is {}%'.format(self.rate)\nclass mortgage(loan):\ndef __init__(self,rate,downpayment):\nsuper().__init__(rate)\nself.downpayment = downpayment\ndef statement(self):\nreturn super().statement()+' and downpayment is {}'.format(self.downpayment)\nl = loan(3)\nm = mortgage(l.rate,10000)\nm.statement()\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-5-definine-name-spacesvariable-scopes","title":"Example 5: Definine name spaces/variable scopes","text":"<pre><code>#global name space\ndef outer_function():\n#enclosed name space\nn=100\ndef inner_function():\n#local name space\nm=200\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-6-demonstrating-local-name-spaces","title":"Example 6: Demonstrating local name spaces","text":"<pre><code>def add_N_to_number(x):\nN=3\nreturn N+x\nN\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-7-demonstrating-global-name-spaces","title":"Example 7: Demonstrating global name spaces","text":"<pre><code>def add_V_to_number(x):\nglobal V\nV=3\nreturn V+x\nadd_V_to_number(5)\n</code></pre> <pre><code>print(V)\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-8-polymorphisms","title":"Example 8: Polymorphisms","text":"<p>A basic polymorphism in python</p> <pre><code>dictionary = {'a':10,'b':20,'c':30,'d':40}\nword = 'alpha'\nList = [0,1,2,3]\nprint(len(dictionary))\nprint(len(word))\nprint(len(List))\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-9-demonstrating-using-a-polymorphism","title":"Example 9: Demonstrating using a polymorphism","text":"<p>Demonstrating using a polymorphism for classes that have attributes of the same name (with a list)</p> <pre><code>class animal:\ndef __init__(self,name):\nself.name = name\nclass plant:\ndef __init__(self,name):\nself.name = name\nfor c in [animal('dolphin'),plant('birch_tree')]:\nprint(c.name)\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-10-polymorphisms-the-share-names","title":"Example 10: Polymorphisms the share names","text":"<p>Demonstrating using a polymorphism for classes that have attributes of the same name (with a function)</p> <pre><code>class animal:\ndef __init__(self,name):\nself.name = name\nclass plant:\ndef __init__(self,name):\nself.name = name\ndef class_polymorphism_printer(c):\nprint(c.name)\nclass_polymorphism_printer(plant('venus_fly_trap'))\nclass_polymorphism_printer(animal('cow'))\n</code></pre>"},{"location":"lectures/Intro_to_Python/oop/OOP_part2/#example-11-class-polymorphism-with-inheritence","title":"Example 11: Class polymorphism with inheritence","text":"<p>using the parent-child inheritence to alter the methods of the child class.</p> <pre><code>class animal:\ndef __init__(self,name):\nself.name = name\nclass vertebrae(animal):\ndef __init__(self,warm_blooded):\nsuper().__init__('alligator')\nself.warm_blooded = warm_blooded\nv = vertebrae(False)\nprint(v.name)\nprint(v.warm_blooded)\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/","title":"Intro to Spark with Python","text":"<p>by Joe Ganser</p> <p>Github Repo Link</p>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#beginning-a-py-spark-session","title":"Beginning a Py-Spark session","text":"<p>The begin working with spark on a local machine (with Python), we should import the <code>SparkSession</code> package and use it's <code>.getOrCreate()</code> method to develop a spark work flow.</p> <pre><code>try:\nfrom pyspark.sql import SparkSession\nexcept:\n!pip install pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Python Spark SQL basic example\")\\\n                            .config(\"spark.some.config.option\", \"some-value\")\\\n                            .getOrCreate()\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#load-a-data-frame","title":"Load a data frame","text":"<p>Heres a a simple example of loading a dataframe from a source on Github.</p> <ul> <li>Add the remote file to our spark context using <code>spark.sparkContext.addFile(url)</code></li> <li>Use the <code>SparkFiles</code> method to access temporarily downloaded csv file to our <code>SparkSession</code></li> <li>Tell spark we want to look for the dataframes header by specifying <code>header=True</code></li> <li>Tell spark to infer the schema (figure out the column name data types), etc using <code>inferSchema=True</code>.</li> <li>Show the first 20 rows uing the <code>.show()</code> method</li> </ul> <pre><code>url = 'https://raw.githubusercontent.com/JoeGanser/Spark-The-Definitive-Guide/master/data/flight-data/csv/'\ncsv_file = \"2015-summary.csv\"\nurl = url+csv_file\nfrom pyspark import SparkFiles\nspark.sparkContext.addFile(url)\nflightData2015 = spark.read.csv(\"file://\"+SparkFiles.get(csv_file), header=True, inferSchema= True)\nflightData2015.show()\n</code></pre> <pre><code>+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|   15|\n|       United States|            Croatia|    1|\n|       United States|            Ireland|  344|\n|               Egypt|      United States|   15|\n|       United States|              India|   62|\n|       United States|          Singapore|    1|\n|       United States|            Grenada|   62|\n|          Costa Rica|      United States|  588|\n|             Senegal|      United States|   40|\n|             Moldova|      United States|    1|\n|       United States|       Sint Maarten|  325|\n|       United States|   Marshall Islands|   39|\n|              Guyana|      United States|   64|\n|               Malta|      United States|    1|\n|            Anguilla|      United States|   41|\n|             Bolivia|      United States|   30|\n|       United States|           Paraguay|    6|\n|             Algeria|      United States|    4|\n|Turks and Caicos ...|      United States|  230|\n|       United States|          Gibraltar|    1|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#printschema-method","title":"printSchema( ) method","text":"<p>Schemas are an important part of working with Spark. They tell use the column names, data types and if the column can contain null values. We can save computational power by telling Spark the schema of data source before loading it. If we want to take a look at a dataframe's schema we use the <code>.printSchema()</code> method.</p> <pre><code>flightData2015.printSchema()\n</code></pre> <pre><code>root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#selecting-a-specific-columns","title":"Selecting a specific columns","text":"<p>To select a specific column you use the <code>.select(columnname1, columnname2..)</code> method on the dataframe object.</p> <pre><code>selection = flightData2015.select('DEST_COUNTRY_NAME','count')\nselection.show()\n</code></pre> <pre><code>+--------------------+-----+\n|   DEST_COUNTRY_NAME|count|\n+--------------------+-----+\n|       United States|   15|\n|       United States|    1|\n|       United States|  344|\n|               Egypt|   15|\n|       United States|   62|\n|       United States|    1|\n|       United States|   62|\n|          Costa Rica|  588|\n|             Senegal|   40|\n|             Moldova|    1|\n|       United States|  325|\n|       United States|   39|\n|              Guyana|   64|\n|               Malta|    1|\n|            Anguilla|   41|\n|             Bolivia|   30|\n|       United States|    6|\n|             Algeria|    4|\n|Turks and Caicos ...|  230|\n|       United States|    1|\n+--------------------+-----+\nonly showing top 20 rows\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#selecting-with-sql-like-expressions","title":"Selecting with SQL like expressions","text":"<p>We can select specific columns and even pass aggregation functions (e.g. <code>count</code>,<code>max</code>,<code>sum</code>) using the <code>selectExpr()</code> method on Spark dataframes.</p> <pre><code>flightData2015.selectExpr('DEST_COUNTRY_NAME', 'count').show(5)\n</code></pre> <pre><code>+-----------------+-----+\n|DEST_COUNTRY_NAME|count|\n+-----------------+-----+\n|    United States|   15|\n|    United States|    1|\n|    United States|  344|\n|            Egypt|   15|\n|    United States|   62|\n+-----------------+-----+\nonly showing top 5 rows\n</code></pre> <pre><code>flightData2015.selectExpr('count(DEST_COUNTRY_NAME)', 'max(count)').show()\n</code></pre> <pre><code>+------------------------+----------+\n|count(DEST_COUNTRY_NAME)|max(count)|\n+------------------------+----------+\n|                     256|    370002|\n+------------------------+----------+\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#dataframe-analytics","title":"DataFrame analytics","text":"<p>We can describe the dataframe using the <code>.describe()</code> method.</p> <pre><code>flightData2015.describe().show()\n</code></pre> <pre><code>+-------+-----------------+-------------------+------------------+\n|summary|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|             count|\n+-------+-----------------+-------------------+------------------+\n|  count|              256|                256|               256|\n|   mean|             null|               null|       1770.765625|\n| stddev|             null|               null|23126.516918551915|\n|    min|          Algeria|             Angola|                 1|\n|    max|           Zambia|            Vietnam|            370002|\n+-------+-----------------+-------------------+------------------+\n</code></pre> <p>We can combine this method with the <code>select()</code> in a code chain to act only on specific columns.</p> <pre><code>flightData2015.select('DEST_COUNTRY_NAME','ORIGIN_COUNTRY_NAME')\\\n              .describe()\\\n              .show()\n</code></pre> <pre><code>+-------+-----------------+-------------------+\n|summary|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-------+-----------------+-------------------+\n|  count|              256|                256|\n|   mean|             null|               null|\n| stddev|             null|               null|\n|    min|          Algeria|             Angola|\n|    max|           Zambia|            Vietnam|\n+-------+-----------------+-------------------+\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#group-by-aggregation","title":"Group by aggregation","text":"<p>We can group by specific columns using the <code>.groupby()</code> and <code>.agg()</code> functions.</p> <p>For every origin country in the data set, count the number of destination countries. Sort the values in decreasing order.</p> <pre><code>from pyspark.sql.functions import count,col\nflightData2015.groupby('ORIGIN_COUNTRY_NAME')\\\n              .agg(count(col('DEST_COUNTRY_NAME')))\\\n              .orderBy(\"count(DEST_COUNTRY_NAME)\",ascending=False)\\\n              .show()\n</code></pre> <pre><code>+--------------------+------------------------+\n| ORIGIN_COUNTRY_NAME|count(DEST_COUNTRY_NAME)|\n+--------------------+------------------------+\n|       United States|                     132|\n|            Paraguay|                       1|\n|            Anguilla|                       1|\n|              Russia|                       1|\n|              Guyana|                       1|\n|             Senegal|                       1|\n|              Sweden|                       1|\n|            Kiribati|                       1|\n|               Palau|                       1|\n|         Philippines|                       1|\n|           Singapore|                       1|\n|            Malaysia|                       1|\n|                Fiji|                       1|\n|              Turkey|                       1|\n|             Germany|                       1|\n|              Jordan|                       1|\n|Turks and Caicos ...|                       1|\n|              France|                       1|\n|              Greece|                       1|\n|British Virgin Is...|                       1|\n+--------------------+------------------------+\nonly showing top 20 rows\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#adding-a-new-column-to-the-dataset","title":"Adding a new column to the dataset","text":"<p>If we want to add a new column to our dataframe, we can use the <code>.withColumn(column_name, column_data)</code> method, where <code>column_name</code> and <code>column_data</code> are the arguments.</p> <p>In this example we create a columnd <code>double the count</code> that takes the values of the <code>count</code> column and doubles them.</p> <pre><code>flightData2015 = flightData2015.withColumn('double the count',flightData2015['count']*2)\nflightData2015.show()\n</code></pre> <pre><code>+--------------------+-------------------+-----+----------------+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|double the count|\n+--------------------+-------------------+-----+----------------+\n|       United States|            Romania|   15|              30|\n|       United States|            Croatia|    1|               2|\n|       United States|            Ireland|  344|             688|\n|               Egypt|      United States|   15|              30|\n|       United States|              India|   62|             124|\n|       United States|          Singapore|    1|               2|\n|       United States|            Grenada|   62|             124|\n|          Costa Rica|      United States|  588|            1176|\n|             Senegal|      United States|   40|              80|\n|             Moldova|      United States|    1|               2|\n|       United States|       Sint Maarten|  325|             650|\n|       United States|   Marshall Islands|   39|              78|\n|              Guyana|      United States|   64|             128|\n|               Malta|      United States|    1|               2|\n|            Anguilla|      United States|   41|              82|\n|             Bolivia|      United States|   30|              60|\n|       United States|           Paraguay|    6|              12|\n|             Algeria|      United States|    4|               8|\n|Turks and Caicos ...|      United States|  230|             460|\n|       United States|          Gibraltar|    1|               2|\n+--------------------+-------------------+-----+----------------+\nonly showing top 20 rows\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#dropping-columns","title":"Dropping columns","text":"<p>We can drop columns using the <code>.drop()</code> method.</p> <pre><code>flightData2015 = flightData2015.drop('double the count')\nflightData2015.show()\n</code></pre> <pre><code>+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|   15|\n|       United States|            Croatia|    1|\n|       United States|            Ireland|  344|\n|               Egypt|      United States|   15|\n|       United States|              India|   62|\n|       United States|          Singapore|    1|\n|       United States|            Grenada|   62|\n|          Costa Rica|      United States|  588|\n|             Senegal|      United States|   40|\n|             Moldova|      United States|    1|\n|       United States|       Sint Maarten|  325|\n|       United States|   Marshall Islands|   39|\n|              Guyana|      United States|   64|\n|               Malta|      United States|    1|\n|            Anguilla|      United States|   41|\n|             Bolivia|      United States|   30|\n|       United States|           Paraguay|    6|\n|             Algeria|      United States|    4|\n|Turks and Caicos ...|      United States|  230|\n|       United States|          Gibraltar|    1|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#renaming-columns","title":"Renaming Columns","text":"<p>We can rename columns using the <code>.withColumnRenamed(old_column_name, new_column_name)</code> method</p> <pre><code>flightData2015.withColumnRenamed('DEST_COUNTRY_NAME','DEST')\\\n              .withColumnRenamed('ORIGIN_COUNTRY_NAME','ORIGIN')\\\n              .show()\n</code></pre> <pre><code>+--------------------+----------------+-----+\n|                DEST|          ORIGIN|count|\n+--------------------+----------------+-----+\n|       United States|         Romania|   15|\n|       United States|         Croatia|    1|\n|       United States|         Ireland|  344|\n|               Egypt|   United States|   15|\n|       United States|           India|   62|\n|       United States|       Singapore|    1|\n|       United States|         Grenada|   62|\n|          Costa Rica|   United States|  588|\n|             Senegal|   United States|   40|\n|             Moldova|   United States|    1|\n|       United States|    Sint Maarten|  325|\n|       United States|Marshall Islands|   39|\n|              Guyana|   United States|   64|\n|               Malta|   United States|    1|\n|            Anguilla|   United States|   41|\n|             Bolivia|   United States|   30|\n|       United States|        Paraguay|    6|\n|             Algeria|   United States|    4|\n|Turks and Caicos ...|   United States|  230|\n|       United States|       Gibraltar|    1|\n+--------------------+----------------+-----+\nonly showing top 20 rows\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#filtering-columns-for-specific-row-values","title":"Filtering Columns for specific row values","text":"<p>We can filter the dataframe to display only rows yielding specific values using the <code>.filter()</code> method. The arguments use a SQL like syntax</p> <pre><code>flightData2015.filter(\"DEST_COUNTRY_NAME = 'United States'\")\\\n              .show()\n</code></pre> <pre><code>+-----------------+--------------------+-----+\n|DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|\n+-----------------+--------------------+-----+\n|    United States|             Romania|   15|\n|    United States|             Croatia|    1|\n|    United States|             Ireland|  344|\n|    United States|               India|   62|\n|    United States|           Singapore|    1|\n|    United States|             Grenada|   62|\n|    United States|        Sint Maarten|  325|\n|    United States|    Marshall Islands|   39|\n|    United States|            Paraguay|    6|\n|    United States|           Gibraltar|    1|\n|    United States|Federated States ...|   69|\n|    United States|              Russia|  161|\n|    United States|         Netherlands|  660|\n|    United States|             Senegal|   42|\n|    United States|              Angola|   13|\n|    United States|            Anguilla|   38|\n|    United States|             Ecuador|  300|\n|    United States|              Cyprus|    1|\n|    United States|            Portugal|  134|\n|    United States|          Costa Rica|  608|\n+-----------------+--------------------+-----+\nonly showing top 20 rows\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#using-sql-to-query-dataframe-values","title":"Using SQL to query dataframe values","text":"<p>We can run sql queries on a dataframe using <code>spark.sql( query text )</code> method. But before we can do that, we must first register the table as one that can be queried using sql. This is done using the <code>.registerTempTable(tablename)</code> method.</p> <pre><code>#Writing in SQL example\nflightData2015.registerTempTable(\"flightData2015\")\nspark.sql(\"select distinct DEST_COUNTRY_NAME from flightData2015 order by DEST_COUNTRY_NAME ASC\")\\\n     .show(20)\n</code></pre> <pre><code>/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n\n\n+--------------------+\n|   DEST_COUNTRY_NAME|\n+--------------------+\n|             Algeria|\n|              Angola|\n|            Anguilla|\n| Antigua and Barbuda|\n|           Argentina|\n|               Aruba|\n|           Australia|\n|             Austria|\n|          Azerbaijan|\n|             Bahrain|\n|            Barbados|\n|             Belgium|\n|              Belize|\n|             Bermuda|\n|             Bolivia|\n|Bonaire, Sint Eus...|\n|              Brazil|\n|British Virgin Is...|\n|            Bulgaria|\n|        Burkina Faso|\n+--------------------+\nonly showing top 20 rows\n</code></pre> <pre><code>\n</code></pre>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_nb/#sources","title":"Sources","text":"<p>Some of the notes here were taken from the following text book, in conjunction to examples written by myself.</p> <ul> <li>Spark: The Definitive Guide Big Data Processing Made Simple, Bill Chabers &amp; Matel Zaharia, O'Reilly Publications 2018<ul> <li>Purchase here: Amazon Link.</li> </ul> </li> </ul>"},{"location":"lectures/spark/intro_to_spark/intro_to_spark_slides/","title":"Slides","text":""},{"location":"lectures/spark/splink/splink_slides/","title":"splink lecture","text":""},{"location":"lectures/stats/eda/eda/","title":"Exploratory Data Analysis","text":"<p>Notebook github link</p>"},{"location":"lectures/stats/eda/eda/#load-our-data-housing-data-set-with-house-prices","title":"Load our data: Housing data set, with house prices","text":"<pre><code>import pandas as pd\nfrom sklearn.datasets import fetch_openml\nhousing = fetch_openml(name=\"house_prices\", as_frame=True)\nX = pd.DataFrame(housing.data, columns=housing.feature_names)\nX.drop('Id',axis=1,inplace=True)\ny = housing.target\nX.head()\n</code></pre> MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig ... ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition 0 60.0 RL 65.0 8450.0 Pave None Reg Lvl AllPub Inside ... 0.0 0.0 None None None 0.0 2.0 2008.0 WD 1 20.0 RL 80.0 9600.0 Pave None Reg Lvl AllPub FR2 ... 0.0 0.0 None None None 0.0 5.0 2007.0 WD 2 60.0 RL 68.0 11250.0 Pave None IR1 Lvl AllPub Inside ... 0.0 0.0 None None None 0.0 9.0 2008.0 WD 3 70.0 RL 60.0 9550.0 Pave None IR1 Lvl AllPub Corner ... 0.0 0.0 None None None 0.0 2.0 2006.0 WD 4 60.0 RL 84.0 14260.0 Pave None IR1 Lvl AllPub FR2 ... 0.0 0.0 None None None 0.0 12.0 2008.0 WD"},{"location":"lectures/stats/eda/eda/#example-1-dataframe-evaluation","title":"Example 1: Dataframe evaluation","text":"<ul> <li>Demonstrate size of the data set (rows, columns)</li> <li>Identify the numerical columns</li> <li>Identify the categorical columns</li> <li>Identify the number of unique categories in the feature <code>SaleCondition</code><ul> <li>Put these into dummy columns</li> </ul> </li> </ul> <pre><code>print(X.shape)\n</code></pre> <pre><code>(1460, 79)\n</code></pre> <pre><code>numerical_columns = [j for j in X.columns if X[j].dtype in [float,int]]\nX[numerical_columns].head()\n</code></pre> MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 BsmtFinSF2 ... GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold 0 60.0 65.0 8450.0 7.0 5.0 2003.0 2003.0 196.0 706.0 0.0 ... 548.0 0.0 61.0 0.0 0.0 0.0 0.0 0.0 2.0 1 20.0 80.0 9600.0 6.0 8.0 1976.0 1976.0 0.0 978.0 0.0 ... 460.0 298.0 0.0 0.0 0.0 0.0 0.0 0.0 5.0 2 60.0 68.0 11250.0 7.0 5.0 2001.0 2002.0 162.0 486.0 0.0 ... 608.0 0.0 42.0 0.0 0.0 0.0 0.0 0.0 9.0 3 70.0 60.0 9550.0 7.0 5.0 1915.0 1970.0 0.0 216.0 0.0 ... 642.0 0.0 35.0 272.0 0.0 0.0 0.0 0.0 2.0 4 60.0 84.0 14260.0 8.0 5.0 2000.0 2000.0 350.0 655.0 0.0 ... 836.0 192.0 84.0 0.0 0.0 0.0 0.0 0.0 12.0 <pre><code>categorical_columns = [j for j in X.columns if j not in numerical_columns]\nX[categorical_columns].head()\n</code></pre> MSZoning Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 ... GarageType GarageFinish GarageQual GarageCond PavedDrive PoolQC Fence MiscFeature SaleType SaleCondition 0 RL Pave None Reg Lvl AllPub Inside Gtl CollgCr Norm ... Attchd RFn TA TA Y None None None WD 1 RL Pave None Reg Lvl AllPub FR2 Gtl Veenker Feedr ... Attchd RFn TA TA Y None None None WD 2 RL Pave None IR1 Lvl AllPub Inside Gtl CollgCr Norm ... Attchd RFn TA TA Y None None None WD 3 RL Pave None IR1 Lvl AllPub Corner Gtl Crawfor Norm ... Detchd Unf TA TA Y None None None WD 4 RL Pave None IR1 Lvl AllPub FR2 Gtl NoRidge Norm ... Attchd RFn TA TA Y None None None WD <p>5 rows \u00d7 43 columns</p> <pre><code>print(X['SaleCondition'].describe(),'\\n')\nprint(X['SaleCondition'].unique())\nX['SaleCondition'].head()\n</code></pre> <pre><code>count       1460\nunique         6\ntop       Normal\nfreq        1198\nName: SaleCondition, dtype: object\n\n['Normal' 'Abnorml' 'Partial' 'AdjLand' 'Alloca' 'Family']\n\n0     Normal\n1     Normal\n2     Normal\n3    Abnorml\n4     Normal\nName: SaleCondition, dtype: object\n</code></pre> <pre><code>pd.get_dummies(X['SaleCondition'])\n</code></pre> Abnorml AdjLand Alloca Family Normal Partial 0 0 0 0 0 1 1 0 0 0 0 1 2 0 0 0 0 1 3 1 0 0 0 0 4 0 0 0 0 1 ... ... ... ... ... ... 1455 0 0 0 0 1 1456 0 0 0 0 1 1457 0 0 0 0 1 1458 0 0 0 0 1 1459 0 0 0 0 1 <p>1460 rows \u00d7 6 columns</p>"},{"location":"lectures/stats/eda/eda/#example-2-house-price-histograms","title":"Example 2: House Price histograms","text":"<ul> <li>Include a vertical bars indicating the mean price</li> <li>Include vertical bars indicating the mean minus and plus 1 standard deviation</li> </ul> <pre><code>import matplotlib.pyplot as plt\nplt.figure(figsize=(8,6))\nplt.hist(y)\nplt.xticks(rotation=45)\nplt.title('example2.png')\nplt.xlabel('house prices')\nplt.ylabel('observations')\nplt.axvline(y.mean(),c='red',label='mean')\nplt.axvline(y.mean()+y.std(),c='orange',label='mean + 1std')\nplt.axvline(y.mean()-y.std(),c='orange',label='mean - 1std')\nplt.legend()\nplt.savefig('example2.png')\nplt.show()\n</code></pre>"},{"location":"lectures/stats/eda/eda/#example-3-beeswarm-plots","title":"Example 3: Beeswarm plots","text":"<p><code>categorical_features = ['Exterior1st','Exterior2nd','ExterQual','ExterCond']</code></p> <pre><code>import seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nn=1\ncategorical_features = ['Exterior1st','Exterior2nd','ExterQual','ExterCond']\nplt.subplots(figsize=(15,15))\nfor col in categorical_features:\nplt.subplot(len(categorical_features),1,n)\nplt.ylabel('SalePrice')\nsns.swarmplot(x=col,y='SalePrice',data=pd.concat([X,y],axis=1))\nn=n+1\nplt.subplots_adjust(hspace=0.3)\nplt.savefig('example3.png')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/stats/eda/eda/#example-4-feature-pearson-correlation","title":"Example 4: Feature Pearson Correlation","text":"<p>Find the pearson correlation coefficient for all the numerical features, as compared to the target variable <code>SalePrice</code>. Sort them in a dataframe</p> <pre><code>from scipy.stats import pearsonr\nnumerical_columns = [j for j in X.columns if X[j].dtype in [float,int]]\ncoefs = {}\nfor col in numerical_columns:\n_ = pd.concat([X[col],y],axis=1).dropna()\npearson_coef = round(pearsonr(_[col],_['SalePrice'])[0],3)\ncoefs[col] = [pearson_coef]\ncoefficients = pd.DataFrame(coefs).transpose().sort_values(by=0,ascending=False)\ncoefficients.columns = ['Pearson correlation coefficient']\ncoefficients\n</code></pre> Feature Pearson coefficient OverallQual 0.791 GrLivArea 0.709 GarageCars 0.640 GarageArea 0.623 TotalBsmtSF 0.614 1stFlrSF 0.606 FullBath 0.561 TotRmsAbvGrd 0.534 YearBuilt 0.523 YearRemodAdd 0.507 GarageYrBlt 0.486 MasVnrArea 0.477 Fireplaces 0.467 BsmtFinSF1 0.386 LotFrontage 0.352 WoodDeckSF 0.324 2ndFlrSF 0.319 OpenPorchSF 0.316 HalfBath 0.284 LotArea 0.264 BsmtFullBath 0.227 BsmtUnfSF 0.214 BedroomAbvGr 0.168 ScreenPorch 0.111 PoolArea 0.092 MoSold 0.046 3SsnPorch 0.045 BsmtFinSF2 -0.011 BsmtHalfBath -0.017 MiscVal -0.021 LowQualFinSF -0.026 YrSold -0.029 OverallCond -0.078 MSSubClass -0.084 EnclosedPorch -0.129 KitchenAbvGr -0.136"},{"location":"lectures/stats/eda/eda/#example-5-feature-mutual-information","title":"Example 5: Feature Mutual Information","text":"<p>Join the previous data frame with the mutual information between the target <code>SalePrice</code> and the associated numerical feature.</p> <pre><code>import numpy as np\nfrom sklearn.feature_selection import mutual_info_regression\nmir = mutual_info_regression(X[numerical_columns].fillna(X[numerical_columns].mean()),y)\nmir = pd.DataFrame(mir)\nmir.index = numerical_columns\ncoefficients['mutual_information'] = mir.apply(lambda x: round(x,3))\ncoefficients\n</code></pre> Feature mutual_information pearson correlation coefficient OverallQual 0.791 0.563 GrLivArea 0.709 0.483 GarageCars 0.640 0.357 GarageArea 0.623 0.366 TotalBsmtSF 0.614 0.366 1stFlrSF 0.606 0.309 FullBath 0.561 0.258 TotRmsAbvGrd 0.534 0.224 YearBuilt 0.523 0.366 YearRemodAdd 0.507 0.246 GarageYrBlt 0.486 0.295 MasVnrArea 0.477 0.098 Fireplaces 0.467 0.167 BsmtFinSF1 0.386 0.145 LotFrontage 0.352 0.197 WoodDeckSF 0.324 0.105 2ndFlrSF 0.319 0.199 OpenPorchSF 0.316 0.149 HalfBath 0.284 0.077 LotArea 0.264 0.156 BsmtFullBath 0.227 0.017 BsmtUnfSF 0.214 0.121 BedroomAbvGr 0.168 0.072 ScreenPorch 0.111 0.024 PoolArea 0.092 0.000 MoSold 0.046 0.000 3SsnPorch 0.045 0.005 BsmtFinSF2 -0.011 0.002 BsmtHalfBath -0.017 0.003 MiscVal -0.021 0.006 LowQualFinSF -0.026 0.014 YrSold -0.029 0.007 OverallCond -0.078 0.124 MSSubClass -0.084 0.275 EnclosedPorch -0.129 0.036 KitchenAbvGr -0.136 0.029"},{"location":"lectures/stats/eda/eda/#example-6-numerical-features-vs-sale-price","title":"Example 6: Numerical features vs sale price","text":"<p>Make a scatter plot of numerical columns versus the sale price.     * Include the Pearson correlation and mutual information in the title for each plot     * Do this for 16 of the numerical features     * Plot should be 4x4, as seen below.</p> <pre><code>n_numerical_features = 16\nn=1\nplt.figure(figsize=(16,16))\nfor col in numerical_columns[:16]:\nplt.subplot(4,4,n)\nplt.scatter(X[col],y)\nplt.xlabel(col)\nif (n-1)%4==0:\nplt.ylabel('SalePrice')\npearson_coef = coefficients.loc[col]['Pearson correlation coefficient']    \nmutual_info = coefficients.loc[col]['mutual_information']\nplt.title('Pearson correlation: {} \\n Mutual information: {}'.format(pearson_coef,mutual_info))\nn=n+1\nplt.subplots_adjust(hspace=0.7,wspace=0.7)\nplt.savefig('example5.png')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/stats/eda/eda/#example-7-heatmap-evaluation","title":"Example 7: Heatmap evaluation","text":"<p>** Make a heat map to describe the mutual correlation (pearson correlation coefficient) of the features that had a correlation coefficient with the target price greater than |+-0.3|.**</p> <pre><code>from seaborn import heatmap\nimport numpy as np\nfrom scipy.stats import pearsonr\nnumerical_columns = [j for j in X.columns if X[j].dtype in [float,int]]\nrelevant_numerical = []\nfor col in numerical_columns:\n_ = pd.concat([X[col],y],axis=1).dropna()\ncoef = round(pearsonr(_[col],_['SalePrice'])[0],3)\nif np.abs(coef)&gt;0.3:\nrelevant_numerical.append(col)\nnum_corr = X[relevant_numerical].corr()\nplt.figure(figsize=(20,20))\nheatmap(num_corr, cmap=\"YlGnBu\", annot=True)\nplt.title('Heat Map for features with pearson correlation withSalePrice greater than |0.3| \\n\\n i.e.   pearson(feature,SalePrice)&gt;=|0.3|',fontsize=20)\nplt.savefig('example6.png')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/stats/eda/eda/#example-8-line-plot-the-prices","title":"Example 8: Line plot the prices","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nval = 0. # this is the value where you want the data to appear on the y-axis.\nar = np.arange(10) # just as an example array\nplt.scatter(y, np.zeros_like(y) + val,marker='x',c='red')\nplt.scatter(y.mean(),0,label='mean',s=200)\nplt.scatter(y.mean()+y.std(),0,label='mean + 1std',marker='D',s=200,color='green')\nplt.scatter(y.mean()-y.std(),0,label='mean - 1std',marker='D',s=200,color='green')\nax = plt.gca()\nax.axes.yaxis.set_ticks([])\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"lectures/stats/hypothesis_testing/hypothesis_testing2/","title":"hypothesis testing part 2","text":""},{"location":"lectures/stats/hypothesis_testing/hypothesis_testing_intro/","title":"intro to hypothesis testing","text":""},{"location":"lectures/stats/hypothesis_testing/hypothesis_testing_notebook/hypothesis_testing_notebook/","title":"Hypothesis Testing with Python","text":"<ul> <li>Github Notebook link</li> <li>By Joe Ganser</li> </ul>"},{"location":"lectures/stats/hypothesis_testing/hypothesis_testing_notebook/hypothesis_testing_notebook/#exercise-1-t-test-degrees-of-freedom","title":"Exercise 1: t-test degrees of freedom","text":"<p>Using the formula below (which applies to t-tests with unequal degrees of freedom), create a python function that performs this calculation</p> <p></p> <ul> <li> <p>s1 = standard deviation of sample 1</p> </li> <li> <p>n1 = sample 1 size</p> </li> </ul> <p>Calculate the degrees of freedom for:</p> <ul> <li>s1 = 5.32, n1 = 10</li> <li>s2 = 7.03, n2 = 12</li> </ul> <pre><code>def degrees_of_freedom_t_test(s1,n1,s2,n2):\ns1n1 = (s1**2)/n1\ns2n2 = (s2**2)/n2\ntop = (s1n1+s2n2)**2\nbottom = ((s1n1)**2)/(n1-1) + ((s2n2)**2)/(n2-1)\nreturn top/bottom\ndegrees_of_freedom_t_test(5.32,10,7.03,12)\n</code></pre> <pre><code>19.853795244472487\n</code></pre>"},{"location":"lectures/stats/hypothesis_testing/hypothesis_testing_notebook/hypothesis_testing_notebook/#exercise-2-perform-a-two-sample-t-test","title":"Exercise 2: Perform a two sample t-test","text":"<p>Create a python function performs a two sample t-test of unequal variances. Use your result from problem 1 to calculate the degrees of freedom. The function arguments should be;</p> <ul> <li>sample_mean1</li> <li>sample_mean2</li> <li>sample_std1 (standard deviation)</li> <li>sample_std2</li> <li>n1 (sample 1 size)</li> <li>n2</li> <li>tails (the number of tails for the test)</li> <li>alpha (the significance level)</li> </ul> <p>Make the function print the;</p> <ul> <li>p_value</li> <li>test statistic</li> <li>critical value</li> <li>alpha</li> </ul> <p>Make the function <code>return</code> one of two conclusions;</p> <ul> <li><code>Reject null hypothesis</code> (if <code>p_value &lt; alpha</code> and <code>|test_statistic|&gt;|critical_value|</code>)</li> <li>Otherwise, <code>Fail to reject null hypothesis</code></li> </ul> <p>RECALL</p> <p>We calculate the <code>test_statistic</code> by; </p> <p>HINTS</p> <ul> <li><code>p_value</code> can be calculated using <code>(1 - t.cdf(abs(t_statistic), df))</code> FOR EACH TAIL</li> <li><code>critical_value</code> can be calculate using <code>t.ppf(1 - alpha / tails, df)</code></li> </ul> <p>Run this function for the following values (from lecture notes):</p> <ul> <li>sample_mean1 = 22.29</li> <li>sample_mean2 = 16.3</li> <li>sample_std1 = 5.32</li> <li>sample_std2 = 7.03</li> <li>n1 = 10</li> <li>n2 = 12</li> <li>tails = 2</li> <li>alpha = 0.05</li> </ul> <pre><code>from scipy.stats import t\ndef calculate_t_test(sample_mean1, sample_mean2, sample_std1, sample_std2, n1, n2, tails,alpha):\n# Calculate the t-statistic\ndf = round(degrees_of_freedom_t_test(sample_std1,n1,sample_std2,n2))\nstandard_error = ((sample_std1 ** 2) / n1 + (sample_std2 ** 2) / n2) ** 0.5\ntest_statistic = (sample_mean1 - sample_mean2) / (standard_error)\n# Calculate the p-value\np_value = tails*(1 - t.cdf(abs(test_statistic), df))\n# Calculate the critical value\ncritical_value = t.ppf(1 - alpha / tails, df)\nprint('p_value: {}'.format(p_value))\nprint('critical_value: {}'.format(critical_value))\nprint('test_statistic: {}'.format(test_statistic))\nprint('Significance level alpha: {}'.format(alpha))\nif p_value&lt;=alpha:\nif critical_value&lt;0 and test_statistic&lt;critical_value:\nreturn \"Reject Null Hypothesis\"\nelif critical_value&gt;0 and test_statistic&gt;critical_value:\nreturn \"Reject Null Hypothesis\"\nelse:\nreturn \"Fail to Reject Null Hypothesis\"\ncalculate_t_test(22.29, 16.3, 5.32, 7.03, 10, 12, tails=2,alpha=0.05)\n</code></pre> <pre><code>p_value: 0.03425038740869879\ncritical_value: 2.0859634472658364\ntest_statistic: 2.2723574890239813\nSignificance level alpha: 0.05\n\n\n\n\n\n'Reject Null Hypothesis'\n</code></pre>"},{"location":"lectures/stats/hypothesis_testing/hypothesis_testing_notebook/hypothesis_testing_notebook/#exercise-3-plotting-a-t-distribution","title":"Exercise 3: Plotting a t-distribution","text":"<p>Using <code>matplotlib</code> make a plot of the t-distribution for <code>20</code> degrees of freedom.</p> <ul> <li>Define the horizontal values using <code>x = np.linspace(-4,4,500)</code></li> <li>Define the vertical values using <code>t.pdf(horizontal_values, degrees of freedom)</code></li> <li>Use the <code>plt.plot(horizontal_values, vertical_values, label = ....)</code> to plot the graph</li> <li>Give the horizontal axis a label <code>t-score</code></li> <li>Give the vertical axis a label <code>Probability density</code></li> <li>Make a vertical line for the test statistic using <code>plt.axviline(test_statistic,linestyle=':',label='test_statistic')</code></li> <li>Give it a legend using the <code>plt.legend()</code> function</li> <li>Shade the tails using the following;<ul> <li>alpha: <code>plt.fill_between(x, pdf, where=(x &lt; -critical_value) | (x &gt; critical_value),...)</code> give it the color red and an alpha (transparency parameter) value of 0.2 and label of <code>alpha = 0.05</code></li> <li>p-value: <code>plt.fill_between(x, pdf, where=abs(x) &gt; test_statistic, color='darkred',...)</code> transparency parameter= 0.4 and label <code>p_value</code></li> </ul> </li> <li>Show the plot</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import t\n# Set the degrees of freedom\ndf = 20\n# Set the range of x-values\nx = np.linspace(-4, 4, 500)\n# Compute the probability density function (PDF) for the t-distribution\npdf = t.pdf(x, df)\n# Create the plot\nplt.plot(x, pdf, 'b-', label='t-distribution (df=20)')\n# Compute the critical values for alpha = 0.05 (two-tailed test)\nalpha = 0.05\ncritical_value = t.ppf(1 - alpha/2, df)\ntest_statistic = 2.272\n# Shade the critical regions\nplt.fill_between(x, pdf, where=(x &lt; -critical_value) | (x &gt; critical_value), color='red', alpha=0.2, label='Alpha = 0.05')\nplt.fill_between(x, pdf, where=abs(x) &gt; test_statistic, color='darkred', alpha=0.4,label='p-value')\n# Add labels and a legend\nplt.xlabel('t score')\nplt.ylabel('Probability Density')\nplt.title('t-Distribution with 20 Degrees of Freedom')\nplt.axvline(test_statistic,linestyle=':',label='test statistic')\nplt.legend()\n# Display the plot\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/stats/hypothesis_testing/hypothesis_testing_notebook/hypothesis_testing_notebook/#exercise-4-chi2-distribution","title":"Exercise 4: Chi2 distribution","text":"<p>Using the observations from the lecture slides;</p> <pre><code>                      [[60, 40, 30]\n                       [80, 70, 50]\n                       [70, 90, 40]]\n</code></pre> <ul> <li>Perform a chi2 test using the <code>chi2_contingency()</code> function from the <code>scipy.stats</code> package</li> <li>Let <code>alpha=0.05</code></li> <li> <p>Find the <code>critical_value</code> using <code>chi2.ppf(1 - alpha, df)</code> from function from <code>scipy.stats</code></p> </li> <li> <p>Print <code>Reject the null hypothesis</code> if <code>p_value&lt;=alpha</code> &amp; <code>test_statistic&gt;critical_value</code></p> </li> <li>Otherwise, <code>Fail to reject null hypothesis</code></li> </ul> <pre><code>from scipy.stats import chi2_contingency,chi2\n# Observed frequencies (contingency table)\nalpha = 0.05\nobserved_freq = np.array([[60, 40, 30],\n[80, 70, 50],\n[70, 90, 40]])\n# Perform chi-square test of independence\ntest_statistic, p_value, df, expected_freq = chi2_contingency(observed_freq)\ncritical_value = chi2.ppf(1 - alpha, df)\n# Print the chi-square test statistic, degrees of freedom, and p-value\nprint(\"Chi-square statistic:\", test_statistic)\nprint(\"Degrees of freedom:\", dof)\nprint(\"p-value:\", p_value)\nprint('alpha:',alpha)\nprint('critical_value:',critical_value)\nif p_value&lt;=alpha and test_statistic&gt;critical_value:\nprint(\"Reject Null hypothesis\")\nelse:\nprint(\"Fail to reject null hypothesis\")\n</code></pre> <pre><code>Chi-square statistic: 8.469322344322341\nDegrees of freedom: 4\np-value: 0.07582258667376501\nalpha: 0.05\ncritical_value: 9.487729036781154\nFail to reject null hypothesis\n</code></pre> <pre><code>\n</code></pre>"},{"location":"lectures/supervised_learning/KNN_%26_Decision_Trees/KNN_and_Decision_Trees_Notebook/","title":"K-nearest Neighbors &amp; Decision Trees","text":"<p>By Joe Ganser</p> <p>Github Repo link</p> <p>The following data set is the iris flower data set. It basically describes three types the petal sizes of several species of flowers. The graphs are plots of different features from these data sets. See the extra code file on how to generate this plot.</p> <pre><code>import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.datasets import load_iris\nimport pandas as pd\niris = load_iris(as_frame=True)\ndata = pd.concat([iris['data'],iris['target']],axis=1)\nX = data.drop('target',axis=1)\ny = data['target']\ndata.head()\n</code></pre> sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 5.1 3.5 1.4 0.2 0 4.9 3.0 1.4 0.2 0 4.7 3.2 1.3 0.2 0 4.6 3.1 1.5 0.2 0 5.0 3.6 1.4 0.2 0 make_figure.py<pre><code>from itertools import combinations\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nimport pandas as pd\niris = load_iris(as_frame=True)\ndata = pd.concat([iris['data'],iris['target']],axis=1)\nX = data.drop('target',axis=1)\ny = data['target']\ndef compare_x_y_iris(data,x_axis,y_axis,legend=False):\ncolors = {'setosa': (0,'red'), 'versicolor': (1,'green'), 'virginica': (2,'blue')}\nfor key,values in colors.items():\nplt.scatter(x=data[data['target']==values[0]][x_axis], y=data[data['target']==values[0]][y_axis], c=values[1], label=key)\nplt.xlabel(x_axis,fontsize=15)\nplt.ylabel(y_axis,fontsize=15)\nif legend:\nplt.legend(fontsize=12)\ncols = [j for j in list(data.columns) if j!='target']\ncombos = list(combinations(cols, 2))\nn=1\nplt.figure(figsize=(20,20*(2/3)))\nfor c in combos:\nplt.subplot(2,3,n)\nif n==1:\nlegend=True\nelse:\nlegend=False\ncompare_x_y_iris(data,c[0],c[1],legend)\nn=n+1\nplt.savefig('iris_plots.png')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/supervised_learning/KNN_%26_Decision_Trees/KNN_and_Decision_Trees_Notebook/#example-1-k-nearest-neighbors-knn","title":"Example 1: K-Nearest Neighbors (KNN)","text":"<ul> <li>Using KNN on the iris data set with visualization and grid search.</li> <li>Put the features all on the same scale, using <code>StandardScaler()</code> to help with modelling</li> <li>Split the iris data set into a train and test set</li> <li>Put the train set into the grid search<ul> <li>Use different values for <code>neighbors</code>, <code>weights</code> and <code>p</code> in the <code>GridSearchCV</code></li> <li>print the best parameters</li> </ul> </li> <li>Using the best model from grid search, evaluate it's accuracy on the test set </li> </ul> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nX = StandardScaler().fit_transform(X)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nneighbors = [3,5,7,10]\nweights = ['uniform','distance']\np=[1,2]\nknn_parameters = {'n_neighbors':neighbors,'weights':weights,'p':p}\ngs_knn = GridSearchCV(estimator=KNeighborsClassifier(),param_grid=knn_parameters,cv=3)\ngs_knn.fit(X_train,y_train)\nprint(gs_knn.best_params_)\nknn_best = KNeighborsClassifier(**gs_knn.best_params_)\nknn_best.fit(X_train,y_train)\ny_pred_knn = knn_best.predict(X_test)\nprint(accuracy_score(y_pred_knn,y_test))\n</code></pre> <pre><code>{'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n0.9333333333333333\n</code></pre>"},{"location":"lectures/supervised_learning/KNN_%26_Decision_Trees/KNN_and_Decision_Trees_Notebook/#example-2-decisison-trees","title":"Example 2: Decisison Trees","text":"<ul> <li>Using decision tree classifier on the iris data set</li> <li>Perform the same task as example 1, except this time search through values of <code>criterion</code>, <code>max_depth</code>,<code>max_features</code></li> <li>Print the best decison tree parameters</li> <li>Using the best decision tree model from <code>GridSearchCV</code>, evaluate it's accuracy on the test data. </li> <li>Visualizing the decision tree classifier using<ul> <li>text representation</li> <li>visual graphic representation</li> </ul> </li> </ul> <pre><code>from sklearn.tree import DecisionTreeClassifier\ncriterion=[\"gini\",\"entropy\",\"log_loss\"]\nmax_depth = [3,5,10,None]\nmax_features = [None,2,\"log2\",\"auto\",\"sqrt\"]\ndt_parameters = {'criterion':criterion,'max_depth':max_depth,'max_features':max_features}\ngs_dt = GridSearchCV(estimator=DecisionTreeClassifier(),param_grid=dt_parameters,cv=3)\ngs_dt.fit(X_train,y_train)\nprint(gs_dt.best_params_)\ndt_best = DecisionTreeClassifier(**gs_dt.best_params_)\ndt_best.fit(X_train,y_train)\ny_pred_dt = dt_best.predict(X_test)\nprint(accuracy_score(y_pred_dt,y_test))\n</code></pre> <pre><code>{'criterion': 'gini', 'max_depth': 3, 'max_features': None}\n0.9\n</code></pre> <pre><code>from sklearn import tree\ntext_representation = tree.export_text(dt_best,feature_names=iris.feature_names)\nprint(text_representation)\n</code></pre> <pre><code>|--- petal length (cm) &lt;= -0.74\n|   |--- class: 0\n|--- petal length (cm) &gt;  -0.74\n|   |--- petal width (cm) &lt;= 0.72\n|   |   |--- petal length (cm) &lt;= 0.90\n|   |   |   |--- class: 1\n|   |   |--- petal length (cm) &gt;  0.90\n|   |   |   |--- class: 2\n|   |--- petal width (cm) &gt;  0.72\n|   |   |--- class: 2\n</code></pre> <pre><code>fig = plt.figure(figsize=(25,20))\ndt_plot = tree.plot_tree(dt_best, feature_names=iris.feature_names,  class_names=iris.target_names,filled=True)\nplt.show()\n</code></pre>"},{"location":"lectures/supervised_learning/KNN_%26_Decision_Trees/KNN_and_Decision_Trees_Notebook/#sources","title":"sources","text":"<ul> <li>https://mljar.com/blog/visualize-decision-tree/</li> </ul>"},{"location":"lectures/supervised_learning/KNN_%26_Decision_Trees/KNN_and_Decision_Trees_Slides/","title":"Slides","text":""},{"location":"lectures/supervised_learning/Regression_part1/Regression_part1/","title":"Regression part 1","text":"<p>By Joe Ganser</p> <p>Github Repo Link</p>"},{"location":"lectures/supervised_learning/Regression_part1/Regression_part1/#plotting-regression","title":"Plotting Regression","text":"<p>A typical ordinary least squares plot graphic;</p> <pre><code>* Y vs X\n* The function for the line of Y\n* R2 score\n* MSE value\n* Axis labels\n</code></pre> <p>See the attached python file to see how to plot this example</p> make_figure.py<pre><code>import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom scipy import stats\nfrom sklearn.metrics import r2_score,mean_squared_error\nX,y = make_regression(n_samples=1000, n_features=1, n_informative=1, n_targets=1,noise=15,bias=0)\nX = X+3\ny = y+200\nplt.figure(figsize=(7,7))\nplt.scatter(X,y)\nslope, intercept, r_value, p_value, std_err = stats.linregress(X.reshape(1,-1),y)\nline = slope*X+intercept\nif intercept&gt;0:\nplusminus='+'\nelif intercept&lt;0:\nplusminus=' '\nplt.plot(X,line,color='red',label='OLS line;\\n y={0}*X{1}{2}'.format(round(slope,2),plusminus,round(intercept,2)))\nr2 = round(r2_score(y,line),3)\nmse = round(mean_squared_error(y,line),3)\nplt.title('Fish length vs weight \\n R2={}, MSE={}'.format(r2,mse),fontsize=20)\nplt.ylabel('Y axis: Length (cm)',fontsize=20)\nplt.xlabel('X Axis: Weight (kg)',fontsize=20)\nplt.legend(fontsize=20)\nplt.savefig('fish_length_weight.png')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/supervised_learning/Regression_part1/Regression_part1/#example-1-plotting-regression","title":"Example 1: Plotting Regression","text":"<p>Using the following saved data to make the plot above</p> <p><code>import pandas as pd df = pd.read_csv('example1_fish_weight.csv') X = df['weight(kg)'] y = df['length(cm)']</code></p> <ul> <li>Use the <code>scipy.stats.linregress</code> method for producing the ordinary least squares line</li> <li>get the <code>r2_score</code>, <code>mean_squared_error</code> to find the <code>r2</code> and <code>rmse</code> metrics</li> <li>plot the <code>OLS</code> line</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport numpy as np\nimport pandas as pd\ndf = pd.read_csv('example1_fish_weight.csv')\nX = df['weight(kg)']\ny = df['length(cm)']\nplt.figure(figsize=(7,7))\nplt.scatter(X,y)\nslope, intercept, r_value, p_value, std_err = stats.linregress(X,y)\nline = slope*X+intercept\nplt.plot(X,line,color='red',label='OLS line;\\n y={0}*X+{1}'.format(round(slope,2),round(intercept,2)))\nr2 = round(r2_score(y,line),3)\nrmse = round(np.sqrt(mean_squared_error(y,line)),3)\nplt.title('Fish length vs weight \\n R2={}, RMSE={}'.format(r2,rmse),fontsize=20)\nplt.ylabel('Y axis: Length (cm)',fontsize=20)\nplt.xlabel('X Axis: Weight (kg)',fontsize=20)\nplt.legend(fontsize=20)\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/supervised_learning/Regression_part1/Regression_part1/#example-2-regression-on-train-test-split","title":"Example 2: Regression on train test split","text":"<p>Repeating example 1, but this time performing a train-test split and using <code>LinearRegression</code> from <code>sklearn.linear_model</code></p> <ul> <li> <p>For the train and test set, print</p> <ul> <li><code>r2_test</code>, <code>r2_train</code></li> <li><code>rmse_test</code>, <code>rmse_train</code></li> </ul> </li> <li> <p>from the <code>LinearRegression</code> model, print the;</p> <ul> <li>slope value of OLS using <code>.coef_</code> attribute</li> <li>intercept value using <code>.intercept_</code> attribute</li> </ul> </li> </ul> <p>output should look like this</p> <p><code>r2_train:  0.94 rmse_train:  15.523 r2_test:  0.94 rmse_test:  15.745 slope:  [63.37004067] intercept:  16.582701899567525</code></p> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nX_train = np.array(X_train).reshape(X_train.shape[0],1)\nX_test = np.array(X_test).reshape(X_test.shape[0],1)\nols = LinearRegression().fit(X_train,y_train)\ndef report(model,data,target,label):\npred = model.predict(data)\nr2 = round(r2_score(pred,target),3)\nrmse = round(np.sqrt(mean_squared_error(pred,target)),3)\nprint('r2_{}: '.format(label),r2)\nprint('rmse_{}: '.format(label),rmse)\nreport(ols,X_train,y_train,'train')\nreport(ols,X_test,y_test,'test')\nprint('slope: ',ols.coef_)\nprint('intercept: ',ols.intercept_)\n</code></pre> <pre><code>r2_train:  0.94\nrmse_train:  15.523\nr2_test:  0.94\nrmse_test:  15.745\nslope:  [63.37004067]\nintercept:  16.582701899567525\n</code></pre>"},{"location":"lectures/supervised_learning/Regression_part1/Regression_part1/#example-3-plotting-predictions-versus-observations","title":"Example 3: Plotting Predictions versus Observations","text":"<p>Develop a scatter plot of the predictions versus the observations for the train set and test set to describe model performance. * Include the ols line equations as well as the r2 score * Plot should look like this; * Do we see homoskedascity?</p> <p></p> <pre><code>y_pred_train = ols.predict(X_train)\ny_pred_test = ols.predict(X_test)\ndef predictions_vs_observations(observations,predictions,label):\nslope, intercept, r_value, p_value, std_err = stats.linregress(observations,predictions)\nplt.title('predictions vs observations on {0} set \\n r2 score: {1}'.format(label,round(r_value,4)))\nplt.scatter(observations,predictions)\nplt.ylabel('predictions')\nplt.xlabel('observations')\nline = slope*observations+intercept\nplt.plot(observations,line,color='red',label='OLS line;\\n y={0}*X+{1}'.format(round(slope,2),round(intercept,2)))\nplt.legend()\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\npredictions_vs_observations(y_pred_train,y_train,'train')\nplt.subplot(1,2,2)\npredictions_vs_observations(y_pred_test,y_test,'test')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/supervised_learning/Regression_part1/Regression_part1/#example-4-error-histograms-qq-plots","title":"Example 4: Error Histograms &amp; QQ plots","text":"<p>Make a plot of the prediction errors on the train set and and the test set as seen below. Include the histogram for the errors and q-q plot for the errors.</p> <p>Are the errors normally distributed?</p> <p></p> <pre><code>error_train = y_pred_train-y_train\nerror_test = y_pred_test-y_test\ndef hist_qq_plot(error_train,error_test):\nplt.figure(figsize=(10,10))\nplt.subplot(2,2,1)\nplt.hist(error_train)\nplt.title('Histogram of train errors')\nplt.subplot(2,2,2)\nstats.probplot(error_train,plot=plt)\nplt.title('Q-Q plot of train errors')\nplt.xlabel('')\nplt.ylabel('')\nplt.subplot(2,2,3)\nplt.hist(error_test)\nplt.title('Histogram of test errors')\nplt.subplot(2,2,4)\nstats.probplot(error_test,plot=plt)\nplt.title('Q-Q plot of test errors')\nplt.xlabel('')\nplt.ylabel('')\nplt.subplots_adjust(wspace=0.3,hspace=0.3)\nplt.savefig('example4.png')\nplt.show()\nhist_qq_plot(error_train,error_test)\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"lectures/supervised_learning/Regression_part1/Regression_part1_slides/","title":"Slides","text":""},{"location":"lectures/supervised_learning/Regression_part2/Regression_part2/","title":"Regression part 2: Advanced Techniques","text":"<p>By Joe Ganser</p> <p>github link</p>"},{"location":"lectures/supervised_learning/Regression_part2/Regression_part2/#example-1-ols-ridge-lasso","title":"Example 1:  OLS, Ridge &amp; Lasso","text":"<p>Fit ordinary least squares, Ridge regression and Lasso regression to the following data set. (dont worry about tuning hyperparamters for ridge and lasso)</p> <ul> <li>Print their coefficients for each model</li> <li>Use the models to make predictions on the data set</li> <li>Perform the predicted calculations 'by hand' for row zero of the data set.</li> </ul> <pre><code>import pandas as pd\ndf = pd.read_csv('lecture12_example1.csv',index_col=0)\nX = df.drop('arm_length',axis=1)\ny = df['arm_length']\ndf.head()\n</code></pre> age height weight male arm_length 23.3 163 100 1 22.3 34.0 190 95 0 24.3 17.0 180 143 1 22.1 <pre><code>from sklearn.linear_model import Ridge,Lasso,LinearRegression,ElasticNet\nfor m in [Ridge,Lasso,ElasticNet,LinearRegression]:\nm = m()\nm.fit(X,y)\ny_pred = m.predict(X)\nprint(m)\nprint('coefficients: ',m.coef_)\nprint('intercept: ',m.intercept_)\nprint('predictions: ',y_pred)\nprint('\\n')\n</code></pre> <pre><code>Ridge()\ncoefficients:  [ 0.02695934  0.05870384 -0.02395133 -0.00235268]\nintercept:  14.502675925812566\npredictions:  [22.30206924 24.29764732 22.10028344]\n\n\nLasso()\ncoefficients:  [ 0.          0.0599208  -0.02923667 -0.        ]\nintercept:  15.548069696525761\npredictions:  [22.39149279 24.15553771 22.1529695 ]\n\n\nElasticNet()\ncoefficients:  [ 0.          0.06380857 -0.03038895 -0.        ]\nintercept:  14.987166308050938\npredictions:  [22.34906773 24.22384384 22.12708843]\n\n\nLinearRegression()\ncoefficients:  [ 0.02701024  0.05884593 -0.02395852 -0.00235768]\nintercept:  14.476984164678683\npredictions:  [22.3 24.3 22.1]\n</code></pre>"},{"location":"lectures/supervised_learning/Regression_part2/Regression_part2/#example-2-grid-searching-for-hyperparameters","title":"Example 2: Grid Searching for hyperparameters","text":"<p>Using the dataset, <code>lecture12_example2.csv</code> use <code>GridSearchCV</code> to test different values of <code>alpha</code> for <code>Ridge()</code>, <code>Lasso()</code>, <code>ElasticNet()</code>.</p> <ul> <li>Let <code>alpha=np.arange(0,1,0.01)</code></li> <li>For the case of <code>ElasticNet()</code>, use <code>alpha</code> and also use <code>l1_ratio=np.arange(0,1,0.01)</code></li> </ul> <pre><code>import pandas as pd\ndata2 = pd.read_csv('lecture12_example2.csv',index_col=0)\nX2 = data2.drop('target',axis=1)\ny2 = data2['target']\ndata2.head()\n</code></pre> 0 1 2 3 4 5 6 7 8 9 ... 301 302 303 304 305 306 307 308 309 target 0 2.330870 -1.612841 1.339116 -2.430704 -0.093109 -1.316382 -0.457527 0.462286 0.810499 0.572844 ... 1.376158 0.542736 -1.411426 -1.881519 -0.733447 0.651879 -0.204618 1.350441 0.743563 1 0.027808 -0.679261 0.871705 0.745946 0.688176 -1.963766 -0.254602 1.559677 1.727085 1.680125 ... 0.651409 -1.883939 0.600215 -0.506214 0.083978 0.804308 0.090795 -0.573402 -0.865638 2 1.186985 -0.130896 1.458549 -0.629930 -3.175641 1.135818 -1.184185 2.579488 2.357140 0.128912 ... 0.612371 0.840436 -2.393615 0.320818 0.158502 -0.701265 0.641657 -0.303029 0.078271 3 -1.614443 0.410332 0.557796 -0.273931 2.244855 0.303914 0.957551 0.596357 -0.297291 -1.785487 ... 0.027101 -1.205863 -0.781089 -0.557228 1.682864 0.588004 0.771023 1.279060 0.373258 4 -0.397127 -2.174227 1.210069 0.805470 2.251384 -0.513468 -1.572685 -0.690919 -0.549232 0.203158 ... -1.027788 -0.478835 1.215282 0.148531 -0.449885 0.926758 -0.275255 -0.460199 -0.826239 <p>5 rows \u00d7 311 columns</p> <pre><code>import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nalpha = np.arange(0,1,0.01)\nfor m in [Ridge,Lasso,ElasticNet]:\nif m!=ElasticNet:\nparameters = {'alpha':alpha}\nelse:\nparameters = {'alpha':alpha,'l1_ratio':alpha}\ngs = GridSearchCV(m(),parameters)\ngs.fit(X2,y2)\nprint(gs.best_estimator_)\nprint(gs.best_score_)\nprint('\\n')\n</code></pre> <pre><code>Ridge(alpha=0.0)\n0.603656993865098\n\n\nLasso(alpha=0.03)\n0.4571337721470381\n\n\nElasticNet(alpha=0.08, l1_ratio=0.84)\n0.6041495435590333\n</code></pre>"},{"location":"lectures/supervised_learning/Regression_part2/Regression_part2/#example-3-grid-search-stochastic-gradient-descent","title":"Example 3: Grid Search &amp; Stochastic Gradient Descent","text":"<ul> <li><code>SGDRegressor</code> can use <code>Ridge</code>, <code>Lasso</code> or <code>ElasticNet</code> as it's regression protocol.</li> <li>Let the parameters cycled through be <ul> <li><code>sgd_params = {'penalty':['l1','l2'],'alpha':np.arange(0,1,0.1),'l1_ratio':np.arange(0,1,0.1)}</code></li> </ul> </li> </ul> <pre><code>import pandas as pd\nimport numpy as np\ndf3 = pd.read_csv('lecture12_example3.csv',index_col=0)\nX3 = df3.drop('target',axis=1)\ny3 = df3['target']\ndf3.head()\n</code></pre> 0 1 2 3 4 5 6 7 target 0 0.440476 -0.539437 -0.741995 0.222795 -0.174341 0.286436 -0.930370 0.270159 1 -1.194987 -0.162277 -1.511640 0.726551 1.905424 -0.084078 -0.075408 -1.097597 2 -0.754477 -1.311020 0.494098 -1.356511 -0.893744 -1.901495 -0.354791 -1.292560 3 -1.551001 0.273258 0.111347 -0.161415 -0.491677 -1.181606 -0.669466 -3.642809 4 1.270708 -0.786369 -0.577232 1.197173 0.013121 0.612230 0.011464 2.359498 <pre><code>from sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import GridSearchCV\nsgd_params = {'penalty':['l1','l2'],'alpha':np.arange(0,1,0.1),'l1_ratio':np.arange(0,1,0.1)}\ngs_sgd = GridSearchCV(SGDRegressor(loss='squared_loss'),param_grid=sgd_params)\ngs_sgd.fit(X3,y3)\nprint(gs_sgd.best_estimator_)\nprint(gs_sgd.best_score_)\n</code></pre> <pre><code>SGDRegressor(alpha=0.0, l1_ratio=0.7000000000000001)\n0.9999999999763147\n</code></pre>"},{"location":"lectures/supervised_learning/Regression_part2/Regression_part2_slides/","title":"Slides","text":""},{"location":"lectures/supervised_learning/classification_part1/Classification_part1/","title":"Classification Part 1","text":"<p>by Joe Ganser</p> <ul> <li>Github repo link</li> </ul>"},{"location":"lectures/supervised_learning/classification_part1/Classification_part1/#step-1-loading-our-data-and-examining-what-it-looks-like","title":"Step 1: Loading our data, and examining what it looks like","text":"<ul> <li>This is the famous \"iris\" data set from the UCI Machine learning repository.</li> <li>It describes a the flower petal width and lengths for three different flower species.</li> <li>Load the data and examine the first 5 columns<ul> <li>columns names are; <code>columns = ['sepal_length','sepal_width','petal_length','petal_width']</code></li> </ul> </li> </ul> <pre><code>from sklearn import datasets\nimport pandas as pd\niris = datasets.load_iris()\ncolumns = ['sepal_length','sepal_width','petal_length','petal_width']\nX = pd.DataFrame(iris[\"data\"],columns=columns)\ny = pd.DataFrame(iris[\"target\"],columns=['target'])\ndata = pd.concat([X,y],axis=1)\ndata.head()\n</code></pre> sepal_length sepal_width petal_length petal_width target 5.1 3.5 1.4 0.2 0 4.9 3.0 1.4 0.2 0 4.7 3.2 1.3 0.2 0 4.6 3.1 1.5 0.2 0 5.0 3.6 1.4 0.2 0"},{"location":"lectures/supervised_learning/classification_part1/Classification_part1/#step-2-eda","title":"Step 2: EDA","text":"<p>Let's get a value count of each class, to see their distribution</p> <ul> <li>Are these classes balanced?</li> <li>How many values are there for each class?</li> </ul> <pre><code>data['target'].value_counts()\n</code></pre> <pre><code>0    50\n1    50\n2    50\nName: target, dtype: int64\n</code></pre>"},{"location":"lectures/supervised_learning/classification_part1/Classification_part1/#step-3-simple-model","title":"Step 3: Simple Model","text":"<p>Lets fit a simple logistic regression model to the data, and see the over all accuracy score</p> <ul> <li>Load the data<ul> <li>Dont worry about train/test/split</li> </ul> </li> <li>Instantiate the <code>LogisticRegression</code> class</li> <li>Fit the <code>X,y</code> data to it.</li> <li>Use the fitted model to make predictions</li> <li>Using the predictions, find the <code>accuracy_score</code></li> </ul> <p>Question: Does the score mean anything if we havent split the data?</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\nX = data.drop('target',axis=1)\ny = data['target']\nlog_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\")\nlog_reg.fit(X,y)\ny_pred = log_reg.predict(X)\nprint(accuracy_score(y,y_pred))\n</code></pre> <pre><code>0.9733333333333334\n</code></pre>"},{"location":"lectures/supervised_learning/classification_part1/Classification_part1/#step-4-one-vs-rest-classification","title":"Step 4: \"One vs Rest classification\"","text":"<p>Lets re-do example 3, except this time focusing only on classifying everything as either class 2, or not. </p> <ul> <li>This makes it a binary classification problem</li> <li>Use only the 3rd column (<code>petal_length</code>) of the data set to make predictions.</li> <li>Also, lets visualize the predicted probability of it being class 2 as a function of the petal/sepal length/width.<ul> <li>Plot the predicted probability of being class 2 versus the petal length</li> <li>Use the <code>predict_proba</code> function on the <code>LogisticRegression()</code> function to do the calculations.</li> <li>The petal width bounds between 0 and the max value of that feature<ul> <li>What does this mean?</li> </ul> </li> </ul> </li> </ul> <pre><code>from sklearn import datasets\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\niris = datasets.load_iris()\nX = np.array(data['petal_length']).reshape(-1,1)\ny = data['target'].apply(lambda x: 1 if x==2 else 0).values\nlog_reg = LogisticRegression()\nlog_reg.fit(X,y)\nX_new = np.linspace(0,X.max(),1000).reshape(-1,1)\ny_proba = log_reg.predict_proba(X_new)\nplt.figure(figsize=(7,7))\nplt.plot(X_new,y_proba[:,1],\"g--\",label=\"Iris-Virginica (class 2)\")\nplt.plot(X_new,y_proba[:,0],\"b--\",label=\"Not Iris-Virginica (class 1 or 3)\")\nplt.xlabel('petal length')\nplt.legend(loc='center left')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/supervised_learning/classification_part1/Classification_part1/#step-5-train-test-split","title":"Step 5: Train test Split","text":"<p>Split the data set into train, test and validation sets</p> <ul> <li>train set: 70% of data</li> <li>test set: 15% of data</li> <li> <p>validation set: 15% of data</p> </li> <li> <p>Use an artificially generated data set;</p> </li> </ul> <p><code>from sklearn.datasets import make_classification X,y = make_classification(n_samples=1000, n_features=20, n_informative=18, n_redundant=2, n_repeated=0,                            n_classes=3)</code></p> <pre><code>from sklearn.datasets import make_classification\nX,y = make_classification(n_samples=1000, n_features=20, n_informative=18, n_redundant=2, n_repeated=0, \nn_classes=3)\n# perform train test val split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\nX_test,X_val,y_test,y_val = train_test_split(X_test,y_test,test_size=0.5)\n</code></pre>"},{"location":"lectures/supervised_learning/classification_part1/Classification_part1/#step-6-cross-validation","title":"Step 6: Cross Validation","text":"<p>Use cross validation to split our dataset into several 5 folds, and evaluate how a simple logistic regression classifier does on each fold</p> <pre><code>from sklearn.datasets import make_classification\nX,y = make_classification(n_samples=1000, n_features=20, n_informative=18, n_redundant=2, n_repeated=0, \nn_classes=3)\n# perform train test val split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(LogisticRegression(),X_train,y_train,cv=5)\n</code></pre> <pre><code>array([0.60714286, 0.70714286, 0.56428571, 0.62142857, 0.6       ])\n</code></pre>"},{"location":"lectures/supervised_learning/classification_part1/Classification_part1/#step-7-grid-searching","title":"Step 7: Grid searching","text":"<p>Use grid search on the data set in example 6 to find the best value of c in the ranges of <code>[0.0001,0.001,0.01,0.1,1,10]</code></p> <pre><code>from sklearn.datasets import make_classification\nX,y = make_classification(n_samples=1000, n_features=20, n_informative=18, n_redundant=2, n_repeated=0, \nn_classes=3)\n# perform train test val split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'C':[0.0001,0.001,0.01,0.1,1,10]}\ngs = GridSearchCV(LogisticRegression(),parameters,cv=5)\ngs.fit(X_train,y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)\n</code></pre> <pre><code>0.6857142857142857\n{'C': 0.001}\n</code></pre>"},{"location":"lectures/supervised_learning/classification_part1/Classification_part1/#step-8-testing-hyperparameters","title":"Step 8:  Testing hyperparameters","text":"<p>Using example 5 data, run through a series of hyper parameters (values of <code>c</code>) for <code>LogisticRegression()</code> and find the value of <code>c</code> that fits the best score on the train set, test set and validation set.</p> <ul> <li><code>c</code> is the hyper parameter of <code>LogisticRegression()</code></li> <li>Since the set had 3 classes, let the model by <code>LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",C=c)</code></li> <li>Loop through possible values of <code>c</code>, finding the best one<ul> <li>This means we want to minimize the score difference between the train, val and test sets</li> <li>At each value of <code>c</code>, save the scores values to the dictionary</li> </ul> </li> </ul> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nX,y = make_classification(n_samples=1000, n_features=20, n_informative=18, n_redundant=2, n_repeated=0, \nn_classes=3)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\nX_test,X_val,y_test,y_val = train_test_split(X_test,y_test,test_size=0.5)\nscores = {}\nmax_score = 0\nmin_diff = 100\nfor c in [0.0001,0.001,0.01,0.1,1,10]:\nlog_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",C=c)\nlog_reg.fit(X_train,y_train)\ny_pred = log_reg.predict(X_val)\ny_pred_train = log_reg.predict(X_train)\ntrain_score = accuracy_score(y_pred_train,y_train)\nval_score = accuracy_score(y_pred,y_val)\nscores[c] = (val_score,train_score,np.abs(val_score-train_score))\nif val_score&gt;max_score:\nmax_score = val_score\nif np.abs(val_score-train_score)&lt;min_diff:\nmin_diff = np.abs(val_score-train_score)\nprint(\"Best validation score: \",max_score)\nc_best = [j for j in scores.keys() if scores[j][0]==max_score][0]\nlog_reg_best = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",C=c_best)\nlog_reg_best.fit(X_train,y_train)\ny_pred_test = log_reg.predict(X_test)\nprint(\"Best C value: \",c_best)\n</code></pre> <pre><code>Best validation score:  0.7066666666666667\nBest C value:  0.1\n</code></pre>"},{"location":"lectures/supervised_learning/classification_part1/Classification_part1_slides/","title":"Slides","text":""},{"location":"lectures/supervised_learning/classification_part2/Classification_part2/","title":"Classification Part 2: Metrics","text":"<p>By Joe Ganser</p> <ul> <li>Github Repo</li> </ul>"},{"location":"lectures/supervised_learning/classification_part2/Classification_part2/#example-1-confusion-matrics-classification-report","title":"Example 1: Confusion Matrics &amp; Classification Report","text":"<p>Working with confusion matrices, classification_report, loss functions and cross entropy</p> <p>Suppose our observations are;</p> <p><code>predicted_probabilies = np.array([[0.25,0.75],[0.05,0.95],[0.25,0.75],[0.15,0.85]])</code></p> <p><code>predictions = np.array([0,0,0,0])</code></p> <p><code>observations = np.array([1,0,0,0])</code></p> <ul> <li>Put these into a confusion matrix and classification report</li> <li>Put these into the log loss function</li> </ul> <p><pre><code>import numpy as np\npredicted_probabilies = np.array([[0.25,0.75],[0.05,0.95],[0.25,0.75],[0.15,0.85]])\npredictions = np.array([0,0,0,0])\nobservations = np.array([1,0,0,0])\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(predictions,observations))\ntn, fp, fn, tp = confusion_matrix(predictions,observations).ravel()\nprint(tn,fp,fn,tp)\n</code></pre>                   precision    recall  f1-score   support</p> <pre><code>           0       1.00      0.75      0.86         4\n           1       0.00      0.00      0.00         0\n\n    accuracy                           0.75         4\n   macro avg       0.50      0.38      0.43         4\nweighted avg       1.00      0.75      0.86         4\n\n3 1 0 0\n</code></pre> <p>Print the confusion matrix</p> <pre><code>from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(predictions,observations))\n</code></pre> <pre><code>[[3 1]\n [0 0]]\n</code></pre> <p>Use the confusion matrix to print the count of the true negatives, false positives, false negatives and true positives </p> <pre><code>tn, fp, fn, tp = confusion_matrix(predictions,observations).ravel()\nprint(tn,fp,fn,tp)\n</code></pre> <pre><code>3 1 0 0\n</code></pre> <p>Print the classification report</p> <pre><code>from sklearn.metrics import classification_report\nprint(classification_report(predictions,observations))\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n           0       1.00      0.75      0.86         4\n           1       0.00      0.00      0.00         0\n\n    accuracy                           0.75         4\n   macro avg       0.50      0.38      0.43         4\nweighted avg       1.00      0.75      0.86         4\n</code></pre> <p>Print the log loss</p> <pre><code>from sklearn.metrics import log_loss\nlog_loss(observations,predicted_probabilies)\n</code></pre> <pre><code>1.6417071730028858\n</code></pre>"},{"location":"lectures/supervised_learning/classification_part2/Classification_part2/#example-2-grid-search-with-imbalanced-classes","title":"Example 2: Grid Search with imbalanced classes","text":"<p>Using grid search CV to fit logistic regression for imbalanced classes</p> <ul> <li>Print the first 5 rows of the data set</li> <li>Print the percentage break down of each class in the dataset<ul> <li>Do the grid search on a <code>train</code> set</li> <li>Evaluate best model on a <code>test</code> set</li> </ul> </li> <li>Grid search through a range of hyper parameters to find the best ones that maximize the <code>F1_score</code> metric<ul> <li><code>parameters=[0.0001,0.001,0.01,0.1,1,10,100]</code></li> </ul> </li> <li> <p>Print the <code>best_score_</code> and <code>best_params_</code> attribute of <code>GridSearchCV</code></p> </li> <li> <p>Dataset link: https://raw.githubusercontent.com/JoeGanser/Datasets-1/master/pima-indians-diabetes.csv</p> </li> <li>columns = <code>['times pregnant','glucose_concentration','blood-pressure','skinfold-thickness','serum-insulin','BMI','diabetes-function','age','class']</code></li> </ul> <pre><code>columns = ['times pregnant','glucose_concentration','blood-pressure','skinfold-thickness',\n'serum-insulin','BMI','diabetes-function','age','class']\ndata1 = pd.read_csv('https://raw.githubusercontent.com/JoeGanser/Datasets-1/master/pima-indians-diabetes.csv',names=columns)\nX = data1.drop('class',axis=1)\ny = data1['class']\nprint(data1.shape)\ndata1.head()\n</code></pre> <pre><code>(768, 9)\n</code></pre> times pregnant glucose_concentration blood-pressure skinfold-thickness serum-insulin BMI diabetes-function age class 6 148 72 35 0 33.6 0.627 50 1 1 85 66 29 0 26.6 0.351 31 0 8 183 64 0 0 23.3 0.672 32 1 1 89 66 23 94 28.1 0.167 21 0 0 137 40 35 168 43.1 2.288 33 1 <p><pre><code>100*data1['class'].value_counts()/len(data1)\n</code></pre>     0    65.104167     1    34.895833     Name: class, dtype: float64</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import make_scorer,f1_score\nfrom sklearn.model_selection import GridSearchCV\nlogreg = LogisticRegression()\nparameters=[0.0001,0.001,0.01,0.1,1,10,100]\nparameters = {'C':parameters}\nf1 = make_scorer(f1_score, greater_is_better=True)\ngs = GridSearchCV(logreg, parameters,scoring=f1)\ngs.fit(X, y)\nprint(gs.best_score_)\nprint(gs.best_params_)\n</code></pre> <pre><code>0.6383289882125565\n{'C': 10}\n</code></pre>"},{"location":"lectures/supervised_learning/classification_part2/Classification_part2/#example-3-making-a-classification-model-report","title":"Example 3: Making a classification model report","text":"<p>Using the model from example 2, split the data set from example 2 into a train and test set. Using the best parameters from the <code>GridSearchCV</code>, generate the following metrics (for both the train set and test set!) * Classification report * f1 score * accuracy score * confusion matrix graph * roc-auc curve * auc metric</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score,roc_curve,auc\nX = data1.drop('class',axis=1)\ny = data1['class']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\nlogreg = LogisticRegression(C=10)\nlogreg.fit(X_train,y_train)\ny_pred_test = logreg.predict(X_test)\ny_pred_prob_test = logreg.predict_proba(X_test)\ny_pred_train = logreg.predict(X_train)\ny_pred_prob_train = logreg.predict_proba(X_train)\n</code></pre> <p>Classification Reports</p> <pre><code>print('Classification report train set \\n ')\nprint(classification_report(y_pred_train,y_train))\nprint('Classification report test set \\n ')\nprint(classification_report(y_pred_test,y_test))\n</code></pre> <pre><code>Classification report train set\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.81      0.84       378\n           1       0.62      0.74      0.67       159\n\n    accuracy                           0.79       537\n   macro avg       0.75      0.77      0.76       537\nweighted avg       0.80      0.79      0.79       537\n\nClassification report test set\n\n              precision    recall  f1-score   support\n\n           0       0.86      0.80      0.83       164\n           1       0.58      0.67      0.62        67\n\n    accuracy                           0.76       231\n   macro avg       0.72      0.74      0.72       231\nweighted avg       0.78      0.76      0.77       231\n</code></pre> <p>Accuracy Score</p> <pre><code>print('Accuracy score train set \\n ')\nprint(accuracy_score(y_pred_train,y_train))\nprint('Accuracy Score test set \\n ')\nprint(accuracy_score(y_pred_test,y_test))\n</code></pre> <pre><code>Accuracy score train set\n\n0.7858472998137802\nAccuracy Score test set\n\n0.7619047619047619\n</code></pre>"},{"location":"lectures/supervised_learning/classification_part2/Classification_part2/#example-4-roc-auc-plots","title":"Example 4: ROC-AUC plots","text":"<p>ROC-AUC plot for train and test sets</p> <pre><code>y_pred_prob_test = y_pred_prob_test[:,1]\nfalse_positive_rate_test, true_positive_rate_test, thresholds = roc_curve(y_test, y_pred_prob_test)\nroc_auc_test = auc(false_positive_rate_test, true_positive_rate_test)\nplt.figure(figsize=(5,5))\nplt.title('Receiver Operating Characteristic Test Set')\nplt.plot(false_positive_rate_test,true_positive_rate_test, color='red',label = 'AUC = %0.2f' % roc_auc_test)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n</code></pre> <p></p> <p>ROC-AUC curve for train set, with AUC metric</p> <pre><code>y_pred_prob_train = y_pred_prob_train[:,1]\nfalse_positive_rate_train, true_positive_rate_train, thresholds = roc_curve(y_train, y_pred_prob_train)\nroc_auc_train = auc(false_positive_rate_train, true_positive_rate_train)\nplt.figure(figsize=(5,5))\nplt.title('Receiver Operating Characteristic Train Set')\nplt.plot(false_positive_rate_train,true_positive_rate_train, color='red',label = 'AUC = %0.2f' % roc_auc_train)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/supervised_learning/classification_part2/Classification_part2/#example-5-confusion-matrix-graphic-for-test-set","title":"Example 5: Confusion Matrix Graphic for test set","text":"<pre><code>import matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(logreg, X_test, y_test,normalize=None)\nplt.title('Confusion matrix for test set \\n (without normalization)')\nplt.show()\n</code></pre> <p>Same Plot, normalized</p> <pre><code>plot_confusion_matrix(logreg, X_train, y_train,normalize='true')  \nplt.title('Confusion matrix for train set \\n (normalized)')\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(figsize=(5,5))\nplt.title('Receiver Operating Characteristic Train Set')\nplt.plot(false_positive_rate_train,true_positive_rate_train, color='red',label = 'AUC = %0.2f' % roc_auc_train)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplot_confusion_matrix(logreg, X_train, y_train,normalize='true')  \nplt.title('Confusion matrix for train set \\n (normalized)')\nplt.show()\n</code></pre> <p></p> <p></p> <ul> <li>https://stackoverflow.com/questions/49473587/why-is-my-implementations-of-the-log-loss-or-cross-entropy-not-producing-the-s</li> <li>https://machinelearningmastery.com/cross-entropy-for-machine-learning/</li> <li>https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/</li> <li>https://machinelearningmastery.com/standard-machine-learning-datasets-for-imbalanced-classification/</li> <li>https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names</li> </ul>"},{"location":"lectures/unsupervised_learning/GMM/notebook/GMM_notebook/","title":"Gaussian Mixture Models: Unsupervised Learning","text":"<p>By Joe Ganser</p> <p>Link to notebook on Github</p>"},{"location":"lectures/unsupervised_learning/GMM/notebook/GMM_notebook/#i-objective","title":"I Objective","text":"<p>In this tutorial, we'll be using Gaussian mixture models to describe the multimodal distributions of Geyser Eruptions. Working with lengths of eruption times and the time period until the next eruption, we can properly model the clusters in the dataset. </p>"},{"location":"lectures/unsupervised_learning/GMM/notebook/GMM_notebook/#ii-exploring-data","title":"II Exploring Data","text":"<p>Our dataset consists of two features:</p> <ul> <li>eruptions  numeric  Eruption time in mins</li> <li>waiting    numeric  Waiting time to next eruption</li> </ul> <p></p> eruptions waiting 3.600 79 1.800 54 3.333 74 2.283 62 4.533 85"},{"location":"lectures/unsupervised_learning/GMM/notebook/GMM_notebook/#iii-visualizing-data","title":"III Visualizing Data","text":"<p>Using visualization techniques, we can observe how the data is distributed. This will suggest the number of clusters we can use to describe the data. We should consider</p> <ul> <li>Histograms of each feature (To observe the multi-modal property)</li> <li>Box plots</li> <li>Scatter plot of one eruption time vs waiting time</li> </ul> make_figure.py<pre><code>  import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nn = 4\nplt.figure(figsize=(2*n,n))\nplt.subplot(2,2,1)\ncol = 'eruptions'\nsns.distplot(data[col],bins=int(np.sqrt(data[col].shape[0])))\nplt.ylabel('% of data')\nplt.xlabel('eruption time (min)')\nplt.subplot(2,2,2)\nplt.boxplot(data[col],vert=False)\n####\nplt.subplot(2,2,3)\ncol = 'waiting'\nsns.distplot(data[col],bins=int(np.sqrt(data[col].shape[0])))\nplt.ylabel('% of data')\nplt.xlabel('waiting time until next eruption (min)')\nplt.subplot(2,2,4)\nplt.boxplot(data[col],vert=False)\nplt.subplots_adjust(hspace=0.5)\nplt.subplots_adjust(wspace=0.25)\nplt.figure(figsize=(n,n))\nplt.scatter(data['eruptions'],data['waiting'])\nplt.xlabel('eruption time (min)')\nplt.ylabel('waiting time until next eruption (min)')\nplt.show()\n</code></pre> <p></p> <p></p>"},{"location":"lectures/unsupervised_learning/GMM/notebook/GMM_notebook/#iv-estimating-gmm-parameters","title":"IV Estimating GMM parameters","text":"<p>We can see through visualization that the data has a multimodal distribution (bimodal, probably). Despite this, to properly evaluate Gaussian mixture models on the data, it's best to loop through a range of possible parameters and visualize the model performance. There are two possible parameters we evaluate;</p> <ul> <li><code>covariance_type</code><ul> <li><code>['diag','spherical','full','tied']</code></li> </ul> </li> <li><code>n</code> clusters<ul> <li><code>range(1,11)</code></li> </ul> </li> </ul> <p>We plot the values of the Akaike information criterion (AIC) as a function of the model parameters. Gaussian mixture models tell us the AIC info measure for each parameter set. A parsimoneous model is one that minimizes the AIC and simultaneously minimizes the number of clusters. This code inspired by source [2].</p> <pre><code>from sklearn.mixture import GaussianMixture as GMM\nimport matplotlib.pyplot as plt\nn_components = range(1,11)\ncovariances = ['diag','spherical','full','tied']\nmodels = {c:[] for c in covariances}\nfor cov in covariances:\nfor n in n_components:\nmodels[cov].append(GMM(n,covariance_type=cov,random_state=0).fit(data))\nfor cov in covariances:\nplt.plot(n_components, [m.aic(data) for m in models[cov]], label='{} covariance'.format(cov))\nplt.legend(loc='best')\nplt.ylabel('Akaike information criterion (AIC)')\nplt.xlabel('number of clusters')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/GMM/notebook/GMM_notebook/#v-plotting-gmm-clusters","title":"V Plotting GMM clusters","text":"<p>We know from the plot above that <code>n_components=2</code> is the optimal number of clusters, and <code>covariance_type='full'</code> minimizes the AIC. The <code>.predict()</code> method of Gaussian mixture models predicts the cluster memmbership of each data point (row). Considering this dataset is two dimensional, we can visualize the cluster membership of each data point (row) using colors.</p> <pre><code>preds = GMM(n_components=2,covariance_type='full').fit(data).predict(data)\ndata['cluster'] = preds\ndata.head()\n</code></pre> eruptions waiting cluster 3.600 79 0 1.800 54 1 3.333 74 0 2.283 62 1 4.533 85 0 <pre><code>preds = GMM(n_components=2,covariance_type='full').fit(data).predict(data)\npreds = ['red' if i==0 else 'blue' for i in preds]\ndata['cluster'] = pd.Series(preds)\ncluster_colors = {'red':'cluster1','blue':'cluster2'}\nfor color in cluster_colors.keys():\n_ = data[data[data.columns[-1]]==color]\nplt.scatter(_['eruptions'],_['waiting'],c=color,label=cluster_colors[color])\ndata.drop('cluster',axis=1,inplace=True)\nplt.legend()\nplt.xlabel('eruption time (min)')\nplt.ylabel('waiting time until next eruption (min)')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/GMM/notebook/GMM_notebook/#vi-plotting-gmm-by-density","title":"VI Plotting GMM by density","text":"<p>One of the advantages of Gaussian mixture models is that it predicts each data point's probability of cluster membership. We can visualize by performing the same technique above, with the added feature of color density.</p> <pre><code>pred_proba = GMM(n_components=2,covariance_type='full').fit(data).predict_proba(data)\ndata['cluster membership probability'] = [i for i in pred_proba]\ndata.head()\n</code></pre> eruptions waiting cluster membership probability 3.600 79 [2.6846917033762443e-09, 0.9999999973153084] 1.800 54 [0.9999999981364178, 1.8635824263855646e-09] 3.333 74 [8.646713809338013e-06, 0.9999913532861907] 2.283 62 [0.9999894929795239, 1.0507020476591217e-05] 4.533 85 [1.0853706914214446e-21, 1.0] <pre><code>pred_proba = GMM(n_components=2,covariance_type='full').fit(data).predict_proba(data)\n# Generate two random colors\ncolor1 = np.random.rand(3)\ncolor2 = np.random.rand(3)\n# Create an array of weights that determine the mixture of the two colors\nweights = pred_proba[:,0]\n# Create an array of RGB color tuples that represent the mixture of the two colors\ncolors = np.array([(1 - weight) * color1 + weight * color2 for weight in weights])\n# Create the scatter plot with the mixed colors\nplt.scatter(data['eruptions'],data['waiting'], c=colors)\n# Show the plot\nimport matplotlib.colors as colors\ncmap = colors.ListedColormap([color1,color2])\nplt.colorbar(cmap=cmap)\nplt.title(\"Plotting the Clusters on a spectrum\")\nplt.xlabel('eruption time (min)')\nplt.ylabel('waiting time until next eruption (min)')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/GMM/notebook/GMM_notebook/#vii-visualizing-gmm-model-performance","title":"VII Visualizing GMM model performance","text":"<p>The <code>silhouette_score</code> is a useful metric for evaluating unsupervised learning models. We can visualize the performance of each cluster, as well as the combinations of all the clusters in the data. Below, we perform a silhouette plot for the full dataset.</p> <pre><code>from sklearn.metrics import silhouette_samples, silhouette_score\nN_clus = 2\nlabels = preds = GMM(n_components=N_clus,covariance_type='full').fit(data).predict(data)\nsilhouette_scores = silhouette_samples(data, labels)\n# Get the silhouette score for the entire dataset\noverall_silhouette_score = silhouette_score(data, labels)\n# Plot the silhouette scores for each cluster\nfig, ax = plt.subplots()\nax.set_xlim([-1, 1])\nax.set_ylim([0, len(data)])\ny_lower = 0\nfor i in range(N_clus):\ncluster_silhouette_scores = silhouette_scores[labels == i]\ncluster_silhouette_scores.sort()\nsize_cluster_i = cluster_silhouette_scores.shape[0]\ny_upper = y_lower + size_cluster_i\nax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_scores,\nalpha=0.7, edgecolor='none',label='cluster {} score'.format(i))\nax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\ny_lower = y_upper + 10\nax.axvline(x=overall_silhouette_score, color=\"red\", linestyle=\"--\",label='over all silhouette score')\nax.set_title('''Silhouette plot for GMM clustering \\n\n                (n_clusters = {}) \\n full silhouette score: {}'''.format(set(labels),round(overall_silhouette_score,4)))\nax.set_xlabel(\"Silhouette Coefficient Values\")\nax.set_ylabel(\"Number of data points\")\nplt.legend(loc='lower left')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/GMM/notebook/GMM_notebook/#viii-sources","title":"VIII Sources","text":"<ol> <li>(data source) https://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat</li> <li>https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html</li> <li>https://en.wikipedia.org/wiki/Akaike_information_criterion</li> <li>https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py</li> </ol>"},{"location":"lectures/unsupervised_learning/GMM/slides/GMM_slides/","title":"Slides","text":""},{"location":"lectures/unsupervised_learning/KMeans/KMeans_notebook/","title":"Kmeans: Iris data vs Algorithm clusters","text":"<p>by Joe Ganser</p> <p>Github notebook link</p> <p>In this notebook, we'll be fitting the kmeans algorithm to the iris data set, using standard metric procedures, and see how the fit compares to the natural clusters of the Iris dataset. Below is a scatter plot of the data points and the clusters, as compared from various axes.</p> <p>Iris dataset: 150 rows \u00d7 5 columns</p> sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) cluster 5.1 3.5 1.4 0.2 0 4.9 3.0 1.4 0.2 0 4.7 3.2 1.3 0.2 0 4.6 3.1 1.5 0.2 0 5.0 3.6 1.4 0.2 0 ... ... ... ... ... 6.2 3.4 5.4 2.3 2 5.9 3.0 5.1 1.8 2 <p></p>"},{"location":"lectures/unsupervised_learning/KMeans/KMeans_notebook/#example-1-fit-different-measure-k-values","title":"Example 1: Fit different  &amp; measure K values","text":"<ul> <li>Put the dataframe into <code>StandardScaler()</code> format</li> <li>Use a for loop to test values of <code>k</code> in <code>range(2,40)</code></li> <li>At each iteration, fit <code>KMeans</code> and evaluate for <code>k</code><ul> <li>Measure the sum of squared errors (aka 'Inertia')</li> <li>Measure the silhouette coefficient for the clusters</li> <li>Save each measure to a dictionary</li> </ul> </li> <li>Print the dictionary into dataframe format</li> </ul> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X.drop('cluster',axis=1))\nscores = {'sse':[],'silhouette':[]}\nfor k in range(2,40):\nmodel = KMeans(n_clusters=k,max_iter=100)\nmodel.fit(X_scaled)\nsilhouette = silhouette_score(X_scaled,model.labels_,metric='euclidean')\nscores['sse'].append(model.inertia_)\nscores['silhouette'].append(silhouette)\npd.DataFrame(scores).head()\n</code></pre> k value sse silhouette 0 222.361705 0.581750 1 139.825435 0.459378 2 114.304803 0.388220 3 90.807283 0.341947 4 80.022188 0.322037"},{"location":"lectures/unsupervised_learning/KMeans/KMeans_notebook/#example-2-visualize-the-k-means-performance","title":"Example 2: Visualize the K means performance","text":"<p>Plot the <code>SSE</code> and <code>Silhouette</code> scores as a function of <code>K</code> using scatter plots. This will allow us to detect the 'elbow' to select the appropriate value of <code>K</code> for <code>Kmeans</code>.</p> <pre><code>import matplotlib.pyplot as plt\nplt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nplt.scatter(range(2,40),scores['sse'])\nplt.title('K vs SSE',fontsize=20)\nplt.xlabel('K')\nplt.ylabel('SSE')\nplt.subplot(1,2,2)\nplt.scatter(range(2,40),scores['silhouette'])\nplt.title('K vs  silhouette',fontsize=20)\nplt.ylabel('Silhouette Score')\nplt.xlabel('K')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/KMeans/KMeans_notebook/#example-3-use-kneelocator-package-to-find-k","title":"Example 3: Use <code>KneeLocator</code> package to find <code>K</code>","text":"<p>Using the <code>kneed</code> package, find the optimal value of <code>k</code> from the sum of squared errors and silhouette scores. Then, using the plots from example 2 above, make a vertical bar with location of the optimal values of <code>k</code> for each metric.</p> <pre><code>from kneed import KneeLocator\nk_values = range(2,40)\nknee_sse = KneeLocator(k_values,scores['sse'],curve='convex',direction='decreasing')\noptimal_k_sse = knee_sse.knee\nplt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nplt.scatter(range(2,40),scores['sse'],c='r')\nplt.axvline(optimal_k_sse,\nlabel='optimal K={} \\n according to SSE'.format(optimal_k_sse),\nc='r',linestyle=':')\nplt.title('K vs SSE',fontsize=20)\nplt.xlabel('K',fontsize=15)\nplt.ylabel('SSE',fontsize=15)\nplt.legend(fontsize=20)\nknee_silh = KneeLocator(k_values,scores['silhouette'],curve='convex',direction='decreasing')\noptimal_k_silh = knee_silh.knee\nplt.subplot(1,2,2)\nplt.scatter(range(2,40),scores['sse'],c='b')\nsse = round(scores['sse'][optimal_k_silh],3)\nplt.axvline(optimal_k_silh,\nlabel='optimal K={} \\n according to silhouette'.format(optimal_k_silh),\nc='b',linestyle=':')\nplt.title('K vs silhouette',fontsize=20)\nplt.xlabel('K',fontsize=15)\nplt.ylabel('silhouette',fontsize=15)\nplt.legend(fontsize=20)\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/KMeans/KMeans_notebook/#example-4-silhouette-plot-for-each-cluster","title":"Example 4: Silhouette plot for each cluster","text":"<p>It's known that their are three natural seperate clusters in the Iris dataset, and each of these clusters will have their own silhouette score. Vertically plot these silhouette scores and compare them to the overall silhouette score.</p> <pre><code>from sklearn.metrics import silhouette_samples, silhouette_score\nimport numpy as np\noptimal_k = min(optimal_k_sse,optimal_k_silh,3)\nmodel = KMeans(n_clusters=optimal_k,max_iter=100)\nmodel.fit(X_scaled)\ndf = pd.DataFrame(X_scaled)\ndf['cluster'] = model.labels_\n# Get the silhouette score for the entire dataset\nsilhouette = silhouette_score(df[df.columns[:-1]],df['cluster'],metric='euclidean')\nsilhouette_scores = silhouette_samples(df[df.columns[:-1]],df['cluster'],metric='euclidean')\n# Plot the silhouette scores for each cluster\nfig, ax = plt.subplots()\nax.set_xlim([-1, 1])\nax.set_ylim([0, len(X)])\ny_lower = 0\nfor i in range(optimal_k):\ncluster_silhouette_scores = silhouette_scores[model.labels_ == i]\ncluster_silhouette_scores.sort()\nsize_cluster_i = cluster_silhouette_scores.shape[0]\ny_upper = y_lower + size_cluster_i\nif i&lt;4:\nlabel = 'cluster {} score'.format(i)\nelse:\nlabel = None\nax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_scores,\nalpha=0.7, edgecolor='none',label=label)\nif i&lt;8:\ns = str(i)\nelse:\ns = '.'\nax.text(-0.05, y_lower + 0.5 * size_cluster_i, s)\ny_lower = y_upper\nax.set_title('''Silhouette plot for K Means clustering \n\\n(n_clusters = {})'''.format(set(model.labels_)))\nax.set_xlabel(\"Silhouette Coefficient Values\")\nax.set_ylabel(\"Number of data points\")\nax.axvline(x=silhouette, color=\"red\", linestyle=\"--\",\nlabel='over all \\n silhouette score: {}'.format(round(silhouette,4)))\nplt.legend(loc='upper left')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/KMeans/KMeans_notebook/#example-5-cluster-centers","title":"Example 5: Cluster centers","text":"<p>Using the model for <code>k=3</code>, find the cluster centers.</p> <pre><code>clusters = pd.DataFrame(model.cluster_centers_)\nclusters = pd.DataFrame(scaler.inverse_transform(clusters),columns=X.columns[:-1])\nclusters\n</code></pre> sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 5.006000 3.428000 1.462000 0.246000 6.780851 3.095745 5.510638 1.972340 5.801887 2.673585 4.369811 1.413208"},{"location":"lectures/unsupervised_learning/KMeans/KMeans_notebook/#example-6-scatter-plot-the-kmeans-clusters","title":"Example 6: Scatter plot the Kmeans clusters","text":"<p>Using the model built for <code>k=3</code>, make scatter plot for each axes to show what K means predicts as the clusters.</p> <pre><code>import matplotlib.pyplot as plt\nn=1\nplt.figure(figsize=(10,30))\nfrom itertools import combinations\nfrom matplotlib.colors import ListedColormap\nv=3\nX['cluster'] = model.labels_\nnum_range = range(1, optimal_k + 1)\ncolor_range = np.random.rand(3, 3)\nz_range = range(0, optimal_k+1)\nnum_colors = len(z_range)\ncolor_range = np.random.rand(num_colors, 3)[:3]\n# Create a colormap that maps each z value to a unique color\ncmap = ListedColormap(color_range)\nv=2\nn=1\nfor c in list(combinations(X.columns[:-1],2)):\nplt.subplot(len(combinations_list)//v,v,n)\n# Create the scatter plot\nx = X[c[1]]\ny = X[c[0]]\nz = X['cluster']\nplt.scatter(x, y, c=z, cmap=cmap)\n# Add a colorbar to the plot\nplt.scatter(x=clusters[c[1]],y=clusters[c[0]],marker='*',label='cluster center',c='r')\nif n==1:\nplt.legend()\nplt.ylabel(c[0])\nplt.xlabel(c[1])\nn+=1\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/KMeans/KMeans_notebook/#example-7-whats-the-discrepency","title":"Example 7: What's the discrepency?","text":"<p>Comparing the plots shown at the beginning and the plots predicted by the algorithm shows the imperfect nature of Kmeans. Kmeans, by its nature, is a fundamentally random algorithm - picking random positions and adjusting the position until metrics are minimized. We can see in example 4 that we can get relatively good clusters according to the silhouette scores, however these dont necessarily match what occurs in nature (example 6). It's important to be aware that the random nature of Kmeans can make large discrpencies between naturally occurring clusters and the ones predicted by the algorithm.</p>"},{"location":"lectures/unsupervised_learning/db_scan/db_scan_notebook/","title":"Density Based Scanning: Unsupervised Learning","text":"<p>By Joe Ganser</p> <p>Notebook github repo link</p> <p>Suppose salamanders were caught in a NJ lake and measurements of mass and length were taken of three species. Using this (fake) data, we will attempt to fit the denisty based scanning algorithm on the data.</p> <p>Notebook goals * Properly tune DB Scan on an data set * Determine the appropriate hyperparameters (epsilon &amp; min points) * Make validating plots</p>"},{"location":"lectures/unsupervised_learning/db_scan/db_scan_notebook/#example-1-load-plot-the-data","title":"Example 1: Load &amp; Plot the data","text":"<p>Make a scatter plot, like the one above, but for each species of salamander - give the graph a different color.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('db_clustering_salamanders_example1.csv',index_col=0)\ndef compare_x_y(data,x_axis,y_axis,legend=False):\ncolors = {'species 0': (0,'red'), 'species 1': (1,'green'), 'species 2': (2,'blue')}\nfor key,values in colors.items():\nplt.scatter(x=data[data['species']==values[0]][x_axis], y=data[data['species']==values[0]][y_axis], c=values[1], label=key)\nplt.xlabel(x_axis,fontsize=15)\nplt.ylabel(y_axis,fontsize=15)\nif legend:\nplt.legend(fontsize=12)\nplt.show()\n</code></pre> <pre><code>plt.figure(figsize=(10,10))\ncompare_x_y(df,'length(mm)','mass(g)',legend=True)\nplt.savefig('example1-labels.png')\n</code></pre> <p></p> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre>"},{"location":"lectures/unsupervised_learning/db_scan/db_scan_notebook/#example-2-fit-db-scan-without-tuning","title":"Example 2: Fit DB Scan without tuning","text":"<p>Fit the DB scan algorithm to the data without actually tuning it. Evaluate the results and see how the model performs.</p> <ul> <li>Use DBscan to color each point in accordance to whether its an core point, outer point or edge point</li> <li>Plot the cluster centers with stars</li> <li>Print the V score of the model</li> <li>Print the Silhoette score of the model</li> <li>Print the estimated number of noise points</li> <li>Print the estimated number of clusters</li> </ul> <pre><code>from sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import v_measure_score\nX = df.drop('species',axis=1)\ny = df['species']\ndbscan_cluster = DBSCAN()\ndbscan_cluster.fit_predict(X)\n# Visualizing DBSCAN\nplt.scatter(X[X.columns[0]], X[X.columns[1]], c=dbscan_cluster.labels_,label=set(y))\n# plot the centroids of the DB scan clusters\ncentroids = []\nfor i in range(np.max(labels) + 1):\ncentroids.append(np.mean(X[labels == i], axis=0))\ncentroids = pd.DataFrame(np.array(centroids),columns=['mass (g)','length (mm)'])\nplt.scatter(centroids['mass (g)'],centroids['length (mm)'],marker='*',s=100,label='cluster centers',c='red')\nplt.xlabel(\"mass(g)\")\nplt.ylabel(\"length(mm)\")\nplt.legend()\n# Number of Clusters\nlabels=dbscan_cluster.labels_\nN_clus=len(set(labels))-(1 if -1 in labels else 0)\nprint('Estimated no. of clusters: %d' % N_clus)\n# Identify Noise\nn_noise = list(dbscan_cluster.labels_).count(-1)\nprint('Estimated no. of noise points: %d' % n_noise)\n# Calculating v_measure\nprint('v_measure =', v_measure_score(y, labels))\n# Calculate silhouetteScore\nprint('silhouette_score =',silhouette_score(X,labels))\n</code></pre> <pre><code>Estimated no. of clusters: 15\nEstimated no. of noise points: 257\nv_measure = 0.7343038605554137\nsilhouette_score = 0.2649627233948808\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/db_scan/db_scan_notebook/#example-3-silhouette-plots","title":"Example 3: Silhouette Plots","text":"<p>Make a silhouette plot for each of the clusters estimated in example 1.</p> <ul> <li>Make a vertical bar representing the \"overall\" silhouette score of the whole data set.</li> <li>Vertically stack the horizontal silhouette scores on top of each other.</li> <li>Most clusters will be tiny, so only give the first 4 labels.<ul> <li>Give the plot a legend.</li> </ul> </li> </ul> <pre><code>from sklearn.metrics import silhouette_samples, silhouette_score\nimport numpy as np\nsilhouette_scores = silhouette_samples(X, labels)\n# Get the silhouette score for the entire dataset\noverall_silhouette_score = silhouette_score(X, labels)\n# Plot the silhouette scores for each cluster\nfig, ax = plt.subplots()\nax.set_xlim([-1, 1])\nax.set_ylim([0, len(X)])\ny_lower = 0\nfor i in range(N_clus):\ncluster_silhouette_scores = silhouette_scores[labels == i]\ncluster_silhouette_scores.sort()\nsize_cluster_i = cluster_silhouette_scores.shape[0]\ny_upper = y_lower + size_cluster_i\nif i&lt;4:\nax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_scores,\nalpha=0.7, edgecolor='none',label='cluster {} score'.format(i))\nax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\nelif i&gt;6:\nax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_scores,\nalpha=0.7, edgecolor='none')\nax.text(-0.05, y_lower, '.')\ny_lower = y_upper + 10\nax.set_title(\"Silhouette plot for DB Scan clustering \\n(n_clusters = {})\".format(set(labels)))\nax.set_xlabel(\"Silhouette Coefficient Values\")\nax.set_ylabel(\"Number of data points\")\nax.axvline(x=overall_silhouette_score, color=\"red\", linestyle=\"--\",label='over all silhouette score')\nplt.legend(loc='upper left')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/db_scan/db_scan_notebook/#example-3-determining-epsilon-using-kneelocator","title":"Example 3: Determining epsilon using <code>KneeLocator</code>","text":"<p>Without tuning the model, we calculated the estimated number of clusters. Using the <code>KneeLocator</code> and the <code>NearestNeighbors</code> packages, use the estimated number of clusters to find the value of epsilon that makes the density drop the fastest. We will then use this as our hyperparameter when we re-tune the model.</p> <ul> <li>Fit the data to the nearest neighbors with <code>N_clus</code> (example 1) as the parameter</li> <li>Calculate the distances, sort them by the <code>N_clus</code> number of clusters</li> <li>Pass this to the <code>KneeLocator</code> function</li> </ul> <pre><code>from sklearn.neighbors import NearestNeighbors\nimport numpy as np\nnearest_neighbors = NearestNeighbors(n_neighbors=N_clus)\nneighbors = nearest_neighbors.fit(df)\ndistances, indices = neighbors.kneighbors(df)\ndistances = np.sort(distances[:,N_clus-1], axis=0)\nfrom kneed import KneeLocator\ni = np.arange(len(distances))\nknee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\nEpsilon = distances[knee.knee]\nprint('Epsilon :',Epsilon)\n#fig = plt.figure(figsize=(10, 10))\n#plt.figure(figsize=(10, 10))\nknee.plot_knee()\nplt.xlabel(\"Points\")\nplt.ylabel(\"Radius\")\nplt.title('Radius (epsilon) where density drops the fastest \\n with N={} min points in the radius'.format(N_clus),fontsize=15)\nplt.show()\n</code></pre> <pre><code>Epsilon : 1.102447265591868\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/db_scan/db_scan_notebook/#example-4-grid-searching-for-min-points","title":"Example 4: Grid searching for min points","text":"<p>We've determine the value of epsilon where density drops the fastest between clusters. This value of epsilon was found using 15 min points. Now we grid search around 15 points the find the best number of minimum points to fit DB scan. When we find the best number of minimum points, we should also have an optimal number of clusters - 3 - because there were only 3 in the data set to begin with.</p> <ul> <li>Loop between 5 and 15 min points, fitting DB scan using an epsilon=1.102447265</li> <li>Find the value of min points for which DB scan predicts 3 clusters AND maximimizes the overall sihlouette score.</li> </ul> <pre><code>scores = []\nfor minima in range(5,16):\ndbscan_cluster = DBSCAN(eps=distances[knee.knee], min_samples=minima)\ndbscan_cluster.fit(X)\nlabels=dbscan_cluster.labels_\nN_clus=len(set(labels))-(1 if -1 in labels else 0)\nscore = silhouette_score(X,labels)\nscores.append((N_clus,score,minima))\nbest = sorted([j for j in scores if j[0]==3],key=lambda x:x[1],reverse=True)[0]\nprint(best)\nprint('Best scoring min samples was: {}'.format(best[-1]))\n</code></pre> <pre><code>(3, 0.74925376513721, 9)\nBest scoring min samples was: 9\n</code></pre>"},{"location":"lectures/unsupervised_learning/db_scan/db_scan_notebook/#example-5-properly-fitting-db-scan","title":"Example 5: Properly fitting DB Scan","text":"<p>Repeat example 2 with the proper hyper parameters.</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import v_measure_score\ndbscan_cluster = DBSCAN(eps=distances[knee.knee], min_samples=best[-1])\ndbscan_cluster.fit(X)\n# Visualizing DBSCAN\nX2 = X.copy()\nX2['cluster'] = dbscan_cluster.labels_\n# Visualizing DBSCAN\nplt.scatter(X[X.columns[0]], X[X.columns[1]], c=dbscan_cluster.labels_,label=set(y))\n# plot the centroids of the DB scan clusters\ncentroids = []\nfor i in range(np.max(labels) + 1):\ncentroids.append(np.mean(X[labels == i], axis=0))\ncentroids = pd.DataFrame(np.array(centroids),columns=['mass (g)','length (mm)'])\nplt.scatter(centroids['mass (g)'],centroids['length (mm)'],marker='*',s=100,label='cluster centers',c='red')\nplt.xlabel(\"mass(g)\")\nplt.ylabel(\"length(mm)\")\nplt.legend()\nprint(\"Number of min points: \",n)\nprint('Radius (epsilon) for DB scan: ',distances[knee.knee])\n# Number of Clusters\nlabels=dbscan_cluster.labels_\nN_clus=len(set(labels))-(1 if -1 in labels else 0)\nprint('Estimated no. of clusters: %d' % N_clus)\n# Identify Noise\nn_noise = list(dbscan_cluster.labels_).count(-1)\nprint('Estimated no. of noise points: %d' % n_noise)\n# Calculating v_measure\nprint('v_measure =', v_measure_score(y, labels))\n# Calculate silhouetteScore\nprint('silhouette_score =',silhouette_score(X,labels))\nplt.show()\n</code></pre> <pre><code>Number of min points:  11\nRadius (epsilon) for DB scan:  1.102447265591868\nEstimated no. of clusters: 3\nEstimated no. of noise points: 74\nv_measure = 0.9396202026007384\nsilhouette_score = 0.74925376513721\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/db_scan/db_scan_notebook/#example-6-well-fit-silhouette-plots","title":"Example 6: Well fit Silhouette plots","text":"<p>Repeat example 3 with the hyperparameters of example 2.</p> <pre><code>silhouette_scores = silhouette_samples(X, labels)\n# Get the silhouette score for the entire dataset\noverall_silhouette_score = silhouette_score(X, labels)\n# Plot the silhouette scores for each cluster\nfig, ax = plt.subplots()\nax.set_xlim([-1, 1])\nax.set_ylim([0, len(X)])\ny_lower = 0\nfor i in range(N_clus):\ncluster_silhouette_scores = silhouette_scores[labels == i]\ncluster_silhouette_scores.sort()\nsize_cluster_i = cluster_silhouette_scores.shape[0]\ny_upper = y_lower + size_cluster_i\nax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_scores,\nalpha=0.7, edgecolor='none',label='cluster {} score'.format(i))\nax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\ny_lower = y_upper + 10\nax.set_title(\"Silhouette plot for DB Scan clustering \\n(n_clusters = {})\".format(set(labels)))\nax.set_xlabel(\"Silhouette Coefficient Values\")\nax.set_ylabel(\"Number of data points\")\nax.axvline(x=overall_silhouette_score, color=\"red\", linestyle=\"--\",label='over all silhouette score')\nplt.legend(loc='upper left')\nplt.show()\n</code></pre> <p></p>"},{"location":"lectures/unsupervised_learning/db_scan/db_scan_notebook/#sources","title":"Sources","text":"<p>The following sources were very helpful in making this lecture;</p> <p>[1] https://machinelearningknowledge.ai/tutorial-for-dbscan-clustering-in-python-sklearn/</p>"},{"location":"lectures/unsupervised_learning/db_scan/db_scan_slides/","title":"Slides","text":""},{"location":"research/2017-12-21-Bitcoin-Ethereum-timeseries/","title":"Before the Bitcoin Bubble Burst: Predicting day to day prices with ARIMA &amp; Neural Nets","text":"<p>Joe Ganser </p>"},{"location":"research/2017-12-21-Bitcoin-Ethereum-timeseries/#abstract","title":"Abstract","text":"<p>The objective of this blog post is to experiment with time series techniques as well as methods in signal/noise extraction to make a prediction on the prices of both bitcoin and ethereum in the last week of August 2017, using all the previous pricing data.</p> <p>In this short term price forecast, I am predicting the next day's price based upon all the prices leading upto it. Then on the next day, I am repeating this process again, once next day's price is received. Model techniques like this could be used for high frequency trading, not long term investing. This is NOT about predicting long term trends.</p> <p>Because I am only predicting the next day out, and I am extracting the signal from the noise and what the models ACTUALLY predict is the noise. The predicted noise is then plugged back into the signal function, and that turns into the measurable prediction. This concept is explained a little more below.</p> <p>Lets take a look at the pricing data upto this point so far;</p> <p> </p>"},{"location":"research/2017-12-21-Bitcoin-Ethereum-timeseries/#iintroduction","title":"I.Introduction","text":"<p>WORKFLOW</p> <p>The general process of how I built my models can be summarized in a few steps.</p> <ol> <li> <p>Extract the signal from the noise (find stationarity via dickey fuller test). More about this below.</p> </li> <li> <p>Model the noise, and predict the noise for the next day (ARIMA and Neural nets)</p> </li> <li> <p>Plug the predicted noise into the signal, calculating the price value for the next day</p> </li> <li> <p>Compare the predicted price of the next day to the actual next day's price</p> </li> <li> <p>Repeat steps 2 and 3 once when obtaining the actual price of the next day  (use a for loop)</p> </li> </ol>"},{"location":"research/2017-12-21-Bitcoin-Ethereum-timeseries/#iimodel-choices-arima-neural-net-recursion","title":"II.Model Choices: ARIMA &amp; Neural Net Recursion","text":"<p>The model choice was done simply for comparitive experimental reasons, only because ARIMA and Neural Net LSTM can predict time series. Other techniques could be done to, but I just wanted to compare these out of curiosity. Their performance metrics were the cumulative root mean square error across all the days predicted.</p> <p>ASSUMPTIONS</p> <p>The assumption, which is validated, is that the exponential long term trend shall remain over the last few days of August of 2017. The assumption was made through the techniques of signal noise extraction which are explained below. I do NOT assume that the exponential trend will go on forever.</p> <p>MODEL PREDICTIONS</p> <p>Here are the final results - the actual predictions of my models.</p> <p> </p> <p>Predictions for Bitcoin</p> Date Bitcoin Price ARIMA prediction Neural Net Prediction 2017-08-23 4318.35 4321.92 4369.75 2017-08-24 4364.41 4368.25 4423.16 2017-08-25 4352.30 4356.23 4397.79 2017-08-26 4345.75 4349.69 4384.69 2017-08-27 4390.31 4394.20 4430.15 2017-08-28 4597.31 4601.04 4643.98 2017-08-29 4583.02 4587.37 4643.36 <p>Predictions for Ethereum</p> Date Ethereum Price ARIMA prediction Neural Net Prediction 2017-08-23 325.28 327.39 325.36 2017-08-24 330.06 331.70 325.76 2017-08-25 332.86 334.69 334.74 2017-08-26 347.88 350.00 352.49 2017-08-27 347.66 348.96 350.64 2017-08-28 372.35 374.69 373.62 2017-08-29 383.86 384.72 388.87"},{"location":"research/2017-12-21-Bitcoin-Ethereum-timeseries/#iii-model-evluation","title":"III. Model Evluation","text":"<p>So looking at these results, what were the model performance metrics? The performance metric I used to compare the results of the techniques was the summed root mean square error across all the days (seven, in total) predicted.</p> Model Bitcoin RMSE Ethereum RMSE ARIMA 3.8985 9.73758 Neural Net 49.40448 1.67389 <p>Cumulative across the dates of Aug 23-Aug 29 2017.</p> <p>Interesting Results!</p> <p>ARIMA tended to perform better on bitcoin than neural nets, but the neural nets performed much better on the ethereum data! This may be related to the signal function I used, which is explained below.</p> <p>EXTRACTING THE SIGNAL FROM THE NOISE USING THE AUGMENTED DICKEY FULLER TEST</p> <p> </p> <p>The augmented dickey fuller test is a statistical test to determine stationarity. Stationarity occurs when a series of data is transformed into format which clearly identifies the signal (aka a \"trend\") and all we are left with is noise. More specifically, stationarity occurs when we put the time series data is put into it's correct inverse function. With outputs similar to a Z-test, the test tells us if the new format we have has clearly identified the signal within the data. The null hypothesis of the Dickey Fuller test is that the signal and noise are NOT decoupled. The strategy behind using this is that by extracting the signal, we model and predict the noise and plug those predictions back into the signal.</p> <p>Stationarity data has four fundamental criteria:</p> <p></p> <p>THE BEST FIT FUNCTION &amp; IT'S INVERSE</p> <p>Knowing the best fitting function is not enough to model the time series. What is needed is not only a best fit function, but that function's inverse as well. By putting the data of the time series into the inverse function, we can get the noise values. It's noise values that are put into the Dickey Fuller test.</p> <p>A graphical example of an inverse function: log(x) is the inverse of e^(x)</p> <p> </p> <p>Extracting the signal from the noise can be summed into three steps</p> <ol> <li>Find the a function similar to the time series data.</li> <li>Find that function's inverse.</li> <li> <p>Plug the time series data into the inverse, and plug that transformed data set into      the Dickey Fuller Test</p> <p>*If we haven't rejected the null hypothesis in step 3 try step 1 over starting with a  different function.</p> </li> </ol> <p>By looking at the very first plot, we can see that the trend of the cryptocurrency prices are in some way exponential. Thus, I can hypothesize that the signal is exponential in form.</p> <p> </p> <p>The actual function form that allowed me to seperate the signal from the noise was indeed exponential in form but not simply exponential. It was by using this function's inverse that I could pass the dickey fuller test.</p> <p>The actual function that worked was this:</p> <p></p> <p>These function transformations worked with both the bitcoin &amp; ethereum data.</p> <p>In code form, it is:</p> <p><code>noise = (np.log(data['Price'])-np.log(data['Price'].shift())).dropna()</code></p> <p>Where the <code>'.shift()'</code> cooresponds to the day before.</p> <p>FAILING AND THEN PASSING THE DICKEY FULLER TEST</p> <p>In the images passing the Dickey Fuller test without perfmoring any transformation, and then (botton of each tab), the Dickey Fuller test is passed (meaning we can reject the null hypothesis, and the data is stationary.)</p> <p>So now I have clearly identified the stationary form of the time series. These stationary forms can be plugged into the models to predict the noise, and using the noise predictions we can plug those back into the signal.</p> <p>The plots below show the series, the rolling mean and standard deviations plotted side by side. The first is what the original series looked like, and the second is what the series looks like after it's input into the inverse transformation function.</p> <p> </p> <p>The specific statistics for the tests were:</p> <p>Bitcoin Dickey-Fuller Test:|Results when transformed to stationary|Results results before stationary (failing) ------|---------|--------| Test Statistic|-20.254025|3.433511 p-value|0.000000|1.00000 Number Lags Used|4.000000|28.0000 Number of Observations Used| 2596.000000|2573.000000 Critical Value (1%)|-3.962262|-3.962293 Critical Value (5%)|-3.412183|-3.412198 Critical Value (10%)|-3.128047|-3.128055</p>"},{"location":"research/2017-12-21-Bitcoin-Ethereum-timeseries/#iv-model-code","title":"IV: Model Code","text":"<p>So the prediction steps are structured in a FOR LOOP:</p> <ol> <li>Plug the noise into the model</li> <li>Train the model on the history of the noise</li> <li>Predict the noise of the next day</li> <li>Plug the next day's predicted noise into the signal function.</li> <li>Add the actual price of the next day to the price data set, transform it into      stationarity and start again at step 1.</li> </ol> <p>MODELLING WITH ARIMA</p> <p>There are several steps to building an ARIMA model. ARIMA operates on three parameters. Based upon the plots of auto-correlation, partial auto-correlation and the amount of Bayesian information criterion we select these three parameters. Ofcourse, the end goal is to minimize the root mean squared error of the model.</p> <p>AUTO CORRELATION &amp; PARTIAL AUTOCORRELATION PLOTS</p> <p>The plots of auto-correlation &amp; partial auto-correlation for bitcoin &amp; ethereums stationary formats were:</p> <p> </p> <p>Using these plots I can estimate the ARIMA parameters (p,q) of the parameters (p,q,d) for ARIMA models. Just by looking at the graphs it can be estimate that p &amp; q are around no more than 7 each. After grid searching through lots of combinations it was found that Bitcoins ARIMA parameters were (p=0,d=1,q=1) and Ethereum's ARIMA parameters were (p=2,d=0,q=0). These were validated by the minimization of the root mean squared error.</p> <p>Here is the ARIMA model code:</p> <pre><code>def ARIMA_predictions(original_series,stationary_series,parameters, days_out):\n# Here we predict the noise\ntrain = stationary_series[:-days_out]\n# test the model on the last N points of the data, the models I used were at 7 days\ntest = stationary_series[-days_out:]\n#\nhistory = [x for x in train]\ntrain.dropna(inplace=True)\ntest.dropna(inplace=True)\npredicted_values = []\ntested = []\n#1. Plug the noise into the model\n#2. Train the model on the history of the noise\n#3. Predict the noise of the next day\n#4. Plug the next day's predicted noise into the signal function.\n#5. Add the actual price of the next day to the price data set, transform it into stationarity and start again at step 1.\nfor i in range(len(test)):\nmodel = ARIMA(history, order=parameters)\nmodel_fit = model.fit(disp=0)\nyhat = float(model_fit.forecast(steps=1)[0])\npredicted_values.append(yhat)\ntested_values = list(test)[i]\ntested.append(tested_values)\nhistory.append(tested_values)\npredictions_series = pd.Series(predicted_values, index = test.index)\n# This part couples the signal to the noise.\na = original_series.loc[original_series.index[-(days_out+1):]]['Price']\nb = np.exp(predictions_series)\nfull_predictions = pd.DataFrame(a*b,columns=['Predicted with ARIMA']).dropna()\ndf = pd.concat([original_series.loc[original_series.index[-days_out:]],full_predictions],axis=1)\nerror = str(np.sqrt(mean_squared_error(df['Price'],df['Predicted with ARIMA'])))\nprint(\"ARIMA Root Mean Squared Error: \",error)\ndf.index.name = None\ndf[['Price','Predicted with ARIMA']] = df[['Price','Predicted with ARIMA']].apply(lambda x: round(x,2))\nreturn df\nbitcoin_ARIMA = ARIMA_predictions(bitcoin,bits_log_shift,(0,1,1),7)\n</code></pre> <p>Code for Neural Net LSTM</p> <p>The neural net system was based upon sequential, dense LSTM modelling techniques. Like the ARIMA model, it was dependent upon a stationary time series input, modeling the noise and then putting the noise predictions back into the signal. </p> <pre><code>from sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\ndef Neural_Net_predictions(original_time_series, stationary_time_series, days_out,nb_epoch,neurons):\n# note all these \"sub\" functions are used on the stationary time series.\n# The neural nets are used to predict the noise. Once the noise is predicted\n# Its plugged back into the signal\nX = stationary_time_series\n# Step 2\n# Break the time series into shifted components. Each column is a shifted value \n# previously in the time series\ndef timeseries_to_supervised(data, lag=1):\ndf = pd.DataFrame(data)\ncolumns = [df.shift(i) for i in range(1, lag+1)]\ncolumns.append(df)\ndf = pd.concat(columns, axis=1)\ndf.fillna(0, inplace=True)\nreturn df\n# Step 3\n# We must put the time series onto the scale acceptable by the activation functions\ndef scale(train, test):\nscaler = MinMaxScaler(feature_range=(-1, 1))\nscaler = scaler.fit(train)\ntrain = train.reshape(train.shape[0],train.shape[1])\ntrain_scaled = scaler.transform(train)\ntest = test.reshape(test.shape[0],test.shape[1])\ntest_scaled = scaler.transform(test)\nreturn scaler, train_scaled, test_scaled\n# Step 4\ndef fit_lstm(train, batch_size, nb_epoch, neurons):\nX, y = train[:, 0:-1], train[:, -1]\nX = X.reshape(X.shape[0], 1, X.shape[1])\nmodel = Sequential()\nmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nfor i in range(nb_epoch):\nmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\nmodel.reset_states()\nreturn model\n# Step 5\ndef forecast_lstm(model, batch_size, X):\nX = X.reshape(1, 1, len(X))\nyhat = model.predict(X, batch_size=batch_size)\nreturn yhat[0,0]\n#Step 6\ndef invert_scale(scaler, X, value):\nnew_row = [x for x in X] + [value]\narray = np.array(new_row)\narray = array.reshape(1, len(array))\ninverted = scaler.inverse_transform(array)\nreturn inverted[0, -1]\n# Now use all the above functions\nsupervised = timeseries_to_supervised(X,1)\nsupervised_values = supervised.values\ntrain, test = supervised_values[0:-days_out], supervised_values[-days_out:]\nscaler, train_scaled, test_scaled = scale(train, test)\ntrain_reshaped = train_scaled[:,0].reshape(len(train_scaled),1,1)\nlstm_model = fit_lstm(train_scaled,1,nb_epoch,neurons)\n#Step 7\npredictions = []\nfor i in range(len(test_scaled)):\n#Make one step forecast\nX, y = test_scaled[i,0:-1], test_scaled[i,-1]\nyhat = forecast_lstm(lstm_model,1,X)\n#invert scaling\nyhat = invert_scale(scaler, X, yhat)\n#store forecast\npredictions.append(yhat)\n# Step 8\n# This part plugs it back into the signal\npredictions_series = pd.Series(predictions, index = original_time_series.index[-days_out:])\na = original_time_series.loc[original_time_series.index[-(days_out+1):]]['Price']\nb = np.exp(predictions_series)\nfull_predictions = pd.DataFrame(a*b,columns=['Predicted with Neural Nets']).dropna()\ndf = pd.concat([original_time_series.loc[original_time_series.index[-days_out:]],full_predictions],axis=1)\nerror = str(np.sqrt(mean_squared_error(df['Price'],df['Predicted with Neural Nets'])))\nprint(\"Neural Net Root Mean Squared Error: \",error)\ndf.index.name=None\ndf[['Price','Predicted with Neural Nets']] = df[['Price','Predicted with Neural Nets']].apply(lambda x: round(x,2))\nreturn df\nbitcoin_NN = Neural_Net_predictions(bitcoin,bits_log_shift,days_out=7,nb_epoch=55,neurons=175)\n</code></pre> <p>Link to the github repo</p>"},{"location":"research/2017-12-21-Bitcoin-Ethereum-timeseries/#v-sources","title":"V. Sources","text":"<ol> <li> <p>The Application of Time Series Modelling and Monte Carlo Simulation: Forecasting Volatile Inventory Requirements By Robert Davies, Tim Coole, David Osipyw, https://file.scirp.org/pdf/AM_2014050513382674.pdf</p> </li> <li> <p>How to Get Started with Deep Learning for Time Series Forecasting (7-Day Mini-Course), by Jason Brownlee; https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/</p> </li> <li> <p>Statistical forecasting: notes on regression and time series analysis, By Robert Nau, Duke University https://people.duke.edu/~rnau/411arim.htm</p> </li> </ol>"},{"location":"research/2018-10-20-AMES-tutorial/","title":"Teaching regression: A basic tutorial on how to build a regression model from start to finish","text":"<p>By Joe Ganser</p> <p>Link to the github repo</p>"},{"location":"research/2018-10-20-AMES-tutorial/#iintroduction","title":"I.Introduction","text":"<p>In this notebook, I will be creating a simple work flow tutorial that allows beginner data science students to go through the process of building a regression model. This is for students who have completed most of my introduction to data science class and have basic familiarity with data science.</p> <p>Background info</p> <p>The data set we'll be working with is well known and studied. It describes real-estate sales on houses in AMES Iowa, and the target variable we're trying to predict is the price of a home. Lets load the data and begin exploring. A lot of kaggle projects have been made from it, and you can find them here.</p>"},{"location":"research/2018-10-20-AMES-tutorial/#iiexploratory-data-analysis","title":"II.Exploratory Data Analysis","text":"<p>In any data science project, we always start of by getting a good glimpse of what kind of data we're looking at. This is called \"exploratory data analysis\" (EDA). There are a few things we need to look at at our first round of EDA:</p> <ol> <li>The number of rows/columns.</li> <li>The data types on each columns.</li> <li>Get the min, max, mean values of each column</li> <li>The number of missing values.</li> </ol> <p>Lets start by loading the dataset, and observing it's dimensions. Our target variable <code>y</code> is the house price, and the rest of the variables (<code>X</code>) are our predictors. We want to load the file <code>train.csv</code> because it will train our models.</p> <pre><code>#Type your solution here\n</code></pre> <p>Now split the data set into a predictor table (<code>X</code>) and a target table <code>y</code></p> <pre><code>#Type your solution here\n</code></pre> <p>Now look at the head of the predictor table, and print it's shape.</p> <pre><code>#Type your solution here. It should have the following output\n</code></pre> <pre><code>(1460, 80)\n</code></pre> Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition 1 60 RL 65.0 8450 Pave Reg Lvl AllPub Inside Gtl CollgCr Norm Norm 1Fam 2Story 7 5 2003 2003 Gable CompShg VinylSd VinylSd BrkFace 196.0 Gd TA PConc Gd TA No GLQ 706 Unf 0 150 856 GasA Ex Y SBrkr 856 854 0 1710 1 0 2 1 3 1 Gd 8 Typ 0 Attchd 2003.0 RFn 2 548 TA TA Y 0 61 0 0 0 0 0 2 2008 WD Normal 2 20 RL 80.0 9600 Pave Reg Lvl AllPub FR2 Gtl Veenker Feedr Norm 1Fam 1Story 6 8 1976 1976 Gable CompShg MetalSd MetalSd None 0.0 TA TA CBlock Gd TA Gd ALQ 978 Unf 0 284 1262 GasA Ex Y SBrkr 1262 0 0 1262 0 1 2 0 3 1 TA 6 Typ 1 TA Attchd 1976.0 RFn 2 460 TA TA Y 298 0 0 0 0 0 0 5 2007 WD Normal 3 60 RL 68.0 11250 Pave IR1 Lvl AllPub Inside Gtl CollgCr Norm Norm 1Fam 2Story 7 5 2001 2002 Gable CompShg VinylSd VinylSd BrkFace 162.0 Gd TA PConc Gd TA Mn GLQ 486 Unf 0 434 920 GasA Ex Y SBrkr 920 866 0 1786 1 0 2 1 3 1 Gd 6 Typ 1 TA Attchd 2001.0 RFn 2 608 TA TA Y 0 42 0 0 0 0 0 9 2008 WD Normal 4 70 RL 60.0 9550 Pave IR1 Lvl AllPub Corner Gtl Crawfor Norm Norm 1Fam 2Story 7 5 1915 1970 Gable CompShg Wd Sdng Wd Shng None 0.0 TA TA BrkTil TA Gd No ALQ 216 Unf 0 540 756 GasA Gd Y SBrkr 961 756 0 1717 1 0 1 0 3 1 Gd 7 Typ 1 Gd Detchd 1998.0 Unf 3 642 TA TA Y 0 35 272 0 0 0 0 2 2006 WD Abnorml 5 60 RL 84.0 14260 Pave IR1 Lvl AllPub FR2 Gtl NoRidge Norm Norm 1Fam 2Story 8 5 2000 2000 Gable CompShg VinylSd VinylSd BrkFace 350.0 Gd TA PConc Gd TA Av GLQ 655 Unf 0 490 1145 GasA Ex Y SBrkr 1145 1053 0 2198 1 0 2 1 4 1 Gd 9 Typ 1 TA Attchd 2000.0 RFn 3 836 TA TA Y 192 84 0 0 0 0 0 12 2008 WD Normal <p>Now use the <code>.info()</code> method to find that datatypes on all of the columns (including the target!)</p> <pre><code>#Type your solution here. It should have the following output\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1460 entries, 0 to 1459\nData columns (total 81 columns):\nId               1460 non-null int64\nMSSubClass       1460 non-null int64\nMSZoning         1460 non-null object\nLotFrontage      1201 non-null float64\nLotArea          1460 non-null int64\nStreet           1460 non-null object\nAlley            91 non-null object\nLotShape         1460 non-null object\nLandContour      1460 non-null object\nUtilities        1460 non-null object\nLotConfig        1460 non-null object\nLandSlope        1460 non-null object\nNeighborhood     1460 non-null object\nCondition1       1460 non-null object\nCondition2       1460 non-null object\nBldgType         1460 non-null object\nHouseStyle       1460 non-null object\nOverallQual      1460 non-null int64\nOverallCond      1460 non-null int64\nYearBuilt        1460 non-null int64\nYearRemodAdd     1460 non-null int64\nRoofStyle        1460 non-null object\nRoofMatl         1460 non-null object\nExterior1st      1460 non-null object\nExterior2nd      1460 non-null object\nMasVnrType       1452 non-null object\nMasVnrArea       1452 non-null float64\nExterQual        1460 non-null object\nExterCond        1460 non-null object\nFoundation       1460 non-null object\nBsmtQual         1423 non-null object\nBsmtCond         1423 non-null object\nBsmtExposure     1422 non-null object\nBsmtFinType1     1423 non-null object\nBsmtFinSF1       1460 non-null int64\nBsmtFinType2     1422 non-null object\nBsmtFinSF2       1460 non-null int64\nBsmtUnfSF        1460 non-null int64\nTotalBsmtSF      1460 non-null int64\nHeating          1460 non-null object\nHeatingQC        1460 non-null object\nCentralAir       1460 non-null object\nElectrical       1459 non-null object\n1stFlrSF         1460 non-null int64\n2ndFlrSF         1460 non-null int64\nLowQualFinSF     1460 non-null int64\nGrLivArea        1460 non-null int64\nBsmtFullBath     1460 non-null int64\nBsmtHalfBath     1460 non-null int64\nFullBath         1460 non-null int64\nHalfBath         1460 non-null int64\nBedroomAbvGr     1460 non-null int64\nKitchenAbvGr     1460 non-null int64\nKitchenQual      1460 non-null object\nTotRmsAbvGrd     1460 non-null int64\nFunctional       1460 non-null object\nFireplaces       1460 non-null int64\nFireplaceQu      770 non-null object\nGarageType       1379 non-null object\nGarageYrBlt      1379 non-null float64\nGarageFinish     1379 non-null object\nGarageCars       1460 non-null int64\nGarageArea       1460 non-null int64\nGarageQual       1379 non-null object\nGarageCond       1379 non-null object\nPavedDrive       1460 non-null object\nWoodDeckSF       1460 non-null int64\nOpenPorchSF      1460 non-null int64\nEnclosedPorch    1460 non-null int64\n3SsnPorch        1460 non-null int64\nScreenPorch      1460 non-null int64\nPoolArea         1460 non-null int64\nPoolQC           7 non-null object\nFence            281 non-null object\nMiscFeature      54 non-null object\nMiscVal          1460 non-null int64\nMoSold           1460 non-null int64\nYrSold           1460 non-null int64\nSaleType         1460 non-null object\nSaleCondition    1460 non-null object\nSalePrice        1460 non-null int64\ndtypes: float64(3), int64(35), object(43)\nmemory usage: 924.0+ KB\n</code></pre> <p>Now use the <code>.describe()</code> method on the full data set to get the min,max,mean and other values for each column. Try adding in the <code>.transpose()</code> method to make it a little more readable.</p> <pre><code>#Type your solution here. It should have the following output\n</code></pre> count mean std min 25% 50% 75% max 1460.0 730.5 421.6100093688479 1.0 365.75 730.5 1095.25 1460.0 1460.0 56.897260273972606 42.30057099381035 20.0 20.0 50.0 70.0 190.0 1201.0 70.04995836802665 24.284751774483183 21.0 59.0 69.0 80.0 313.0 1460.0 10516.828082191782 9981.264932379147 1300.0 7553.5 9478.5 11601.5 215245.0 1460.0 6.0993150684931505 1.3829965467415923 1.0 5.0 6.0 7.0 10.0 1460.0 5.575342465753424 1.1127993367127367 1.0 5.0 5.0 6.0 9.0 1460.0 1971.267808219178 30.202904042525265 1872.0 1954.0 1973.0 2000.0 2010.0 1460.0 1984.8657534246574 20.645406807709396 1950.0 1967.0 1994.0 2004.0 2010.0 1452.0 103.68526170798899 181.06620658721818 0.0 0.0 0.0 166.0 1600.0 1460.0 443.6397260273973 456.09809084092456 0.0 0.0 383.5 712.25 5644.0 1460.0 46.54931506849315 161.31927280654057 0.0 0.0 0.0 0.0 1474.0 1460.0 567.2404109589041 441.8669552924342 0.0 223.0 477.5 808.0 2336.0 1460.0 1057.4294520547944 438.7053244594705 0.0 795.75 991.5 1298.25 6110.0 1460.0 1162.626712328767 386.5877380410738 334.0 882.0 1087.0 1391.25 4692.0 1460.0 346.99246575342465 436.5284358862591 0.0 0.0 0.0 728.0 2065.0 1460.0 5.844520547945206 48.623081433519125 0.0 0.0 0.0 0.0 572.0 1460.0 1515.463698630137 525.4803834232027 334.0 1129.5 1464.0 1776.75 5642.0 1460.0 0.42534246575342466 0.5189106060897992 0.0 0.0 0.0 1.0 3.0 1460.0 0.057534246575342465 0.23875264627920764 0.0 0.0 0.0 0.0 2.0 1460.0 1.5650684931506849 0.5509158012954318 0.0 1.0 2.0 2.0 3.0 1460.0 0.38287671232876713 0.5028853810928973 0.0 0.0 0.0 1.0 2.0 1460.0 2.8664383561643834 0.8157780441442212 0.0 2.0 3.0 3.0 8.0 1460.0 1.0465753424657533 0.22033819838402977 0.0 1.0 1.0 1.0 3.0 1460.0 6.517808219178082 1.625393290584064 2.0 5.0 6.0 7.0 14.0 1460.0 0.613013698630137 0.6446663863122344 0.0 0.0 1.0 1.0 3.0 1379.0 1978.5061638868744 24.689724768590214 1900.0 1961.0 1980.0 2002.0 2010.0 1460.0 1.7671232876712328 0.7473150101111116 0.0 1.0 2.0 2.0 4.0 1460.0 472.9801369863014 213.80484145338076 0.0 334.5 480.0 576.0 1418.0 1460.0 94.2445205479452 125.33879435172359 0.0 0.0 0.0 168.0 857.0 1460.0 46.66027397260274 66.25602767664974 0.0 0.0 25.0 68.0 547.0 1460.0 21.954109589041096 61.11914860172879 0.0 0.0 0.0 0.0 552.0 1460.0 3.4095890410958902 29.317330556782203 0.0 0.0 0.0 0.0 508.0 1460.0 15.060958904109588 55.757415281874486 0.0 0.0 0.0 0.0 480.0 1460.0 2.758904109589041 40.17730694453043 0.0 0.0 0.0 0.0 738.0 1460.0 43.489041095890414 496.1230244579311 0.0 0.0 0.0 0.0 15500.0 1460.0 6.321917808219178 2.7036262083595197 1.0 5.0 6.0 8.0 12.0 1460.0 2007.8157534246575 1.328095120552104 2006.0 2007.0 2008.0 2009.0 2010.0 1460.0 180921.19589041095 79442.50288288663 34900.0 129975.0 163000.0 214000.0 755000.0 <p>Now lets code up a script to create a bar chart that counts the number of missing values in each of the predictor columns. Intutively, we could suggest that a feature with a lot of missing values might not be important. This may or may not be true, but it's important to know what is and isn't there to begin with.</p> <pre><code>#Type your solution here. It should have the following output\n</code></pre> <p></p>"},{"location":"research/2018-10-20-AMES-tutorial/#iiifeature-engineering-data-cleaning","title":"III.Feature engineering &amp; Data Cleaning","text":"<p>Cleaning data and manipulating features is a major part of any project. In this part of a data science project we solve problems such as:</p> <ul> <li>Turning categorical (string) variables into numerical ones</li> <li>Dealing with missing values by changing them or removing them</li> <li>Dealing with outliers by changing them or removing them</li> <li>Make corrections to data types so machine learning algorithms can read them; e.g. removing a dollar sign to conver <code>'$4.51'</code> to <code>4.51</code> so it's a float</li> <li>Grouping columns of different data types to be dealt with differently</li> </ul> <p>First, lets seperate the numerical predictors from the cateogircal (string) ones. Do this by going through the list of columns and selecting those with <code>.dtypes(column)=='object'</code> for categorical, and not equal for numerical.</p> <pre><code>#Type your solution here. It should have the following output\n</code></pre> <p>Now lets look at the categorical features. Print the shape and display the head of the table.</p> <pre><code>#Type your solution here. It should have the following output\n</code></pre> <pre><code>(1460, 43)\n</code></pre> MSZoning Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinType2 Heating HeatingQC CentralAir Electrical KitchenQual Functional FireplaceQu GarageType GarageFinish GarageQual GarageCond PavedDrive PoolQC Fence MiscFeature SaleType SaleCondition RL Pave Reg Lvl AllPub Inside Gtl CollgCr Norm Norm 1Fam 2Story Gable CompShg VinylSd VinylSd BrkFace Gd TA PConc Gd TA No GLQ Unf GasA Ex Y SBrkr Gd Typ Attchd RFn TA TA Y WD Normal RL Pave Reg Lvl AllPub FR2 Gtl Veenker Feedr Norm 1Fam 1Story Gable CompShg MetalSd MetalSd None TA TA CBlock Gd TA Gd ALQ Unf GasA Ex Y SBrkr TA Typ TA Attchd RFn TA TA Y WD Normal RL Pave IR1 Lvl AllPub Inside Gtl CollgCr Norm Norm 1Fam 2Story Gable CompShg VinylSd VinylSd BrkFace Gd TA PConc Gd TA Mn GLQ Unf GasA Ex Y SBrkr Gd Typ TA Attchd RFn TA TA Y WD Normal RL Pave IR1 Lvl AllPub Corner Gtl Crawfor Norm Norm 1Fam 2Story Gable CompShg Wd Sdng Wd Shng None TA TA BrkTil TA Gd No ALQ Unf GasA Gd Y SBrkr Gd Typ Gd Detchd Unf TA TA Y WD Abnorml RL Pave IR1 Lvl AllPub FR2 Gtl NoRidge Norm Norm 1Fam 2Story Gable CompShg VinylSd VinylSd BrkFace Gd TA PConc Gd TA Av GLQ Unf GasA Ex Y SBrkr Gd Typ TA Attchd RFn TA TA Y WD Normal <p>Now we have to convert this categorical data into numerical data. This is done using the pandas <code>.get_dummies(data)</code> function, which basically makes a new column for each categorical variable in a given column, and puts a value of 1 if that category is present for that row. The down side of doing this is the number of variables then blows up, which we will deal with later during feature selection.</p> <p>Acting on the original table <code>df</code>, we get dummies of the categories, drop the categories from the table <code>df</code> and then concatenate the dummies to table <code>df</code>. This will lead us with only numbers in our dataframe, no strings.</p> <p>If you did it right you should now have 290 columns.</p> <pre><code>#Type your solution here. It should have the following output\n</code></pre> Id MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr TotRmsAbvGrd Fireplaces GarageYrBlt GarageCars GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold SalePrice MSZoning_C (all) MSZoning_FV MSZoning_RH MSZoning_RL MSZoning_RM Street_Grvl Street_Pave Alley_Grvl Alley_Pave LotShape_IR1 LotShape_IR2 LotShape_IR3 LotShape_Reg LandContour_Bnk LandContour_HLS LandContour_Low LandContour_Lvl Utilities_AllPub Utilities_NoSeWa LotConfig_Corner LotConfig_CulDSac LotConfig_FR2 LotConfig_FR3 LotConfig_Inside LandSlope_Gtl LandSlope_Mod LandSlope_Sev Neighborhood_Blmngtn Neighborhood_Blueste Neighborhood_BrDale Neighborhood_BrkSide Neighborhood_ClearCr Neighborhood_CollgCr Neighborhood_Crawfor Neighborhood_Edwards Neighborhood_Gilbert Neighborhood_IDOTRR Neighborhood_MeadowV Neighborhood_Mitchel Neighborhood_NAmes Neighborhood_NPkVill Neighborhood_NWAmes Neighborhood_NoRidge Neighborhood_NridgHt Neighborhood_OldTown Neighborhood_SWISU Neighborhood_Sawyer Neighborhood_SawyerW Neighborhood_Somerst Neighborhood_StoneBr Neighborhood_Timber Neighborhood_Veenker Condition1_Artery Condition1_Feedr Condition1_Norm Condition1_PosA Condition1_PosN Condition1_RRAe Condition1_RRAn Condition1_RRNe Condition1_RRNn Condition2_Artery Condition2_Feedr Condition2_Norm Condition2_PosA Condition2_PosN Condition2_RRAe Condition2_RRAn Condition2_RRNn BldgType_1Fam BldgType_2fmCon BldgType_Duplex BldgType_Twnhs BldgType_TwnhsE HouseStyle_1.5Fin HouseStyle_1.5Unf HouseStyle_1Story HouseStyle_2.5Fin HouseStyle_2.5Unf HouseStyle_2Story HouseStyle_SFoyer HouseStyle_SLvl RoofStyle_Flat RoofStyle_Gable RoofStyle_Gambrel RoofStyle_Hip RoofStyle_Mansard RoofStyle_Shed RoofMatl_ClyTile RoofMatl_CompShg RoofMatl_Membran RoofMatl_Metal RoofMatl_Roll RoofMatl_Tar&amp;Grv RoofMatl_WdShake RoofMatl_WdShngl Exterior1st_AsbShng Exterior1st_AsphShn Exterior1st_BrkComm Exterior1st_BrkFace Exterior1st_CBlock Exterior1st_CemntBd Exterior1st_HdBoard Exterior1st_ImStucc Exterior1st_MetalSd Exterior1st_Plywood Exterior1st_Stone Exterior1st_Stucco Exterior1st_VinylSd Exterior1st_Wd Sdng Exterior1st_WdShing Exterior2nd_AsbShng Exterior2nd_AsphShn Exterior2nd_Brk Cmn Exterior2nd_BrkFace Exterior2nd_CBlock Exterior2nd_CmentBd Exterior2nd_HdBoard Exterior2nd_ImStucc Exterior2nd_MetalSd Exterior2nd_Other Exterior2nd_Plywood Exterior2nd_Stone Exterior2nd_Stucco Exterior2nd_VinylSd Exterior2nd_Wd Sdng Exterior2nd_Wd Shng MasVnrType_BrkCmn MasVnrType_BrkFace MasVnrType_None MasVnrType_Stone ExterQual_Ex ExterQual_Fa ExterQual_Gd ExterQual_TA ExterCond_Ex ExterCond_Fa ExterCond_Gd ExterCond_Po ExterCond_TA Foundation_BrkTil Foundation_CBlock Foundation_PConc Foundation_Slab Foundation_Stone Foundation_Wood BsmtQual_Ex BsmtQual_Fa BsmtQual_Gd BsmtQual_TA BsmtCond_Fa BsmtCond_Gd BsmtCond_Po BsmtCond_TA BsmtExposure_Av BsmtExposure_Gd BsmtExposure_Mn BsmtExposure_No BsmtFinType1_ALQ BsmtFinType1_BLQ BsmtFinType1_GLQ BsmtFinType1_LwQ BsmtFinType1_Rec BsmtFinType1_Unf BsmtFinType2_ALQ BsmtFinType2_BLQ BsmtFinType2_GLQ BsmtFinType2_LwQ BsmtFinType2_Rec BsmtFinType2_Unf Heating_Floor Heating_GasA Heating_GasW Heating_Grav Heating_OthW Heating_Wall HeatingQC_Ex HeatingQC_Fa HeatingQC_Gd HeatingQC_Po HeatingQC_TA CentralAir_N CentralAir_Y Electrical_FuseA Electrical_FuseF Electrical_FuseP Electrical_Mix Electrical_SBrkr KitchenQual_Ex KitchenQual_Fa KitchenQual_Gd KitchenQual_TA Functional_Maj1 Functional_Maj2 Functional_Min1 Functional_Min2 Functional_Mod Functional_Sev Functional_Typ FireplaceQu_Ex FireplaceQu_Fa FireplaceQu_Gd FireplaceQu_Po FireplaceQu_TA GarageType_2Types GarageType_Attchd GarageType_Basment GarageType_BuiltIn GarageType_CarPort GarageType_Detchd GarageFinish_Fin GarageFinish_RFn GarageFinish_Unf GarageQual_Ex GarageQual_Fa GarageQual_Gd GarageQual_Po GarageQual_TA GarageCond_Ex GarageCond_Fa GarageCond_Gd GarageCond_Po GarageCond_TA PavedDrive_N PavedDrive_P PavedDrive_Y PoolQC_Ex PoolQC_Fa PoolQC_Gd Fence_GdPrv Fence_GdWo Fence_MnPrv Fence_MnWw MiscFeature_Gar2 MiscFeature_Othr MiscFeature_Shed MiscFeature_TenC SaleType_COD SaleType_CWD SaleType_Con SaleType_ConLD SaleType_ConLI SaleType_ConLw SaleType_New SaleType_Oth SaleType_WD SaleCondition_Abnorml SaleCondition_AdjLand SaleCondition_Alloca SaleCondition_Family SaleCondition_Normal SaleCondition_Partial 1 60 65.0 8450 7 5 2003 2003 196.0 706 0 150 856 856 854 0 1710 1 0 2 1 3 1 8 0 2003.0 2 548 0 61 0 0 0 0 0 2 2008 208500 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 2 20 80.0 9600 6 8 1976 1976 0.0 978 0 284 1262 1262 0 0 1262 0 1 2 0 3 1 6 1 1976.0 2 460 298 0 0 0 0 0 0 5 2007 181500 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 3 60 68.0 11250 7 5 2001 2002 162.0 486 0 434 920 920 866 0 1786 1 0 2 1 3 1 6 1 2001.0 2 608 0 42 0 0 0 0 0 9 2008 223500 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 4 70 60.0 9550 7 5 1915 1970 0.0 216 0 540 756 961 756 0 1717 1 0 1 0 3 1 7 1 1998.0 3 642 0 35 272 0 0 0 0 2 2006 140000 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 5 60 84.0 14260 8 5 2000 2000 350.0 655 0 490 1145 1145 1053 0 2198 1 0 2 1 4 1 9 1 2000.0 3 836 192 84 0 0 0 0 0 12 2008 250000 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0"},{"location":"research/2018-10-20-AMES-tutorial/#ivfeature-selection","title":"IV.Feature Selection","text":"<p>Now that we've engineered our features, we should create a dataframe or series that shows the pearson correlation coefficient between the sale price and each feature. Get the histogram of this dataframe. Also print the standard deviation, mean, max and min values of the correlations.</p> <pre><code>#Type your solution here. It should have the following output\n</code></pre> <p></p> <pre><code>Max Correlation:  1.0\nMin Correlation:  -0.589043523409763\nMean Correlation:  0.03910614218619199\nStandard Deviation:  0.2129964043689813\n\nSalePrice                1.000000\nOverallQual              0.790982\nGrLivArea                0.708624\nGarageCars               0.640409\nGarageArea               0.623431\nTotalBsmtSF              0.613581\n1stFlrSF                 0.605852\nFullBath                 0.560664\nBsmtQual_Ex              0.553105\nTotRmsAbvGrd             0.533723\nYearBuilt                0.522897\nYearRemodAdd             0.507101\nKitchenQual_Ex           0.504094\nFoundation_PConc         0.497734\nGarageYrBlt              0.486362\nMasVnrArea               0.477493\nFireplaces               0.466929\nExterQual_Gd             0.452466\nExterQual_Ex             0.451164\nBsmtFinType1_GLQ         0.434597\nHeatingQC_Ex             0.434543\nGarageFinish_Fin         0.419678\nNeighborhood_NridgHt     0.402149\nBsmtFinSF1               0.386420\nSaleType_New             0.357509\nSaleCondition_Partial    0.352060\nLotFrontage              0.351799\nFireplaceQu_Gd           0.339329\nGarageType_Attchd        0.335961\nMasVnrType_Stone         0.330476\n                           ...   \nFence_MnPrv             -0.140613\nNeighborhood_BrkSide    -0.143648\nSaleCondition_Normal    -0.153990\nKitchenQual_Fa          -0.157199\nExterior1st_Wd Sdng     -0.158619\nExterior2nd_Wd Sdng     -0.161800\nExterior2nd_MetalSd     -0.162389\nHouseStyle_1.5Fin       -0.163466\nNeighborhood_IDOTRR     -0.164056\nExterior1st_MetalSd     -0.167068\nNeighborhood_Edwards    -0.179949\nNeighborhood_NAmes      -0.188513\nNeighborhood_OldTown    -0.192189\nElectrical_FuseA        -0.193978\nFoundation_BrkTil       -0.204117\nPavedDrive_N            -0.212630\nRoofStyle_Gable         -0.224744\nSaleType_WD             -0.242598\nCentralAir_N            -0.251328\nBsmtExposure_No         -0.263600\nLotShape_Reg            -0.267672\nMSZoning_RM             -0.288065\nHeatingQC_TA            -0.312677\nFoundation_CBlock       -0.343263\nGarageType_Detchd       -0.354141\nMasVnrType_None         -0.374468\nGarageFinish_Unf        -0.410608\nBsmtQual_TA             -0.452394\nKitchenQual_TA          -0.519298\nExterQual_TA            -0.589044\nName: SalePrice, Length: 290, dtype: float64\n</code></pre> <p>So the if the correlation between the sale price and a feature is very small or close to zero, it's valid to say that such a feature has little to no influence on our price. The important features are going to be ones that have a significantly large enough absolute value of their correlation coefficient.</p> <p>Thus we could specify a threshold of correlation that selects only features whose correlation with the SalePrice has an absolute value greater than that threshold. This could take a little bit of guess work, but we will try with a correlation greater than \\(\\|\\pm0.4\\|\\).</p> <p>Using the series of correlation coefficients just made, select all the ones with an absolute value greater than 0.4, and select the features corresponding to the coefficients. You should have 26 features. Call these features the <code>key_features</code>.</p> <pre><code>#Code your solution here, it should have the following output\n</code></pre> <pre><code>26\n</code></pre> <p>Now lets scatter plot each key feature compared to the sale price. This will allow us to detect things like skews in the data, which we can impute/transform in the following steps.</p> <pre><code>#Code your solution here, it should have the following output\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Its very obvious that some of these features are very skewed. Moreover, the magnitude of some of the features are on significantly different scales. For example, the values for <code>GarageFinish_Unf</code> have a range of 0 and 1, but the feature <code>GarageYrBuilt</code> ranges from 1900 to 2010. Machine learning models work best with unskewed data which is all on a similar scale.</p> <p>So what can we do? We can use a data-transformation technique that allows us to get all the data on the same scale, and (hopefully) removes the skew in the data.</p> <p>Do this in several steps:</p> <ul> <li>For the missing values in the original data frame, impute the mean value into each column.</li> <li>From <code>sklearn.preprocessing</code>, use <code>StandardScaler</code> to transform our our columns into a standardized format</li> </ul> <pre><code>#Code your solution here\n</code></pre> <p>And what about our target variable, the <code>SalePrice</code>? Should it be transformed as well? Using the <code>seaborn</code> function <code>.distplot(data,fit=norm)</code>, plot the sale price. Below it, plot the <code>stats.probplot(data,plot=plt)</code> as well and see what it looks like. Make you have imported all the right things.</p> <p>This accomplishes two tasks:</p> <ul> <li>Distplot will allow us to see how close our SalePrices are to a normal distribution, by creating a historgram.</li> <li>Probplot will allow us to see a Q-Q plot of our SalesPrices, generating a probability of sample data against the quantiles of a normal distribution.</li> </ul> <pre><code>#Code your solution here, it should have the following output\n</code></pre> <p></p> <p></p> <p>Now do the same thing again, but transform the <code>SalePrice</code> data with the function <code>np.log1p(data)</code>.</p> <pre><code>#Code your solution here, it should have the following output\n</code></pre> <p></p> <p></p> <p>As we can see the transformation of <code>np.log1p(y)</code> makes our SalePrice data much more normal and minimizes skew. This is better for linear regression.</p> <p>So what does this mean? It means that every price will be transformed such that \\(y \\rightarrow \\ln(y+1)\\)</p> <p>This also means that when our model predicts values, we must transform it back to its original form to get the predicted price, using the inverse function \\(\\ln(y+1) \\rightarrow e^{y}-1\\), or  <code>np.exp(y)-1</code>.</p>"},{"location":"research/2018-10-20-AMES-tutorial/#vmodel-building","title":"V.Model Building","text":"<p>Ridge regression is one of the most common regularization techniques used in regession tasks. It is easy to use, and we'll let it be our model we work with. We need to fit it to our data to tune it's hyperparameter. We can find the right hyperparameter using cross validation.</p> <ul> <li>In the following cell, import Ridge and cross_val_score</li> <li>Create a function that returns the square root of a five fold cross validation of the negative value for the scoring metric. <code>scoring=\"neg_mean_squared_error\"</code></li> <li>Using the <code>np.linspace()</code> function, create a series of values, between 50 and 70, that we will test as our hyperparameter alpha.</li> <li>Create another series that finds that average cross validation score for each alpha.</li> <li>Make a matplotlib plot that has the alphas on the x-axis, and the RMSE on the vertical.</li> <li>For this to work, you should have used <code>StandardScaler()</code> to transform the features, and <code>np.log1p(y)</code> on the <code>SalePrice</code>.</li> <li>Find the minimum value of alpha - this will be our hyperparameter.</li> </ul> <p>If you did everything right, it should the plots should look like the next one:</p> <pre><code>#Code your solution here, it should have the following output\n</code></pre> <pre><code>Best alpha for ridge is:  62.6530612244898\n</code></pre> <p></p>"},{"location":"research/2018-10-20-AMES-tutorial/#vimodel-development-testing","title":"VI.Model Development &amp; Testing","text":"<p>So now that we've cleaned data, selected the features, transformed them and found the right hyperparameter for our model, it's time to use test our model. Heres what we do next:</p> <ul> <li>Do a <code>train_test_split</code> with a 77-33% split to test</li> <li>Convert the predicted values into actual prices via the inverse function, <code>np.exp(y_pred)+1</code>, and convert the test values of y to actual prices.</li> <li>Plot the predicted prices vs the actual prices </li> <li>Plot the residuals of the predictions using the <code>ResidualsPlot</code> from the <code>yellowbricks.regressor</code> package (install it if you dont have it)!</li> <li>Using the yellow bricks plot, observe the normality of errors and R2 score of the train and test set!</li> </ul> <pre><code>#Code your solution here, it should have the following output\n</code></pre> <pre><code>If you did it correctly, your plot should looks like this:\n</code></pre> <p></p> <pre><code>#Code your solution here, it should have the following output\n</code></pre> <pre><code>Your plot should look like this:\n</code></pre> <p></p> <p>Lastly, make a horizontal bar plot of the coefficients in the ridge model. * Use the attribute <code>model.coef_</code> to get the coefficients. Put them into a series</p> <pre><code>#Code your solution here, it should have the following output\n</code></pre> <pre><code>Your plot should look like this.\n</code></pre> <p></p>"},{"location":"research/2018-10-20-AMES-tutorial/#viisources-credits","title":"VII.Sources &amp; Credits","text":"<p>Some of these tutorials were very helpful in writing this lesson.</p> <ul> <li>https://www.kaggle.com/c/house-prices-advanced-regression-techniques#tutorials</li> <li>https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python</li> <li>https://www.kaggle.com/dgawlik/house-prices-eda</li> <li>https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset</li> <li>https://www.kaggle.com/apapiu/regularized-linear-models</li> </ul>"},{"location":"research/2018-12-15-mushroom-classification/","title":"Is a mushroom safe to eat? Or is it deadly?","text":"<p>By Joe Ganser</p>"},{"location":"research/2018-12-15-mushroom-classification/#abstract","title":"Abstract","text":"<p>In this analysis, a classification model is run on data attempting to classify mushrooms as poisnous or edible. The data itsself is entirely nominal and categorical. The data comes from a kaggle competition and is also found on the UCI Machine learning repository. The objectives included finding the best performing model and drawing conclusions about mushroom taxonomy.</p> <p> </p>"},{"location":"research/2018-12-15-mushroom-classification/#iintroduction","title":"I.Introduction","text":"<p>Occam's razor, also known as the law of parsimony, is perhaps one of the most important principles of all science. The theory based upon the least assumptions tends to be the correct one. In the case of machine learning, a corollary condition could be proposed; the best machine learning models not only require the best performance metrics, but should also require the least amount of data and processing time as well.</p> <p>In this analysis, my objective was to built a model with the highest performance metrics (accuracy and F1 score) using the least amount of data and operating in the shortest amount of time.</p> <p>In conjunction, I wanted to determine what the key factors where in classifying a mushroom as poisonous or edible.</p> <p>Background info</p> <p>Categorizing something as poisnous versus edible wouldn't be a problem taken lightly. If you had any  margin of error, someone could die. Thus, any model that predicts whether or not a mushroom is poisonous or edible needs to have perfect accuracy. At a glance, this is the goal of the data - figure out what to eat versus toss; a typical problem in classification.</p>"},{"location":"research/2018-12-15-mushroom-classification/#iiexploring-cleaning-the-data","title":"II.Exploring &amp; Cleaning the Data","text":"<p>The data itsself was entirely categorical and nominal in structure. In all, the data included 8124 observational rows, and (before cleaning) 23 categorical features. The features were themselves had letter values, with no order structure between the letters. Out of the 8124 rows, 4208 were classified as edible and 3916 were poisonous. The first five rows of the raw data were:</p> class cap-shape cap-surface cap-color bruises odor gill-attachment gill-spacing gill-size gill-color stalk-shape stalk-root stalk-surface-above-ring stalk-surface-below-ring stalk-color-above-ring stalk-color-below-ring veil-type veil-color ring-number ring-type spore-print-color population habitat p x s n t p f c n k e e s s w w p w o p k s u e x s y t a f c b k e c s s w w p w o p n n g e b s w t l f c b n e c s s w w p w o p n n m p x y w t p f c n n e e s s w w p w o p k s u e x s g f n f w b k t e s s w w p w o e n a g <p>Where \"class\" was the target, and p was for poisnonous and e was for edible.</p> <p>Data pre-processing</p> <p>Obviosuly a machine learning model wouldn't be able to process letters when there should be numbers, so an encoding process was waranted. Using the pandas <code>.get_dummies()</code> function I was able to generate a table filled with entirely binary data, where 1 is present if a feature of a given column was present, and 0 otherwise.</p> <p>After converting to binary format, the original 23 columns were transformed to 117 columns. No rows were dropped.</p> <p>Identifying irrelevant features</p> <p>Feature selection decisions were made based upon filtering methods. But before determining the level of influence of each feature, I wanted to find out which features were totally useless. To do this, two methods were used.</p> <ol> <li>Chi-Square hypothesis testing, on the data in it's raw form (1 irrelevant feature found).</li> <li>Bootstrap hypothesis testing of each feature's mean difference between the poisonous and edibles, after the data was converted into binary form (4 irrelevant features found).</li> </ol> <p>In both cases, the null hypothesis was that the distribution of a feature was NOT the same for both the edibles and the poisonous mushrooms. A for loop acted across all the features in the cleaned format, and hypothesis testing was done on each one. After useless features were found, they were discarded. In all, it was found the five features were irrelevant and had no influence determining the category. The data for modelling was then reduced to 112 columns.</p> <p>Selecting important features by filtration</p> <p>Once the data was in binary form, a histogram plot between the correlation of each feature and the class (the target) was made. Using the values of the correlations, a trial and error process was done by fitting an assortment of classification models to a set of features that had a magnitude (absolute value) greater than a threshold correlation value. It was found that all the set of features with a magnitude greater than  abs(\u00b10.34847) was enough data to produce a model that performed with perfect accuracy on a 70-30 train test split. There were 19 features (out of 112) that met this criteria.</p> <p> </p> <p>.</p> <p>Feature Ranking</p> <p>After converting into binary form, features were then fed into the models and ranked descendingly in accordance to the magnitude of their correlation coefficient with the target variable, <code>class</code>. Thus the first feature fed into the model had the highest magnitude of correlation, the second had the second highest, and so on. The first five rows of the feature rank table looked like this;</p> Rank Feature Correlation with target 1 odor_n -0.7855566222367798 2 odor_f 0.6238419598140276 3 stalk-surface-above-ring_k 0.587658257630713 4 stalk-surface-below-ring_k 0.5735240117095786 5 ring-type_p -0.5404689127748091 <p>And so on, upto all 112 engineered features. The 19 most important features will be discussed below.</p> <p>A for loop was designed to feed the five different models sets of data features in order of their correlation rank. So at the first iteration the models were fitted and evaluated on the first feature <code>odor_n</code>, in the second iteration the models were fitted and evaluated on the first two features (<code>odor_n</code> and <code>odor_f</code>), the third iteration used the first three features (<code>ordor_n</code>,<code>odor_f</code>,<code>stalk-surface-above-ring_k</code>), and so on.</p> <p>Hence the loop to build the models went as such;</p> <p><code>for indices in feature_ranks.index:     models.fit(data[feature_ranks['Feature'].loc[:indices]],data['class'])     models.predict(data[feature_ranks['Feature'].loc[:indices]],data['class'])     evaluate models, etc.</code></p>"},{"location":"research/2018-12-15-mushroom-classification/#iii-model-selection","title":"III. Model Selection","text":"<p>Multiple models were chosen for evaluation. These included:</p> <ul> <li>logistic regression</li> <li>K-nearest-neighbors</li> <li>support vector machine</li> <li>decision tree classifier</li> <li>random forest classifier </li> </ul> <p>Model performance was evaluated on:</p> <ul> <li>Accuracy score</li> <li>F1 score</li> <li>The minimum number of features needed to achieve the models highest metrics</li> <li>Combined time of training plus predicting</li> </ul> <p>Each model was fed through the previously mentioned for-loop and evaluated on a 70-30 train test split. As we can see from the graphs below, it was the top 19 ranked features that most of the models began to score with perfect accuracy. </p> <p>Despite random forest, k-nearest-neighbors and decision trees all getting perfect scores when fed 19 features, it was decision trees which performed in the shortest amount time. Thus, decision tree classifier was the best model.</p> <p> </p>"},{"location":"research/2018-12-15-mushroom-classification/#iv-model-evaluation-decision-tree-classifier","title":"IV. Model Evaluation: Decision Tree Classifier","text":"<p>Decision tree classifier was the model which met the criteria of the performing in the least amount of time, with the least number of features and having maximum performance metrics on F1 and accuracy scores.</p> Metric Value Accuracy 1.0000 F1 score 1.0000 AUC 1.0000 Number of features used 19 out of 112 train+predict time 0.006112(s) Test set size 2438 (30%) Train set size 5686 (70%) <p>Specifically, the hyperparameters and roc-auc curve were;</p> <p> </p>  With a confusion matrix of;  <p> </p> <p>Comments on perfect accuracy metrics</p> <p>Though its not common to get perfect scores on models, it does happen. Moreover, it was quite obvious that many other who have worked with this data set on the kaggle competition  achieved perfect scoring metrics as well. </p>"},{"location":"research/2018-12-15-mushroom-classification/#vconclusions","title":"V.Conclusions","text":"<p>Using only 19 pieces of information, we can conclude with 100% certainty that a mushroom is edible or poisonous. These are the 19 features, ranked in descending order by the absolute value with their correlation with the target, class. Recall that in the target class, edible was marked as 0 and poisonous was marked at 1.</p> <p>A negative correlation means if a mushroom has that feature it is more likely to be edible</p> <p>A positive correlation means if a mushroom has that feature it is more likely to be poisonous.</p> Rank Feature Correlation with target 1 odor_n -0.7855566222367798 2 odor_f 0.6238419598140276 3 stalk-surface-above-ring_k 0.587658257630713 4 stalk-surface-below-ring_k 0.5735240117095786 5 ring-type_p -0.5404689127748091 6 gill-size_n 0.5400243574329782 7 gill-size_b -0.5400243574329637 8 gill-color_b 0.5388081615534243 9 bruises_f 0.5015303774076804 10 bruises_t -0.5015303774076804 11 stalk-surface-above-ring_s -0.49131418071172045 12 spore-print-color_h 0.49022917056952875 13 ring-type_l 0.4516190663173296 14 population_v 0.44372237905537937 15 stalk-surface-below-ring_s -0.42544404585499374 16 spore-print-color_n -0.41664529710731296 17 spore-print-color_k -0.3968322009032092 18 spore-print-color_w 0.3573840060325314 19 gill-spacing_c 0.34838678518425714 <p>Out original features (before engineering), the 19 listed above were engineered from 9 of the 22 originals. They were as follows;</p> Rank Feature 1 Odor 2 Stalk-surface-above-ring 3 Stalk-surface-below-ring 4 Ring type 5 Gill size 6 Bruises 7 Spore-print-color 8 population 9 Gill-spacing <p>Interpreting these features</p> <p>The decision tree model has a workflow which helps us draw conclusions.</p> <p>Starting at the top, for a given row (i.e. a given mushroom) if the feature <code>odor_n &lt;=0.5</code> (which really means <code>odor_n = 0</code> or <code>odor_none=False</code>, or it has an odor) AND it <code>bruises_t &lt;=0.5</code> (i.e. <code>bruises_t = 0</code> or, the mushroom does NOT bruise), then we conclude the mushroom is poisonous. More conclusions can be made simply by following the tree.</p> <p> </p>"},{"location":"research/2018-12-15-mushroom-classification/#vireferences","title":"VI.References","text":"<p>Links to my codeded notebooks to build the project</p> <ul> <li>Cleaning the data</li> <li>Model Selection part 1</li> <li>Model Selection part 2</li> <li>Model Evaluation</li> </ul> <p>Links/other sources</p> <ul> <li>The original kaggle competition</li> <li>UCI Machine learning repository, mushroom data set</li> </ul>"},{"location":"research/2019-1-26-superconductivity-regression/","title":"Superconductors - using material data to predict critical temperature","text":"<p>By Joe Ganser</p>"},{"location":"research/2019-1-26-superconductivity-regression/#abstract","title":"Abstract","text":"<p>Data on superconductors was studied with the goal of predicting the critical temperature at which a material undergoes superconductivity; hence a regession task. The data initially consisted of 168 features and 21,263 rows. The goal was not to predict whether or not a material was superconductive, because all data was on superconductive materials. Another researcher had gotten a root mean squared error (RMSE) of \u00b19.5kelvin on this data set, and this project's goal was to out perform this metric while comparing model techniques and analyzing features. This study was able to get \u00b19.4Kelvin RMSE. The data is originally sourced from the superconducting material database. This paper is oriented towards an audience of scientists, and was coded in <code>python 3.6</code>.</p>"},{"location":"research/2019-1-26-superconductivity-regression/#i-introduction","title":"I. Introduction","text":"<p>There were several objectives to this project:</p> <ul> <li>Predict the temperature of superconductivity with the highest possible R2 score and lowest possible RMSE</li> <li>Compare multiple model techniques and find the one that produces the best metrics</li> <li>Identify relevant and irrelevant features</li> <li>Ensure model validity</li> </ul> <p>The approach was to first identify the irrelevant features and discard them, followed by putting the remaining features into a bunch of models for wide comparison.</p>"},{"location":"research/2019-1-26-superconductivity-regression/#ii-what-is-a-superconductor","title":"II. What is a superconductor?","text":"<p>A superconductor is a material that, at a very, very low temperature allows for infinite conductivity (zero electrical resistance). Essentialy this means thats all the electrons/ions will flow on a material without any disturbance what so ever. One of the most interesting things about superconductors is that at their critical temperature they can create magnetic levitation! This is called the Meissner effect. Lexus, the car company, used this phenomenon to create a hoverboard.</p> <p> </p> <p>Superconducting temperature</p> <p>The temperature of at which a material becomes superconductive was what I was trying to predict in this analysis. So why is temperature relevant to super conductivity? To understand this, consider a familiar example of something that's the complete opposite.</p> <p>A toaster works using electrical resistance. In the circuit of a toaster, the electrons are forced to slow down and colide with each other, which in turn causes an accumulation of thermal energy. That thermal energy then raises the temperature, which cooks the toast.</p> <p>The physics of a super conductor is (sort of) like the opposite of a toaster. Instead of electrons bumnping into each other chaotically (creating heat), the cold causes them all move uniformly. When the temperature is low enough, the uniform motion of the electrons becomes comes very coherent, producing powerful magnetic fields.</p> <p>There are many materials that can support superconductivity. Each one is a combination of many different elements, and has many intrinsic properties. Thus by creating a table for each super conductor's properties, along with it's critical temperature, a regression analysis can be done.</p>"},{"location":"research/2019-1-26-superconductivity-regression/#iiipreprocessing-the-target-variable-critical-temperature","title":"III.Preprocessing the target variable: critical temperature","text":"<p>In any regression problem, you want the target variable to be as close to a normal distribution as possible. Sometimes this is possible to do, other times not. In this case I was able to transform the critical temperature to be approximately normal.</p> <p>Originally the data was very, very positively skewed. After a lot of trial and error, I used an exponential transformation to get the data (approximately) normal.</p> <p> </p> <p>Unfortunately in the best fit there remained a slight skew. The highest peak wasn't simply an outlier and couldn't be dropped or imputed. Despite this, the results observed at the end were pretty good.</p>"},{"location":"research/2019-1-26-superconductivity-regression/#ivfiltering-out-irrelevant-features","title":"IV.Filtering out irrelevant features","text":"<p>Initially the data shape was 21263 rows by 168 features. The head of the original dataframe looked like this;</p> number_of_elements mean_atomic_mass wtd_mean_atomic_mass gmean_atomic_mass wtd_gmean_atomic_mass entropy_atomic_mass wtd_entropy_atomic_mass range_atomic_mass wtd_range_atomic_mass std_atomic_mass wtd_std_atomic_mass mean_fie wtd_mean_fie gmean_fie wtd_gmean_fie entropy_fie wtd_entropy_fie range_fie wtd_range_fie std_fie wtd_std_fie mean_atomic_radius wtd_mean_atomic_radius gmean_atomic_radius wtd_gmean_atomic_radius entropy_atomic_radius wtd_entropy_atomic_radius range_atomic_radius wtd_range_atomic_radius std_atomic_radius wtd_std_atomic_radius mean_Density wtd_mean_Density gmean_Density wtd_gmean_Density entropy_Density wtd_entropy_Density range_Density wtd_range_Density std_Density wtd_std_Density mean_ElectronAffinity wtd_mean_ElectronAffinity gmean_ElectronAffinity wtd_gmean_ElectronAffinity entropy_ElectronAffinity wtd_entropy_ElectronAffinity range_ElectronAffinity wtd_range_ElectronAffinity std_ElectronAffinity wtd_std_ElectronAffinity mean_FusionHeat wtd_mean_FusionHeat gmean_FusionHeat wtd_gmean_FusionHeat entropy_FusionHeat wtd_entropy_FusionHeat range_FusionHeat wtd_range_FusionHeat std_FusionHeat wtd_std_FusionHeat mean_ThermalConductivity wtd_mean_ThermalConductivity gmean_ThermalConductivity wtd_gmean_ThermalConductivity entropy_ThermalConductivity wtd_entropy_ThermalConductivity range_ThermalConductivity wtd_range_ThermalConductivity std_ThermalConductivity wtd_std_ThermalConductivity mean_Valence wtd_mean_Valence gmean_Valence wtd_gmean_Valence entropy_Valence wtd_entropy_Valence range_Valence wtd_range_Valence std_Valence wtd_std_Valence critical_temp H He Li Be B C N O F Ne Na Mg Al Si P S Cl Ar K Ca Sc Ti V Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe Cs Ba La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Hf Ta W Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn 4 88.9444675 57.862692285714296 66.36159243157189 36.1166119053847 1.1817952393305 1.06239554519617 122.90607 31.7949208571429 51.968827786103404 53.6225345301219 775.425 1010.26857142857 718.1528999521299 938.016780052204 1.30596703599158 0.791487788469155 810.6 735.9857142857139 323.811807806633 355.562966713294 160.25 105.514285714286 136.126003095455 84.528422716633 1.2592439721428899 1.2070399870146102 205 42.914285714285704 75.23754049674942 69.2355694829807 4654.35725 2961.5022857142894 724.953210852388 53.5438109235142 1.03312880053102 0.814598190091683 8958.571 1579.58342857143 3306.1628967555 3572.5966237083794 81.8375 111.72714285714301 60.1231785550982 99.4146820543113 1.15968659338134 0.7873816907632231 127.05 80.9871428571429 51.4337118896741 42.55839575195 6.9055 3.8468571428571403 3.4794748493632697 1.04098598567486 1.08857534188499 0.994998193254128 12.878 1.7445714285714298 4.599064116752451 4.66691955388659 107.75664499999999 61.0151885714286 7.06248773046785 0.62197948704754 0.308147989812345 0.262848266362233 399.97342000000003 57.12766857142861 168.854243757651 138.51716251123 2.25 2.25714285714286 2.2133638394006403 2.21978342968743 1.36892236074022 1.0662210317362 1 1.08571428571429 0.43301270189221897 0.43705881545081 29.0 0.0 0 0.0 0.0 0.0 0.0 0.0 4.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.2 1.8 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 5 92.729214 58.518416142857106 73.13278722250651 36.3966020291995 1.44930919335685 1.05775512271911 122.90607 36.161939000000004 47.094633170313394 53.9798696513451 766.44 1010.61285714286 720.605510513725 938.745412527433 1.5441445432697298 0.8070782149387309 810.6 743.164285714286 290.183029138508 354.963511171592 161.2 104.971428571429 141.465214777999 84.3701669575628 1.50832754035259 1.2041147982326001 205 50.571428571428605 67.321319060161 68.0088169554027 5821.4858 3021.01657142857 1237.09508033858 54.0957182556368 1.3144421846210501 0.9148021770663429 10488.571000000002 1667.3834285714302 3767.4031757706202 3632.64918471043 90.89 112.316428571429 69.8333146094209 101.166397739874 1.42799655342352 0.83866646563365 127.05 81.2078571428572 49.438167441765096 41.6676207979191 7.7844 3.7968571428571405 4.40379049753476 1.0352511158281401 1.37497728009085 1.07309384625263 12.878 1.59571428571429 4.473362654648071 4.60300005985449 172.20531599999998 61.37233142857139 16.0642278788044 0.6197346323305469 0.847404163195705 0.5677061078766371 429.97342000000003 51.4133828571429 198.554600255545 139.630922368904 2.0 2.25714285714286 1.8881750225898 2.2106794087065498 1.55711309805765 1.04722136819323 2 1.12857142857143 0.6324555320336761 0.468606270481621 26.0 0.0 0 0.0 0.0 0.0 0.0 0.0 4.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.9 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.1 1.9 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 4 88.9444675 57.885241857142894 66.36159243157189 36.1225090359592 1.1817952393305 0.9759804641654979 122.90607 35.741099 51.968827786103404 53.65626773209821 775.425 1010.82 718.1528999521299 939.009035665864 1.30596703599158 0.773620193146673 810.6 743.164285714286 323.811807806633 354.804182855034 160.25 104.685714285714 136.126003095455 84.214573243296 1.2592439721428899 1.13254686280436 205 49.31428571428571 75.23754049674942 67.797712320685 4654.35725 2999.15942857143 724.953210852388 53.9740223651659 1.03312880053102 0.760305152674073 8958.571 1667.3834285714302 3306.1628967555 3592.01928133231 81.8375 112.213571428571 60.1231785550982 101.082152388012 1.15968659338134 0.7860067360250121 127.05 81.2078571428572 51.4337118896741 41.63987779712121 6.9055 3.8225714285714303 3.4794748493632697 1.03743942474191 1.08857534188499 0.927479442031317 12.878 1.75714285714286 4.599064116752451 4.64963546519334 107.75664499999999 60.94376 7.06248773046785 0.6190946827042739 0.308147989812345 0.250477444193951 399.97342000000003 57.12766857142861 168.854243757651 138.54061273834998 2.25 2.27142857142857 2.2133638394006403 2.23267852196607 1.36892236074022 1.02917468729772 1 1.11428571428571 0.43301270189221897 0.444696640464954 19.0 0.0 0 0.0 0.0 0.0 0.0 0.0 4.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.1 1.9 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 4 88.9444675 57.8739670714286 66.36159243157189 36.1195603503211 1.1817952393305 1.0222908923957 122.90607 33.7680099285714 51.968827786103404 53.63940496787 775.425 1010.54428571429 718.1528999521299 938.512776724546 1.30596703599158 0.7832066603612908 810.6 739.575 323.811807806633 355.18388442194396 160.25 105.1 136.126003095455 84.371352045645 1.2592439721428899 1.17303291789271 205 46.11428571428571 75.23754049674942 68.52166497852029 4654.35725 2980.33085714286 724.953210852388 53.758486291021796 1.03312880053102 0.7888885322214609 8958.571 1623.48342857143 3306.1628967555 3582.3705966440502 81.8375 111.970357142857 60.1231785550982 100.24495020209 1.15968659338134 0.7869004893749151 127.05 81.0975 51.4337118896741 42.1023442240235 6.9055 3.83471428571429 3.4794748493632697 1.0392111922717702 1.08857534188499 0.96403104867923 12.878 1.7445714285714298 4.599064116752451 4.658301352402599 107.75664499999999 60.97947428571429 7.06248773046785 0.6205354084838871 0.308147989812345 0.257045108326848 399.97342000000003 57.12766857142861 168.854243757651 138.528892724768 2.25 2.26428571428571 2.2133638394006403 2.2262216392083 1.36892236074022 1.04883427304761 1 1.1 0.43301270189221897 0.440952123830019 22.0 0.0 0 0.0 0.0 0.0 0.0 0.0 4.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.15 1.85 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 4 88.9444675 57.840142714285705 66.36159243157189 36.11071573753821 1.1817952393305 1.12922373013507 122.90607 27.8487427142857 51.968827786103404 53.5887706050743 775.425 1009.7171428571401 718.1528999521299 937.025572960086 1.30596703599158 0.8052296408133571 810.6 728.8071428571429 323.811807806633 356.31928137213 160.25 106.342857142857 136.126003095455 84.8434418389765 1.2592439721428899 1.26119371912948 205 36.514285714285705 75.23754049674942 70.63444843787241 4654.35725 2923.8451428571398 724.953210852388 53.1170285737926 1.03312880053102 0.859810869291435 8958.571 1491.78342857143 3306.1628967555 3552.6686635803203 81.8375 111.24071428571399 60.1231785550982 97.7747186271033 1.15968659338134 0.7873961825201741 127.05 80.76642857142859 51.4337118896741 43.4520592088545 6.9055 3.8711428571428597 3.4794748493632697 1.04454467078021 1.08857534188499 1.04496953590973 12.878 1.7445714285714298 4.599064116752451 4.6840139510763095 107.75664499999999 61.0866171428571 7.06248773046785 0.624877733754845 0.308147989812345 0.272819938850094 399.97342000000003 57.12766857142861 168.854243757651 138.493671473918 2.25 2.24285714285714 2.2133638394006403 2.20696281450132 1.36892236074022 1.0960519179005401 1 1.05714285714286 0.43301270189221897 0.42880945770867496 23.0 0.0 0 0.0 0.0 0.0 0.0 0.0 4.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0.0 0.3 1.7 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 <p>Thus, several steps were taken to eliminate the irrelevant features. Specifically:</p> <ul> <li>Dropping features that varied 5% of less across all rows (using <code>VarianceThreshold</code>)</li> <li>Dropping duplicated features (using <code>.transpose().drop_duplicates(keep='first').transpose()</code>)</li> <li>Dropping features with a very small correlation with the target variable; less than abs(\u00b10.1)</li> <li>Dropping features that were highly correlated with each other, greater than abs(\u00b10.8)</li> </ul> <p>It was concluded that 132 features out of the original 168 had very little relevance. </p> <p>The data that remained, which would be used for modelling, had 36 features and 21263 rows.</p> <p>The head of the cleaned dataframe was;</p> number_of_elements mean_atomic_mass range_atomic_mass wtd_range_atomic_mass mean_fie wtd_mean_fie wtd_entropy_fie range_fie wtd_range_fie mean_atomic_radius wtd_range_atomic_radius mean_Density range_Density mean_ElectronAffinity wtd_mean_ElectronAffinity range_ElectronAffinity wtd_range_ElectronAffinity mean_FusionHeat range_FusionHeat mean_ThermalConductivity wtd_mean_ThermalConductivity gmean_ThermalConductivity wtd_entropy_ThermalConductivity range_ThermalConductivity range_Valence wtd_range_Valence critical_temp O S Ca Cu Sr Y Ba Tl Bi 4.0 88.9444675 122.90607 31.7949208571429 775.425 1010.26857142857 0.791487788469155 810.6 735.9857142857139 160.25 42.914285714285704 4654.35725 8958.571 81.8375 111.72714285714301 127.05 80.9871428571429 6.9055 12.878 107.75664499999999 61.0151885714286 7.06248773046785 0.262848266362233 399.97342000000003 1.0 1.08571428571429 29.0 4.0 0.0 0.0 1.0 0.0 0.0 0.2 0.0 0.0 5.0 92.729214 122.90607 36.161939000000004 766.44 1010.61285714286 0.8070782149387309 810.6 743.164285714286 161.2 50.571428571428605 5821.4858 10488.571000000002 90.89 112.316428571429 127.05 81.2078571428572 7.7844 12.878 172.20531599999998 61.37233142857139 16.0642278788044 0.5677061078766371 429.97342000000003 2.0 1.12857142857143 26.0 4.0 0.0 0.0 0.9 0.0 0.0 0.1 0.0 0.0 4.0 88.9444675 122.90607 35.741099 775.425 1010.82 0.773620193146673 810.6 743.164285714286 160.25 49.31428571428571 4654.35725 8958.571 81.8375 112.213571428571 127.05 81.2078571428572 6.9055 12.878 107.75664499999999 60.94376 7.06248773046785 0.250477444193951 399.97342000000003 1.0 1.11428571428571 19.0 4.0 0.0 0.0 1.0 0.0 0.0 0.1 0.0 0.0 4.0 88.9444675 122.90607 33.7680099285714 775.425 1010.54428571429 0.7832066603612908 810.6 739.575 160.25 46.11428571428571 4654.35725 8958.571 81.8375 111.970357142857 127.05 81.0975 6.9055 12.878 107.75664499999999 60.97947428571429 7.06248773046785 0.257045108326848 399.97342000000003 1.0 1.1 22.0 4.0 0.0 0.0 1.0 0.0 0.0 0.15 0.0 0.0 4.0 88.9444675 122.90607 27.8487427142857 775.425 1009.7171428571401 0.8052296408133571 810.6 728.8071428571429 160.25 36.514285714285705 4654.35725 8958.571 81.8375 111.24071428571399 127.05 80.76642857142859 6.9055 12.878 107.75664499999999 61.0866171428571 7.06248773046785 0.272819938850094 399.97342000000003 1.0 1.05714285714286 23.0 4.0 0.0 0.0 1.0 0.0 0.0 0.3 0.0 0.0"},{"location":"research/2019-1-26-superconductivity-regression/#vselecting-the-right-model","title":"V.Selecting the right model","text":"<p>Seven different types of regression were performed on the data, with the intention of simply estimating the performance of each one without tuning hyperparameters. The metrics of evaluation for each one were the R2 score and the root mean squared error (RMSE). I also measured the time required for each model to fit and predict the data. After all the models and metrics were observed, the best performing one was then selected hyperparameter tuning.</p> <ul> <li>Ordinary Least Squares</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Elastic Net regression</li> <li>Bayesian Ridge</li> <li>K-nearest neighbors regression</li> <li>Random Forest Regression</li> </ul> <p>The process of evaluating the models was done by a for loop. At each loop, a sequentially increasing number of features from the data was used on all the models. The order of the features fed into the models was based upon the magnitude of correlation each feature had with the critical_temperature. Hence the first feature used was <code>range_ThermalConductivity</code> which had a correlation of 0.687654, and the last feature was <code>mean_fie</code> which had a correlation of 0.102268. The loop passed through all 36 features. The first iteration had the first feature, the second had the first two features, the third had the first three, and so on. Each model was initially fit without tuning hyperparameters.</p> <p>In code, the structure was;</p> <pre><code>#get the correlation of each feature with the target variable\ncorr = pd.DataFrame(data.corr()['critical_temp'])\n#get the absolute value of the correlation\ncorr['abs'] = np.abs(corr['critical_temp'])\n#sort the values by their absolute value, in descending order\ncorr = corr.sort_values(by='abs',ascending=False)\n#loop through the rows of the corr dataframe, getting each feature sequentially\nfor row in corr.index:\n#add more features iteratively &amp; cumulatively, in order of correlation magnitude\nfeatures = list(corr['index'].loc[:row])\n#use all those features up to that point\nX = df[features]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n#fit/train the model\nmodel.fit(X_train,y_train)\n#then do RMSE and R2 scoring\n</code></pre> <p>See the links to coded notebooks for actual code</p> <p>At each iteration the seven models were fit on a 70-30 train test split. The R2 and RMSE were then measured.</p> <p>Graphing the evaluation metrics of each model as a function of the number of features used, we have;</p> <p> </p> <p>The best performance metrics by each model were (without tuning hyperparameters);</p> model RMSE R2 score # of features used time RandomForestRegressor 10.3001 0.9104 30 1.6523 KNN 15.5705 0.7917 14 0.0596 BayesianRidge 20.5437 0.6363 27 0.0167 Ridge 20.5642 0.6356 27 0.0061 OLS 20.5645 0.6356 27 0.0123 ElasticNet 22.9051 0.5479 27 0.0153 Lasso 23.5355 0.5227 27 0.0145 <p>Thus, the best performing model (Random Forest regressor) was then fine tuned with it's hyperparameters.</p>"},{"location":"research/2019-1-26-superconductivity-regression/#virandom-forest-tuning-hyperparameters","title":"VI.Random Forest: Tuning hyperparameters","text":"<p>Because the main goal is to get the most accurate conclusions about superconductors, accuracy, in this context, is more important than efficiency. Hence, despite random forest regressor being the slowest model to fit the data (over 1 second), it was chosen to be the most important because it had the lowest loss function.</p> <p>The hyperparameter that was tuned was the number of decision trees in the random forest. To do this,I again used a trial and error approach where I cycled through a range of forest sizes. In conjunction, I also tried using different 'max feature' parameters, specifically <code>log2</code>, <code>sqrt</code> and the default.</p> <p> </p> <p>After lots of trial and error, a RMSE of \u00b19.396Kelvin was found, before tuning it was \u00b110.3Kelvin.</p>"},{"location":"research/2019-1-26-superconductivity-regression/#viimodel-validation","title":"VII.Model Validation","text":"<p>As in any regression analysis, R2 score and RMSE are not the only standards of evaluation. Examining the plot of the predicted values versus actual values is important, as is looking for normality and homoskedascity.</p> <p>The plot of the predicted temperatures versus actual temperature was</p> <p> </p> <p>Normality of errors &amp; Homoskedascity</p> <p>Using seaborn's <code>distplot</code> and <code>probplot</code> functions, I could plot the distribution of errors as well as their Q-Q plots (for both the train sets and the test sets). Approximately normality was observed.</p> <p> </p> <p>Feature Importance</p> <p>One of the more convenient things about the random forest package is it does feature importance analysis automatically. Here, we can see the relative importances of each measure.</p> <p> </p>"},{"location":"research/2019-1-26-superconductivity-regression/#viii-conclusions","title":"VIII. Conclusions","text":"<p>The other researchers that analyzed this data concluded that the important features extracted were ones based on thermal conductivity, atomic radius, valence, electron affinity, and atomic mass. In the graph above we can the same results, with a few extra important features such as the presence of Copper, Oxygen, Barium, the number of elements in the material, et. al. Perhaps it is these few extras I observed that allowed me to get a RMSE of 9.4Kelvin, which is lower than the other research's RMSE of 9.5Kelvin.</p>"},{"location":"research/2019-1-26-superconductivity-regression/#ix-references","title":"IX. References","text":"<p>Links to coded notebooks</p> <ul> <li>Pre-processing the target variable</li> <li>Filtering out irrelevant features</li> <li>Selecting the right model</li> <li>Tuning Random Forest</li> <li>Analyzing prediction errors</li> <li>Identifying important features</li> </ul> <p>Sources/Links</p> <ul> <li>A data-driven statistical model for predicting the critical temperature of a superconductor, by Kam Hamidieh<ul> <li>This was the other research I wanted to compare to.</li> </ul> </li> <li>UCI Machine learning repository on superconducting data<ul> <li>The data can be downloaded from here.</li> </ul> </li> <li>Super conducting material database<ul> <li>From the National Institute of Material Science; the organization that collected the data</li> </ul> </li> </ul>"},{"location":"research/2019-3-24-Fraud-Analytics/","title":"Using Plotly to visualize American Fraud Analytics","text":"<p>March 2019</p> <p>By Joe Ganser</p>"},{"location":"research/2019-3-24-Fraud-Analytics/#abstract","title":"Abstract","text":"<p>In this research, a model is made that estimates the number of crimes of various types committed in each state, and calculates the probability of a citizen comitting a given type of crime for each state. The focus is on fraud crimes. The model uses arrest statistics for various crimes in each town of each American state, and aggregates them to make conclusions about the larger population. The model is validated by examining how it estimates the frequency in which arrests are made for other crimes. This project was constructed in Python 3.</p>"},{"location":"research/2019-3-24-Fraud-Analytics/#i-introducing-the-data","title":"I. Introducing the data","text":"<p>The data used for this analysis was based upon crime stats reported to the FBI in 2015. Before cleaning it had 5874 rows, where each row represented a town. After cleaning, 3664 rows (towns) were left distributed across 37 US states. This covered a population of 60 million citizens, and 54 different types of crimes.</p> <p>The cleaned, model ready data had the following structure;</p> states City Population total_crimes Aggravated Assault Simple Assault Intimidation Murder and Nonnegligent Manslaughter Negligent Man- slaughter Justifiable Homicide Human Trafficking Offenses Commercial Sex Acts Involuntary Servitude Kidnapping/ Abduction Rape Sodomy Sexual Assault With an Object Fondling Sex Offenses, Non- forcible Incest Statutory Rape Arson Bribery Burglary/ Breaking &amp; Entering Counter- feiting/ Forgery Destruction/ Damage/ Vandalism of Property Embezzle- ment Extortion/ Blackmail False Pretenses/ Swindle/ Confidence Game Credit Card/ Automated Teller Machine Fraud Imper- sonation Welfare Fraud Wire Fraud Pocket- picking Purse- snatching Shop- lifting Theft From Building Theft From Coin Op- erated Machine or Device Theft From Motor Vehicle Theft of  Motor  Vehicle Parts or Acces- sories All Other Larceny Motor Vehicle Theft Robbery Stolen Property Offenses Drug/ Narcotic Violations Drug Equipment Violations Betting/ Wagering Operating/ Promoting/ Assisting Gambling Gambling Equipment Violations Sports Tampering Por- nography/ Obscene Material Pros- titution Assisting or Promoting Prostitution Purchasing Prostitution Weapon Law Violations ALABAMA Hoover 85163.0 4627.0 52.0 539.0 201.0 3.0 0.0 0.0 0.0 0.0 0.0 3.0 17.0 6.0 0.0 9.0 0.0 0.0 0.0 1.0 0.0 306.0 63.0 231.0 54.0 0.0 43.0 199.0 111.0 0.0 0.0 3.0 1.0 917.0 348.0 0.0 471.0 101.0 244.0 76.0 51.0 28.0 334.0 148.0 0.0 0.0 0.0 0.0 7.0 15.0 0.0 0.0 45.0 ARIZONA Apache Junction 38519.0 2964.0 82.0 345.0 64.0 0.0 0.0 0.0 0.0 0.0 0.0 3.0 2.0 0.0 1.0 45.0 2.0 0.0 2.0 7.0 0.0 352.0 29.0 528.0 0.0 0.0 65.0 32.0 62.0 0.0 0.0 0.0 0.0 336.0 10.0 0.0 99.0 22.0 381.0 102.0 11.0 10.0 155.0 166.0 0.0 0.0 0.0 0.0 4.0 0.0 10.0 0.0 37.0 ARIZONA Gilbert 247324.0 8676.0 121.0 846.0 202.0 2.0 1.0 1.0 0.0 0.0 0.0 48.0 27.0 1.0 0.0 50.0 20.0 0.0 20.0 20.0 0.0 519.0 82.0 1592.0 18.0 6.0 162.0 149.0 278.0 0.0 0.0 1.0 3.0 832.0 96.0 0.0 751.0 64.0 903.0 137.0 32.0 76.0 701.0 845.0 0.0 0.0 0.0 0.0 13.0 10.0 1.0 0.0 46.0 ARIZONA Yuma 93923.0 7985.0 330.0 668.0 167.0 5.0 1.0 0.0 0.0 0.0 0.0 33.0 35.0 10.0 1.0 59.0 11.0 0.0 11.0 32.0 1.0 680.0 37.0 1713.0 19.0 3.0 278.0 122.0 176.0 0.0 12.0 5.0 5.0 752.0 101.0 23.0 344.0 115.0 742.0 270.0 75.0 22.0 467.0 573.0 0.0 0.0 0.0 0.0 13.0 0.0 0.0 0.0 74.0 ARKANSAS Alma 5581.0 661.0 16.0 129.0 84.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 1.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 35.0 10.0 85.0 0.0 0.0 0.0 15.0 8.0 0.0 1.0 1.0 1.0 64.0 17.0 0.0 34.0 2.0 108.0 6.0 2.0 5.0 18.0 6.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 4.0 <p>Aside from <code>states</code>,<code>town</code>,<code>Population</code>,<code>total_crimes</code>, each column represents the number of arrests made for the given crime in that town in the year of 2015.</p>"},{"location":"research/2019-3-24-Fraud-Analytics/#ii-problem-statement-goals-of-the-analysis","title":"II. Problem Statement  &amp; Goals of the Analysis","text":"<p>The goal is to make a deicison as to which states have the lowest probability of someone comitting a fraudulent act. This allows for insurance companies to make marketing decisions on which states to work with so they minimize their risks. The problem is calculating the probability of a person comitting fraud from each state.</p> <p>The general approach behind the model is one that is mathematically based, integrating basic probability theorems. No machine learning is needed to solve this problem, and and it would be surprising if anyone used it.</p> <p>There are several goals;</p> <ol> <li>Create a model to produces the probability of a person in a given state comitting fraud</li> <li>Rank states by probability of a citizen comitting fraud (find the lowest 5)</li> <li>Visualize a heatmaps of crime distributions</li> <li>Estimate and validate the rate at which arrests are made for crimes.<ul> <li>And compare it to known rates (e.g. credit card fraud).</li> </ul> </li> </ol> <p>Using the listing of arrests for each crime in each town of a state, we are to estimate the number of frauds that happen in each state. Using that, we calculate the probability of a person comitting fraud by dividing by the population of the state.</p>"},{"location":"research/2019-3-24-Fraud-Analytics/#iii-model-construction-and-theory","title":"III. Model Construction and theory","text":"<p>The model used in this analysis is based upon probability theory and making estimations of how many crimes of a given type their are in each town of each state. Then, all the estimates for each town are summed to find the total for the given state. After that, this number is divided by the number of people in the state (all the towns studied) to find the probability of the given crime per person.</p> <p>Mathematical Construction</p> <p>The derivation begins with Baye's theorem;</p> <p> </p> <p>This says the the probability of fraud (or any crime) depends upon the Baye's relation of the probability of fraud given the distribution of other crimes.</p> <p>Because a fraud by definition is a crime, the probability of an event being a crime being a fraud is one, i.e.</p> <p> </p> <p>So this leads to;</p> <p> </p> <p>And if we give the definitions;</p> <p> </p> <p> </p> <p>Therefor;</p> <p> </p> <p>Where N_population is the number of people in the population studied. By what is the actual number of frauds? Is is just the number of arrests? We assume the actual number of frauds in existence exceeds the number of arrests for fraud;</p> <p> </p> <p>We focus on calculating the actual number of frauds in each town. Using the records of arrests for other crimes in the town, we sum them all up with a weighting system. i.e.</p> <p> </p> <p>Now the critical portion of the model comes into play - calculating the weights. The weights are calculated using the correlation coefficients of each other crime with the fraud crime we're studying</p> <p> </p> <p>So we have a threshold for the correlation. This means that we set the weight equal to the correlation between a given crime and fraud if its greater than a threshold value, and set it equal to zero if its not.</p> <p>Now that an estimation is made for the number of frauds in a given town, we sum over all the towns in the state. To find the probability of fraud per person, we simply divide by the population of the state.</p> <p> </p> <p>Where N_est,t is the estimated number of frauds in town t. This model can also be used estimate over all frequency at which arrests are made for crimes. This is done as follows.</p> <p>First we sum over all the estimated number of frauds for all the towns in the USA;</p> <p> </p> <p>Then we sum over all the arrests for frauds, nationwide.</p> <p> </p> <p>And the probability is then;</p> <p> </p> <p>Which is the probability of making arrest for a given fraud (or other crime). </p> <p>We'd expect the probability of an arrest made for a credit card fraud to be very low. We shall see the model produces these results.</p> <p>Model Code in Python</p> <p>The model code is built into a function. It's input is the cleaned data, the crimes we want to study and a threshhold correlation coefficient. The ouput is a dataframe and the nationwide rate (probability) at which arrests are made for the given crime.</p> <pre><code>def model(data,fraud_crimes,threshold):\ndf = data.copy()\nimport warnings\nimport numpy as np\nwarnings.filterwarnings('ignore')\nstates = df['states'].unique() \n#group all the fraud crimes together\nif len(fraud_crimes)&gt;1:\ndf['fraud_count'] = df[fraud_crimes].sum(axis=1)\ndf.drop(fraud_crimes,axis=1,inplace=True)\nelse:\ndf['fraud_count'] = df[fraud_crimes[0]]\ndf.drop(fraud_crimes,axis=1,inplace=True)\ncombined_frame = pd.DataFrame()\n#loop over each state\nfor state in states:\ndummy_frame = df[df['states']==state]\n#if the number of towns in the state is greater than the total number of crimes in the dataset (56),\n#use the correlation coefficients of each crime for only that state\nif len(dummy_frame)&gt;=56:\ncorrelations = pd.DataFrame(dummy_frame.corr()['fraud_count']).reset_index()\ncorrelations['fraud_count'] = correlations['fraud_count'].apply(lambda x: x if x&gt;=threshold else 0)\ncorrelations['crime'] = correlations['index'].apply(lambda x: 1 if x in crimes+['fraud'] else 0)\ncorrelations = correlations[correlations['crime']==1].drop('crime',axis=1)\nfor i in correlations.index:\nfactor = correlations.loc[i]['fraud_count']\nfeature = correlations.loc[i]['index']\nif feature not in ['total_crimes','fraud_count']:\ndummy_frame[feature] = dummy_frame[feature].apply(lambda x: factor*x)\nelif len(dummy_frame)&lt;56:\n#if the state has less than 56, use the correlation coefficients for the entire country\n#to calculate the weights.\ncorrelations = pd.DataFrame(df.corr()['fraud_count']).reset_index()\ncorrelations['fraud_count'] = correlations['fraud_count'].apply(lambda x: x if x&gt;=threshold else 0)\ncorrelations['crime'] = correlations['index'].apply(lambda x: 1 if x in crimes+['fraud'] else 0)\ncorrelations = correlations[correlations['crime']==1].drop('crime',axis=1)\nfor i in correlations.index:\nfactor = correlations.loc[i]['fraud_count']\nfeature = correlations.loc[i]['index']\nif feature not in ['total_crimes','fraud_count']:\ndummy_frame[feature] = dummy_frame[feature].apply(lambda x: factor*x)\ncombined_frame = pd.concat([combined_frame,dummy_frame],axis=0)\nmodel_groupby = combined_frame.groupby('states').sum().reset_index()\nmodel_groupby_crimes = [i for i in model_groupby.columns if i not in ['states','Population','total_crimes']]\nmodel_groupby['est.Frauds'] = model_groupby[model_groupby_crimes].sum(axis=1)\nmodel_groupby['%est.Caught'] = model_groupby['fraud_count'].div(model_groupby['est.Frauds'],axis=0).multiply(100)\nmodel_groupby = model_groupby[['states','Population','total_crimes','fraud_count','est.Frauds','%est.Caught']]\nmodel_groupby['fraud_prob'] = model_groupby['est.Frauds'].div(model_groupby['Population1'],axis=0)\nmodel_groupby.sort_values(by='fraud_prob',ascending=True,inplace=True)\nmodel_groupby = model_groupby[['states','fraud_prob','Population','total_crimes','fraud_count','est.Frauds','%est.Caught']]\nmodel_groupby = model_groupby.reset_index().drop('index',axis=1)\nnation_wide_arrests = model_groupby['fraud_count'].sum()\nnation_wide_estimates = model_groupby['est.Frauds'].sum()\npercent_caught = round(100*nation_wide_arrests/nation_wide_estimates,2)\nprint('crimes analzed: ',fraud_crimes)\nprint('Nation wide % of crimes where arrests are made',percent_caught,'%')\nprint('Nation wide arrests :',round(nation_wide_arrests))\nprint('Nation wide estimate number of crimes: ',round(nation_wide_estimates))\nreturn model_groupby\n</code></pre>"},{"location":"research/2019-3-24-Fraud-Analytics/#iv-key-results","title":"IV. Key Results","text":"<p>The main objective was to find the states with the lowest rate of fraud instances. To solve this problem, all fraud related crimes in the data were aggregated together to be represented as one crime. These crimes included the following labels in the data;</p> <ul> <li><code>FalsePretenses/Swindle/ConfidenceGame</code></li> <li><code>CreditCard/AutomatedTellerMachineFraud</code></li> <li><code>Imper-sonation</code></li> <li><code>WelfareFraud</code></li> <li><code>WireFraud</code></li> <li><code>Counter-feiting/Forgery</code></li> </ul> <p>Throughout this analysis, any time a 'fraud crime' is referred to, it referrs to a person comitting any of these crimes.</p> <p>Nationwide, it was found that about 7.37% of these crimes (aggregated) led to an arrest. There were a total of 302,783 fraud related arrests in the USA in 2015, recorded from a population of 60.2million residents.</p> <p>IV.A Validating the model</p> <p>Any useful model should produce results that are realistic. How could can this model be evaluated to determine if it's a good fit? In this case, we use the predicted rate at which arrests are made for varying crimes to see if it's realistic.</p> <p>It's widely known that credit card fraud goes un-caught quite often. This model predicts that only about 2.7% of domestic credit card fraud crimes lead to an arrest. Other sources predict less than 1%, but regardless this is probability a realistic estimate. (Reference 2)</p> <p>IV.B Identifying states with the lowest rate of fraud</p> <p>In the following table, we see the ranking of the states with the lowest probability of a person comitting a fraudulent act (one of the crimes in the listing above). We also see the estimated percentage for which arrests are made for these crimes.</p> rank states fraud probability %est.Caught 1 CONNECTICUT 0.0186 21.03 2 VERMONT 0.0286 6.46 3 MASSACHUSETTS 0.0297 11.43 4 PENNSYLVANIA 0.0297 15.77 5 RHODE ISLAND 0.0378 9.82 <p>IV.C Nationwide heatmap representations</p> <p>In the following plots, we see nationwide heatmaps of some interesting statistics. In the first, we have a distribution of the probability of a person comitting fraud from each state. If a state is white, it was not present in the data set. These are plotly plots, so click each graph for better interactivity!</p> <p>In this next plot, we see an estimation on the percentage at which fraud crimes lead to an arrest.</p> <p>And in this last heat plot, we have the probability of a person in each state comitting any crime.</p> <p>IV.D Overall Distribution of crimes and frauds</p> <p>It's also good to see the overall distribution of crimes that were recorded, as well as the break down of frauds. These can be represented by bar charts and pie charts. Keep in mind these numbers were extracted from a population of 60.1Million people.</p> <p>In this next chart, we have the counts of each type of crime recorded in the data set. Out of the 60.1 million people included in the data, there were 4.1 million arrests.</p> <p></p> <p>And finally, we have a pie chart break down of the different types of frauds present. There were a total of 302,783arrests for fraud related crimes (about 7.4% of all crimes).</p> <p> </p>"},{"location":"research/2019-3-24-Fraud-Analytics/#v-conclusions","title":"V. Conclusions","text":"<p>Using the model and the data we can draw a few conclusions;</p> <ul> <li> <p>The states with the lowest chance of someone comitting a fraud crime are (1-3%)</p> <ol> <li>CONNECTICUT</li> <li>VERMONT</li> <li>MASSACHUSETTS</li> <li>PENNSYLVANIA</li> <li>RHODE ISLAND</li> </ol> </li> <li> <p>It was noticed that these states also had higher rates of arrests for fraud crimes (6-21% of cases lead to an arrest)</p> </li> <li>Crime rates were also low in these states to (3-5%)</li> <li>The probability of a person comitting a fraud crime varies from (1% to 17%) across the USA</li> <li>The top four types of frauds that people get arrested for are<ul> <li>Swindling (29%)</li> <li>Impersonation (27%)</li> <li>Credit card fraud (23.1%)</li> <li>Counterfitting (19%)</li> </ul> </li> <li>Fraud crimes consist of about 7% of all crimes.</li> <li>About 2.7% of credit card frauds lead to an arrest.</li> </ul>"},{"location":"research/2019-3-24-Fraud-Analytics/#vi-links-to-coded-notebooks","title":"VI. Links to Coded Notebooks","text":"<ul> <li>Cleaning the data</li> <li>Modelling the data</li> <li>Nationwide heatmaps</li> <li>Bar charts</li> <li>Pie chart</li> <li>API usage</li> </ul>"},{"location":"research/2019-3-24-Fraud-Analytics/#vii-sources","title":"VII. Sources","text":"<ul> <li>(1) FBI api&amp;data source</li> <li>(2) Stats on credit card fraud</li> <li>(3) Plotly</li> </ul>"},{"location":"research/2019-4-1-Predicting-Breathing-rate-with-smart-watch-data/","title":"Predicting Breathing Rates using Smart Watch Data","text":"<p>by Joe Ganser</p>"},{"location":"research/2019-4-1-Predicting-Breathing-rate-with-smart-watch-data/#abstract","title":"Abstract","text":"<p>This research was the product of a data driven healthcare hackathon I participated sponsored by Accenture and the School of AI. I was the team leader, and our team came in first place for the NY division. Here we use blood oxygen and electrocardiogram data to predict the rate at which people breath. It turns out, this data can be extracted from a smart watch. Using this data, we could predict a persons respiratory rate with 90% accuracy.</p> <p></p> <p>Our official presentation title</p> <p></p>"},{"location":"research/2019-4-1-Predicting-Breathing-rate-with-smart-watch-data/#i-introducing-the-problem","title":"I. Introducing the problem","text":"<p>Background information &amp; Context</p> <p>Electrocardiogram (ECG) data and photoplethysmogram (PPG) data are extremely useful in healthcare. They are used to assist in diagnostic methods for a wide array of diseases. ECG measures cardiac properties and PPG data measures blood oxygen levels using optical instruments. (1,2)</p> <p>Both ECG and PPG data can be extracted from a smart watch at the same level of accuracy and precisions of machines found in hospitals. ECG and PPG can be combined to predict breathing rate, and using the combination of all this data (3,4,5).</p> <p>The data used for this analysis was not actually collected from a smart watch, but smart watches have the capability to collect the same data. The data used in this analysis was from 53 patients in intensive care, where their ECG, PPG and breathing rates were measured. (4,5,6)</p> <p></p> <p>Goals of the analysis</p> <p>The goal was to use supervised machine learning techniques to predict a persons breathing rate using real time, continuous PPG and ECG data.</p> <p>In conjunction, it was also our goal to investigate the feasibility of using this type of data for enhancing diagnostic processes in healthcare. We ended by speculating on the market evolution of technology that integrates these methods.</p>"},{"location":"research/2019-4-1-Predicting-Breathing-rate-with-smart-watch-data/#ii-data-structure-feature-engineering","title":"II. Data Structure &amp; Feature Engineering","text":"<p>How the data was originally harvested</p> <p>The data used for this analysis was time series recorded from 53 ICU patients, in age ranges between 19-84. Both male and female patients were present. They were recorded using hospital based ECG and PPG devices, and a breathing apparatus. Continuous measurements for each patient were made across apporximately 8 minutes (6).</p> <p>The original data's structure: ECG, PPG &amp; pulminory</p> <p>The data was aggregated from two fundamental sources - one which was collected at 1Hz and the other at 125Hz. These were then joined in a left outer manor. Some of the key features were;</p> <ul> <li>Respiratory rate (the supervised learning target)</li> <li>Pulse</li> <li>Blood oxygen level</li> <li>Pleth (pulmonary data)</li> <li>V (voltage)</li> <li>AVR</li> <li>II</li> </ul> <p>The 1Hz data looked like this;</p> Time (s) HR PULSE SpO2 0 93 92 96 1 92 92 96 2 92 92 96 3 92 93 96 4 92 93 96 <p>The 125Hz data looked like this;</p> Time (s) RESP PLETH II V AVR 0.0 0.25806 0.59531 -0.058594 0.721569 0.859379 0.008 0.26393 0.59042 -0.029297 0.69608 0.69531 0.016 0.269790 0.58358 0.179690 0.7 0.45508 0.024 0.27566 0.57771 0.84375 0.32941 0.041016 0.032 0.2825 0.57283 1.3184 0.078431 -0.099609 <p>After combining with left outer join, we got;</p> Time (s) RESP PLETH V AVR II HR PULSE SpO2 0.0 0.25806 0.59531 0.721569 0.859379 -0.0585944 93 92 96 0.008 0.26393 0.59042 0.69608 0.69531 -0.029297 93 92 96 0.016 0.269790 0.58358 0.7 0.45508 0.17969 93 92 96 0.024 0.27566 0.57771 0.32941 0.041016 0.84375 93 92 96 0.032 0.2825 0.57283 0.078431 -0.099609 1.3184 93 92 96 <p>For each person in the study, this amounted to about 60,000 rows. When all 53 people were combined, we were left with approximately 2.7 million rows (about 1.2Gb of data.)</p> <p>Visualizing the fundamental data</p> <p>The data was fundamentally time series based. Here are a few snapshots of some of the key features;</p> <p></p> <p></p> <p>Feature engineering</p> <p>Considering the aggregation of the data from 125Hz, the values of the metrics varied quite a bit in each second. Thus, summary statistics of the 125 values collected each second could be engineered into features. Specifically, these features were;</p> <ul> <li>Max value</li> <li>Min value</li> <li>Mean value</li> <li>Kurtosis value</li> <li>Skew value</li> </ul> <p>(Over the distribution of the 125 measurements made each second). To create these features, a function was created.</p> <pre><code>from scipy.stats import kurtosis,skew\ndef make_features(frame):\nframe.fillna(numerics.mean(),inplace=True) \nHz_125_cols = [' RESP', ' PLETH', ' V', ' AVR', ' II']\nMin = frame[Hz_125_cols+['sec']].groupby('sec').min()\nMin.columns = [i+'_Min' for i in Min.columns]\nMax = frame[Hz_125_cols+['sec']].groupby('sec').max()\nMax.columns = [i+'_Max' for i in Max.columns]\nMean = frame[Hz_125_cols+['sec']].groupby('sec').mean()\nMean.columns = Mean.columns = [i+'_Mean' for i in Mean.columns]\nKurt = frame[Hz_125_cols+['sec']].groupby('sec').agg(lambda x: kurtosis(x))\nKurt.columns = [i+'_Kurt' for i in Kurt.columns]\nSkw = frame[Hz_125_cols+['sec']].groupby('sec').agg(lambda x: skew(x))\nSkw.columns = [i+'_Skw' for i in Skw.columns]\nsummary_frames = [Min,Max,Mean,Kurt,Skw]\none_sec_summary = pd.concat(summary_frames,axis=1).reset_index()\nframe = frame.merge(one_sec_summary,on='sec',how='outer')\nreturn frame\n</code></pre> Time (s) RESP PLETH V AVR II HR PULSE SpO2 RESP_Min PLETH_Min V_Min AVR_Min II_Min RESP_Max PLETH_Max V_Max AVR_Max II_Max RESP_Mean PLETH_Mean V_Mean AVR_Mean II_Mean RESP_Kurt PLETH_Kurt V_Kurt AVR_Kurt II_Kurt RESP_Skw PLETH_Skw V_Skw AVR_Skw II_Skw 0.0 0.25806 0.59531 0.72157 0.85938 -0.05859 93 92 96 0.25806 0.37732 0.07451 -0.09961 -0.21484 1.0 0.59531 0.87059 1.0254 1.3438 0.70822 0.46446 0.75551 0.81947 -0.02431 -1.40052 -1.08775 10.3465 13.15985 13.94906 -0.24258 0.48232 -3.05758 -3.3357 3.5823 0.008 0.26393 0.59042 0.69608 0.69531 -0.0293 93 92 96 0.25806 0.37732 0.07451 -0.09961 -0.21484 1.0 0.59531 0.87059 1.0254 1.3438 0.70822 0.46446 0.75551 0.81947 -0.02431 -1.40052 -1.08775 10.3465 13.15985 13.94906 -0.24258 0.48232 -3.05758 -3.3357 3.5823 0.016 0.26979 0.58358 0.7 0.45508 0.17969 93 92 96 0.25806 0.37732 0.07451 -0.09961 -0.21484 1.0 0.59531 0.87059 1.0254 1.3438 0.70822 0.46446 0.75551 0.81947 -0.02431 -1.40052 -1.08775 10.3465 13.15985 13.94906 -0.24258 0.48232 -3.05758 -3.3357 3.5823 0.024 0.27566 0.57771 0.32941 0.04102 0.84375 93 92 96 0.25806 0.37732 0.07451 -0.09961 -0.21484 1.0 0.59531 0.87059 1.0254 1.3438 0.70822 0.46446 0.75551 0.81947 -0.02431 -1.40052 -1.08775 10.3465 13.15985 13.94906 -0.24258 0.48232 -3.05758 -3.3357 3.5823 0.032 0.2825 0.57283 0.07843 -0.09961 1.3184 93 92 96 0.25806 0.37732 0.07451 -0.09961 -0.21484 1.0 0.59531 0.87059 1.0254 1.3438 0.70822 0.46446 0.75551 0.81947 -0.02431 -1.40052 -1.08775 10.3465 13.15985 13.94906 -0.24258 0.48232 -3.05758 -3.3357 3.5823"},{"location":"research/2019-4-1-Predicting-Breathing-rate-with-smart-watch-data/#iii-putting-the-big-data-on-amazon-web-service","title":"III. Putting the big data on Amazon Web Service","text":"<p>During the hackathon, Accenture provided us with a $125 gift certificate to create and Amazon Web Service EC2 instance.</p> <p>This enabled us to use a p3.2x large instance, putting 1.2Gb into the system. Despite our enhanced processing capability, it was still challenging and time consuming to run all the models. It took approximately 6-10 minutes to run the full models on AWS.</p>"},{"location":"research/2019-4-1-Predicting-Breathing-rate-with-smart-watch-data/#iv-modelling","title":"IV. Modelling","text":"<p>Multiple attempts using regression techniques were made to model the data. Using resp as our target, our goal was to optimize performance on the metrics of; * R2 score * Mean squared error * Model evaluation time (seconds)</p> <p>Assumptions that had to be made</p> <p>To make a regression analysis on time series data, we had to assume the time series is itself stationary. This means that the value of the feature we're analyzing has an average and variance that is constant in time.</p> <p>Stated mathematically, the assumptions were;</p> <p></p> <p>Are these assumptions valid or realistic? Yes definitiely so. Why? Because the people who were being studied in the original analysis for which the data is being collected were laying down in bed throughout the study. Thus, the there was no stimulus to change in the time series, and it can be assumed to have a constant trend.</p> <p>Model Development</p> <p>A function was created to put the data through a pipeline for which it was fitted and scored on several different types of models. The models that were compared were</p> <ul> <li>Ordinary Least Squares</li> <li>Lasso Regression</li> <li>Elastic Net Regression</li> <li>Ridge regression</li> <li>Bayesian Ridge</li> <li>K-neighbors regression</li> <li>Random Forest Regression</li> </ul> <p>The function that produced this system was this;</p> <pre><code>import time\nimport numpy as np\nfrom sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\nfrom sklearn.linear_model import Lasso,Ridge,ElasticNet, BayesianRidge, LinearRegression\nfrom sklearn import neighbors\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nmodels = {'OLS':LinearRegression(),'ElasticNet':ElasticNet(),\n'BayesianRidge':BayesianRidge(),'Lasso':Lasso(),\n'Ridge':Ridge(),'KNN':neighbors.KNeighborsRegressor(),\n'rff':RandomForestRegressor()}\ndef model_performance(X,y):\ntimes =[]\nkeys = []\nmean_squared_errors = []\nmean_abs_error = []\nR2_scores = []\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\nfor k,v in models.items():\nmodel = v\nt0=time.time()\nmodel.fit(X_train, y_train)\ntrain_time = time.time()-t0\nt1 = time.time()\npred = model.predict(X_test)\npredict_time = time.time()-t1\npred = pd.Series(pred)\nTime_total = train_time+predict_time\ntimes.append(Time_total)\nR2_scores.append(r2_score(y_test,pred))\nmean_squared_errors.append(mean_squared_error(y_test,pred))\nmean_abs_error.append(mean_absolute_error(y_test,pred))\nkeys.append(k)\ntable = pd.DataFrame({'model':keys, 'RMSE':mean_squared_errors,'MAE':mean_abs_error,'R2 score':R2_scores,'time':times})\ntable['RMSE'] = table['RMSE'].apply(lambda x: np.sqrt(x))\nreturn table\nmodel_performance(X,y)\n</code></pre> <p>Best Results</p> <p>After running this function, we got a table of performance metrics for each model. Note this is when we run on one only one person's data. If we ran on everyone in the study, the metrics were approximately the same.</p> model R2 score time(s) RMSE Random Forest 0.90017 2.85901 0.11014 KNN 0.82439 4.02399 0.12569 BayesianRidge 0.53836 0.076529 0.203805 Ridge 0.53835 0.02263 0.20380 OLS 0.53835 0.36989 0.20380 ElasticNet -1.24037e-05 0.02514 0.29996 Lasso -1.24037e-05 0.02388 0.29996 <p>Clearly, it was the random forest regressor that achieved the best results.</p> <p>Validating a well fit model</p> <p>Aside from simply metrics of performance, it's also good to look at how well the model has been fit. Here we see distribution of errors on the train set and the test set;</p> <p></p> <p>The frequency count may be slightly different in scale, but this is ok because its size difference is proprotional to the size differences in the train set and the test set.</p> <p>Feature importance</p> <p>One of the beauties of the random forest package, is it allows us to create an output describing the magnitude of feature importances.</p> <p></p> <p>It was also noticed that if we eliminated the plethysmogram data, we could still predict breathing rate with upto about 80% accuracy.</p>"},{"location":"research/2019-4-1-Predicting-Breathing-rate-with-smart-watch-data/#v-conclusions","title":"V. Conclusions","text":"<p>We can draw a few conclusions. * Using a persons plethysmogram and electrocardiogram data we can predict their respiratory rate with 90% accuracy. * If we train on multiple people's data, we predict anyone's respiratory rate with very good data.</p> <p>Insights on usage</p> <p>Being able to predict user's breathing rate with home based wearable technology opens up a lot of opportunities for healthcare. This can allow us to do things such as;</p> <ul> <li>Have doctors monitor our health at home</li> <li>Enhance and assist with continuous health monitoring</li> <li>Prevent major health crises before they occur.</li> </ul> <p>Prospects on further usage</p> <p>Perhaps these algorithms and data collection techniques can be put into smart watch/phone apps. Software could be created that allows for automation of doctor patient interaction, notifying healthcare professionals in real time when a serious issue arises.</p> <p>Smart watches might save lives one day!</p> <p></p>"},{"location":"research/2019-4-1-Predicting-Breathing-rate-with-smart-watch-data/#vi-links-to-coded-notebooks","title":"VI. Links to coded notebooks","text":"<ul> <li>Downloading and aggregating the data</li> <li>Modelling the data</li> <li>Random forest performance</li> <li>Main github repository</li> </ul>"},{"location":"research/2019-4-1-Predicting-Breathing-rate-with-smart-watch-data/#vii-references","title":"VII. References","text":"<ol> <li>Electrocardiogram (ECG) https://en.wikipedia.org/wiki/Electrocardiography</li> <li>Photoplethysmogram (PPG) https://en.wikipedia.org/wiki/Photoplethysmogram#Photoplethysmograph</li> <li>Probabilistic Estimation of Respiratory Rate from Wearable Sensors, Pimentel, Charlton, Clifton, Institute of Biomedical Engineering, Oxford University http://www.robots.ox.ac.uk/~davidc/pubs/springer2015.pdf</li> <li>PPG data can be extracted using smart watches: https://www.ncbi.nlm.nih.gov/pubmed/26737690</li> <li>ECG data cen be extracted using smart watches: https://www.theatlantic.com/technology/archive/2019/02/the-apple-watch-ekgs-hidden-purpose/573385/</li> <li>Clinical data on breathing rates, ppg, and ecg data from ICU patients https://physionet.org/physiobank/database/</li> </ol>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/","title":"Big Data for Big Dating: Winning 1st place in a Data Science Competition","text":"<p>By Joe Ganser</p>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/#abstract","title":"Abstract","text":"<p>In this data driven study, analysis is made on the viral impact Tinder Gold had on the dating app market. Data records on purchase receipts were used for this analysis. Investors asked the question: why was Tinder Gold such a viral product? This study was motivated by an open source competition hosted by the company Battlefin and sponsored by the investment bank Jefferies. Up against 100 other competitors, I was this first place winner in Spring 2019. This presentation is oriented for business developers and does not contain code.</p> <p>Me presenting my research at New York University. Click the picture for a google slides presentation.</p> <p> </p> <p>MScience &amp; Battlefin presenting the awards.</p> <p> </p>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/#i-the-dating-market-at-a-glance","title":"I. The Dating Market at a Glance","text":"<p>Internet dating started in the early 1990s, and in recent years the phenomenon has gone from an awkward subculture to a level of normality seen in popular media. Since the invention of smart phones and tablets, we've seen a renassiance of dating products emerge that take the social paradigms to another level.</p> <p>Tinder, which started in 2012 (1), has created a level of market dominance that is unprecendented. It's highly addictive smart phone swiping nature has crushed it's competitors and it has gained a major advantage by being one of the first in the market place to boast such features.</p> <p>Despite there being a functional free version, which allows the possibility of acquiring a partner, people are still willing to pay for all sort of features. It will be clear through this analysis that the more features added to their swipe experience, the more customers are willing to pay.</p> <p>This analysis intends to examine and measure the impact Tinder's new product, Tinder Gold, had on both the Tinder brand and the dating market.</p> <p>I.A Tinder Plus versus Tinder Gold</p> <p>Tinder Plus was the original paid version of the app, which allowed users unlimited daily swipes, several \"top choice\" picks per day. Then came Tinder Gold. Tinder Gold allows for the following features (2):</p> <ul> <li>Unlimited swipes</li> <li>1 free boost a month</li> <li>Control age and distance</li> <li>Control who sees you</li> <li>Swipe around the world</li> <li>5 super free likes per day</li> <li>Unlimited rewinds (go back and swipe again, change your mind)</li> <li>No advertisements</li> <li>Seeing who arleady likes you - the only difference between Tinder Plus and Gold</li> </ul> <p>As we shall see, that one subtle difference made a huge change in multiple metrics of market performance.</p> <p>It should also be noted while a Tinder U product does exist, it was not specifically labeled as Tinder U in the Edison Data set. It must have been mixed in by being labeled as one of the other Tinder products.</p>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/#ii-introducing-the-data","title":"II. Introducing the data","text":"<p>The original raw data source, coming from the data company Edison, consisted of email sales receipts sent to iphone and google play users. The raw data consisted of approximately 1.52 million rows, and after isolating, organizing and cleaning the receipts related to dating apps 324,667 rows were left. These receipts were collected between January 1, 2015 and September 30th, 2018 and consisted of 73,953 unique users. A total revenue of $6.12 Million was measured across the entire data set.</p> <p>II.A Demographics on the products throughout the cleaned data set</p> <p>The vast majority of the products in the analyzed data set were Tinder related products. There were several other products, and the breakdown across the entire data set can be seen in following table;</p> <p>Table 1</p> product Number of unique users Number of purchases $revenue Tinder 58803 252672 4.39M Okcupid 15721 55332 1.01M Match 6282 6321 417K twoo 399 1308 10.9K christian_mingle 1234 2137 104K elitesingles 384 450 48.8K jdate 364 789 47.8K jswipe 1020 5468 92.9K howaboutwe 109 190 9.32K <p>Later we shall see the contrast of metrics before and after August 1, 2017, the recognized launch date of Tinder Gold. But to give a simple overview of the contrast, these pie charts describe the percentage of sales (by the number of items sold, not revenue generated) attributed to each product for the two time periods.</p> <p>II.B Pie charts of Before and after Tinder Gold launch</p> <p>This next figure shows the volume each product took up before and after Tinder Gold's launch. For example, we can say that out of all the instances in the data where a product was purchased before 8/1/2017, 68.76% of it was attributed to Tinder Plus. Look at the changes in percentages between the two pies.</p> <p>Figure 1</p> <p></p> <p>Note Products such as elitesingles,twoo, howaboutwe, and jdate were omitted because their percentages were very small.</p> <p>We arrive at out first conclusions: Tinder - a dating app based on smart phone swiping - is dominating the dating market. It's difference in market share must be caused by providing a uniquely different user experience from the other dating products.</p>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/#iii-problem-solving-strategy","title":"III. Problem solving strategy","text":"<p>The general strategy on how to measure Tinder Gold's impact is approached with a fundamental principles. The idea is that, we can find and compare the performance of each dating product and their competitors based upon a few pieces of information:</p> <ul> <li>A unique identity of each customer (<code>user_id</code>)</li> <li>The time stamps of when they made any product purchase (<code>order_time</code>)</li> <li>Tracking the price they paid (<code>item_price</code>)</li> <li>Identifying the name of the product they purchased at each instance (<code>product_description</code>)</li> </ul> <p>Then putting these data points into a time series, we can track the evolution of products. Using these pieces of information we can engineer a few interesting features which become metrics of evaluation as seen in the goals.</p> <p>The metrics and conclusions will be scalable - i.e. any metric analyzed should remain the same if we increase the amout of data.</p> <p>III.A Goals for this analysis</p> <p>Tinder Gold's impact will be measured using the following;</p> <ul> <li>Determine and measure the changes in several metrics that Tinder Gold's affected. These metrics were;</li> <li>Revenue per number of daily users across time</li> <li>New user acquisitions per day</li> <li>New user acquisitions per number of daily users across time (ratio)</li> <li>Product repurchases per day</li> <li>Product repurchases per number of daily users across time (ratio)</li> <li>Churn instances per day</li> <li>Churn instances per number of daily users across time (ratio)</li> <li>Market volume shared by each product across time</li> <li>Identify relationships of causality between changes in metrics.</li> <li>Comment on the data by integrating user experience research.</li> </ul> <p>The Tinder products will be evaluated on these metrics, as will Okcupid since it's the only significantly measured competitor in the data.</p> <p>There will also be tasks that provide; * Identifying any situations analogous to Tinder Gold's that may be affecting the market. * Identifying any sampling errors that are hidden within the Edison data set itself. * Identifying any anomalies in the data.</p>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/#iv-research-results-key-metrics","title":"IV. Research Results &amp; Key Metrics","text":"<p>In this section I present the key results. There are three fundamental results demonstrating the impact Tinder Gold had on the business, and their insights reinforce each other.</p> <p>IV.A Causality in changes in market volume of Tinder Plus vs Tinder Gold</p> <p>In this next plot, we can see the evolution of market volume of Tinder Plus versus Tinder Gold. Market volume, is defined as the percentage of sales (by number of items sold, not revenue) in the data attributed to any product on a given day. For example, on 8/1/2017 approximately 80% of all the recorded items sold on that date were Tinder Plus products (this includes the monthly, 3 month, 6 month and 12 month versions of each) (3).</p> <p>Figure 2: The darker lines are the 30day average, the faint ones are the daily values. </p> <p>Below, in Figure 3, we have a scatter of the values in the plot above put against each other, AFTER the Tinder Gold launch date. In the upper left hand corner are the values of the market share of Tinder Plus (y) and Tinder Gold (x) on the Tinder Gold launch date, Aug 1 2017. On the bottom right is the very end of the time series, September 30th, 2018.</p> <p>Figure 3: X is the percentage of market volume of Tinder Gold, Y is percentage of market volume of Tinder Plus</p> <p></p> <p>Whats most interesting about this plot is that it that both time series are not in any way altered to become stationary (i.e. to seperate the trend from the noise). These are the actual daily volume values, not averaged ones. Using a statistical test called the Augmented Dickey Fuller test, we see a cointegration of the two time series. Using the Granger test,we can prove a causality relationship between the growth of Tinder Gold and the atrophy of Tinder Plus. By causality, we mean predictive causality in that we can predict the values of Tinder Gold from the values of Tinder Plus. (6)(7)(8)(13)(14)(15)</p> <p>We arrive at our second conclusions:</p> <ul> <li>The rise in sales of Tinder Gold shares a predictive causality relation (p=0.01) with a 74% drop in market volume of Tinder Plus between Aug 1 2017 and Sept 30th 2018.</li> <li>This is caused by giving people the ability to know who likes them back on the service (Tinder Gold's advantage).</li> <li>Market changes in Tinder Plus vs Tinder Gold share a causality relationship upto 17 days prior (p&lt;0.01)</li> <li>No significant change was seen in Tinder's top competitor, Okcupid, across the launch date.</li> </ul> <p>IV.B Evolution of new customer acquisition, repeat purchases and churn instances</p> <p>Using feature engineering, I was able to identify instances in the time series that indicated when a customer made their first purchase of a product, a repeat purchase or a churn purchase (i.e. they didn't buy the same product again for atleast 91 days, one business quarter).</p> <p>With these data points, I could calculate the number of such instances on a given day and compare it to the number of unique users who made purchases on that same day. i.e. for a given product</p> <p></p> <p>Note that a churn instance may simultaneously be a repeat purchase or a new purchase - but not all three!</p> <p>Justification for these metrics</p> <p>These metrics indicate the proprotions of how many customers on a given day made what kind of decision. Whats really interesting is they these metrics can provide evidence of stability in a subscription product. During product stability, the evolution of these ratios have a balance with each other. If a change in these values is noticed, as seen in the churn line of figure 4, it indicates a stimulus.</p> <p>These metrics are also scalable, and should be approximately the same if we increase the number of users in our data.</p> <p>In figure 4 we see the plot of these ratios. So on the Tinder Gold launch date, the ratio of churn (blue) purchases to number of unique users was about 0.3 - meaning if we had 100 unique customers who made purchases on that day, then 30 of those customers made a churn purchase. It then went up to around 0.4 and went even higher towards the end of the series.</p> <p>Figure 4: Note that the churn line (blue) is supposed to end early, because you want to give atleast one business quarter for identifying a churn. The thicker lines are the 30day averages, the faint ones are the daily fluctuations.</p> <p></p> <p>The change in the blue churn line (see arrow) indicates that for a daily pool of Tinder Plus users, a signficant increase in the fraction of people who churned was noticed after Tinder Gold launched.</p> <p>By examining the evolution of daily instances of repeat purchases and new purchases of Tinder Plus, we can see a transition there are well.</p> <p>Figure 5. These are simply the average daily recorded values of new purchases and repeat purchases of Tinder Plus (not per number of customers ratio). Look at the drop after Tinder Gold launched. </p> <p>We can conclude the following;</p> <ul> <li>Tinder Plus's number of churns per daily customers increased (the daily fraction) by 77% after Tinder Gold (Figure 4)</li> <li>Both Tinder Plus's daily repeat purchases and new purchases (not the ratio metric) dropped by 72% after Tinder Gold launched (Figure 5).</li> </ul> <p>As for the evolution of these metrics on Tinder Gold post launch, the follwing graph is available.</p> <p>Figure 6: Note the dates on the horizontal start at the launch of Tinder Gold. The thicker lines represent the 30 day average, and the faint lines are daily fluctuations. Notice how to the lines tends towards an equilibrium in ratios as time evolves, similar to how Tinder Plus (figure 4) was before the launch date. </p> <p>IV.C Measuring the change in Tinder's revenue per customer</p> <p>Measuring the performance of individual products such as Tinder Gold and Tinder plus are important, but how well is Tinder as a brand doing? How is it changing?</p> <p>To answer these questions, I decided to calculate a metric that determines the ratio between all the money made by Tinder products on a given day to all the people who purchased Tinder products on that same day;</p> <p></p> <p>Justification for this metric</p> <p>This metric is scallable and allows us to draw conclusions about the population we're sampling from. It gives critical insight as to the performance of the brand itsself, and it's evolution shows a comparitive contrast on the before and after effects of Tinder Gold.</p> <p>Figure 7: The thick line is the 30day average, the faint ones are the day to day values.</p> <p></p> <p>We can see quite a change in the revenue per customer after Tinder Gold's launch. Considering the conclusion previously drawn that Tinder Gold caused a change in market volume of Tinder Plus, we can use Occam's razor to conclude that Tinder Gold also caused the increase in revenue seen here (otherwise, we'd have to assume something else caused it).</p> <p>We can draw the following conclusions:</p> <ul> <li>Tinder Gold caused a 42% increase in daily revenue for all Tinder products combined (between 8/1/2017 and 9/30/2018).</li> <li>Before Tinder Gold, Tinder's CAGR (compounded annual growth rate) was 8.98%. After Tinder Gold, Tinder's CAGR was 17.2%</li> <li>Tinder Gold improved Tinder's CAGR by 91.2%</li> <li>Tinder Gold took Tinder Plus's customers, and despite a decrease in sales of Tinder Plus, the overall sales of Tinder products increased.</li> <li>Okcupid's revenue per customer was evaluated as well, but the results did not indicate significant changes from the Tinder Gold launch.</li> </ul>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/#v-anomalies-and-suprising-things-in-the-data","title":"V. Anomalies and Suprising things in the data","text":"<p>There were four anomalies in the data to be noted. One situation which is relevant to this presentation demonstrates a somewhat analogous situation to Tinder Gold's growth. The other anomalies can be found in the technical document.</p> <p>V.A An analogy to Tinder vs the rest of the market: JDate vs JSwipe</p> <p>JDate and JSwipe make for an interesting sub study on the effects that swipe based dating apps have on people. Because these apps are oriented to a smaller cultural subset, we can see an example somewhat isolated from the general population on the effects that swipe based dating apps have on the market.</p> <p>Here in figure 8 we see the market volume evolution of JDate and JSwipe across Time. Clearly this suggests a pattern - as swipe based dating apps are introduced, they go up in market volume and their direct competitors drop.</p> <p>Figure 8</p> <p></p>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/#vi-relating-the-data-to-user-experience-studies","title":"VI. Relating the data to user experience studies","text":"<p>Considering the overwhelmingly obvious data that Tinder is dominating the dating sphere, it suggests that it\u2019s providing something different from the other dating products.</p> <p>There is a bit of an irony in some user experience surveys. In one study on almost 9,761 college aged users who\u2019d been on the app for at least six months, 70% of them reported they had not actually met anyone through the app (10,11). You\u2019d think that a product would be popular because it\u2019s achieving the result it\u2019s advertised for, but in the artificial world of social media this doesn\u2019t seem to be the case.</p> <p>What does seem to be the case is that users see Tinder more as a source of entertainment, and as a source confidence boosting/social validation. In that same user experience study, 45% of users said they were doing it for confidence boosting, and only 25% said they were seeking a partner (10,11).  Moreover, in consideration of the widely known axiom that social media is highly addictive, it\u2019s pretty easy to call Tinder addictive with every swipe diminishing the user\u2019s attention span. This could partially explain the major market volume gap between Tinder and its competition.</p> <p>Contrasting against the competition, we see that steadily popular dating sites such as Match.com actually offer money back guarantees for finding romantic partners (12). Such a bold offer could only be sustained if it was backed up its results. Yet Tinder sells more, is growing faster but rarely gets people what it\u2019s advertised for(10,11).</p> <p>In reflection, one could conclude that a social media product will sell more than its competitors if it\u2019s designed to be more addictive and provides a myriad of psycho-social stimulation. Move over, it will do so even if it doesn\u2019t get the users the advertised result they had hoped for as often as a less stimulating product.</p>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/#vii-summarizing-the-conclusions","title":"VII. Summarizing the conclusions","text":"<p>As mentioned in section 1.B, the fundamental difference between Tinder Gold and Tinder Plus was the Tinder Gold allowed users to see who liked them back. This one feature made a world of difference in the dating market, because it focuses in on a pscyhological anticipation factor that users really desire.</p> <p>In all we can draw the following conclusions:</p> <ul> <li>Smart phone swipe based dating apps are dominating the dating app market.</li> <li>Tinder Gold's distinct advantage caused a significant drop in the market share of Tinder Plus</li> <li>Tinder Gold caused an increase in Tinder Plus's churns.</li> <li>Tinder Gold didn't make signficantly measurable changes in the volume/revenue of Okcupid.</li> <li>Tinder Gold caused a 42% increase in the Tinder brand's revenue per customer, and is responsible for a 17.2% CAGR</li> <li>Tinder Gold is providing a fundamentally different kind of user experience from other dating apps.</li> <li>The right feature, added at the right time, can change the entire market. Sometimes all it takes is a subtle change. In the dating app market, this particularly applies when we tap into the human needs of social validation.</li> </ul> <p>Quod Erat Demonstrandum</p>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/#viii-draw-backs-and-limitations-of-the-analysis","title":"VIII. Draw backs and Limitations of the Analysis","text":"<p>As with any analysis there are things that limit our analysis and make sensitive to errors. For this project, a few of these were;</p> <ul> <li>The conclusions reinforce each other; if one conclusion is wrong it draws into question the other conclusions.</li> <li>The source data may have been subject to a collection bias towards Tinder products.</li> <li>Bumble, a major competitor to Tinder, was not included in the raw data.</li> <li>There is a lot of misinformation on the product prices. Prices of dating products changed periodically throughout the time period studied and there isn't consistent information as to when these changes happened.</li> </ul>"},{"location":"research/2019-4-22-Tinder-Gold%3B-Big-Data-For-Big-Dating-1st-place-competition/#ix-sources","title":"IX. Sources","text":"<p>[1] Tinder\u2019s history  https://en.wikipedia.org/wiki/Tinder_(app)</p> <p>[2] \u2018Tinder Plus versus Tinder Gold\u2019: https://www.help.tinder.com/hc/en-us/articles/115004487406-Tinder-Plus-and-Tinder-Gold-</p> <p>[3] Tinder Prices on the app store:  https://itunes.apple.com/us/app/tinder/id547702041?mt=8</p> <p>[4] Okcupid Prices https://itunes.apple.com/us/app/okcupid-online-dating-app/id338701294?mt=8</p> <p>[5] Match.com prices https://itunes.apple.com/us/app/match-1-dating/id305939712?mt=8</p> <p>[6] Stationarity in time series: https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/</p> <p>[7] Cointegration of time series: http://www.eco.uc3m.es/~jgonzalo/teaching/EconometriaII/cointegration.HTM</p> <p>[8] Granger Causality  https://www.statisticshowto.datasciencecentral.com/granger-causality/</p> <p>[9] Welch\u2019s t-test:  https://en.wikipedia.org/wiki/Welch%27s_t-test</p> <p>[10] Here's the Real Reason Why Millennials Use Tinder http://money.com/money/4713971/tinder-millennials-dating-apps/</p> <p>[11] LendEdus Tinder study https://lendedu.com/blog/tinder-match-millennials/</p> <p>[12] Match.coms guarantee https://www.match.com/guarantee/rules.aspx</p> <p>[13] Augmented Dickey Fuller Test https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test</p> <p>[14] Granger Causality https://en.wikipedia.org/wiki/Granger_causality</p> <p>[15] Granger Test for Python https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.grangercausalitytests.html</p>"},{"location":"research/2022-8-2_dengu/","title":"Predicting outbreaks of Dengu Fever using Linear Count Models: Driven Data Competition","text":"<p>by Joe Ganser</p> <p></p>"},{"location":"research/2022-8-2_dengu/#abstract","title":"Abstract","text":"<p>Dengue fever is a mosquito passed viral infection that is particularly hazardous in tropical environments. Drivendata.co is a website hosting an open source data science competition to find the best model to predict the number of dengue fever cases per day. Using historical infection records in conjunction with environmental data, the competition's goal is to build a count regression model that minimizes the mean absolute error metric. The overall goal of this analysis to break apart the problem and build a basic model. Inspired by Peter Bull's work on the Dengue Fever competition, we build upon this and explore non-linear models. Further research can illicit more accurate models.</p>"},{"location":"research/2022-8-2_dengu/#i-problem-statement-goal-strategy","title":"I. Problem Statement Goal &amp; Strategy","text":"<p>Predicting the number of daily infections of a disease is a count regression problem with a time series property. This means that the numbers we're predicting can ONLY be positive and finite, and it's possible that some of the variables may evolve over time. To solve this problem, the investigation goes through answering several key questions;</p> <ul> <li>Is the time series property relevant?</li> <li>How does the predictor variables relate to each other and the target variable?</li> <li>How would a simple model of predicting the mean/median/zero each day score on our target metric, mean absolute error?</li> <li>Are there non-linear relationships between the predictor variable and the target?</li> <li>What features are important?</li> <li>How do linear models perform?</li> <li>How do NON-linear models perform?</li> </ul> <p>Answering these questions will guide use to building a relevant model. The general strategy is to test several models, find the one with the best MAE (mean absolute error) score, and use it to predict on our test set. To experiment with different models, we split the data into train and validation sets.</p>"},{"location":"research/2022-8-2_dengu/#ii-etl-extract-transform-and-load-the-data","title":"II. ETL: Extract transform and load the data","text":"<p>The dataset consists of a training dataset and a testing one, which will later be used for forecasting with the best performing model. These are downloaded from AWS S3 buckets and are small enough to be saved locally.</p> <pre><code>import pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\ntest_data_features = 'https://drivendata-prod.s3.amazonaws.com/data/44/public/dengue_features_test.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARVBOBDCYQTZTLQOS%2F20220729%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220729T181653Z&amp;X-Amz-Expires=86400&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=c2cabb56ebd41bae9aa5ed99d2ad1b04c587029a3da26b150fedf274b2ec661e'\ntraining_data_features = 'https://drivendata-prod.s3.amazonaws.com/data/44/public/dengue_features_train.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARVBOBDCYQTZTLQOS%2F20220729%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220729T181653Z&amp;X-Amz-Expires=86400&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=86638e7bbf2f4d0e35e91314a47ae3d94270b000a7255fa48a6eac87ffb3ecfb'\ntraining_data_labels = 'https://drivendata-prod.s3.amazonaws.com/data/44/public/dengue_labels_train.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIARVBOBDCYQTZTLQOS%2F20220729%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220729T181653Z&amp;X-Amz-Expires=86400&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=01891140cdf2983b4155e439da14527ccb65d4a311009564bec5e0b01d23cf9c'\ntry:\ndf_test = pd.read_csv(test_data_features)\ndf_train_features = pd.read_csv(training_data_features)\ndf_train_labels = pd.read_csv(training_data_labels)\ndf_train = df_train_features.merge(df_train_labels,on=['city','year','weekofyear'])\nexcept:\ndf_train = pd.read_csv('train.csv',index_col=0)\ndf_test = pd.read_csv('test.csv',index_col=0)\ntarget = 'total_cases'\ndf_train.head()\n</code></pre> city year weekofyear week_start_date ndvi_ne ndvi_nw ndvi_se ndvi_sw precipitation_amt_mm reanalysis_air_temp_k ... reanalysis_relative_humidity_percent reanalysis_sat_precip_amt_mm reanalysis_specific_humidity_g_per_kg reanalysis_tdtr_k station_avg_temp_c station_diur_temp_rng_c station_max_temp_c station_min_temp_c station_precip_mm total_cases 0 sj 1990 18 1990-04-30 0.122600 0.103725 0.198483 0.177617 12.42 297.572857 ... 73.365714 12.42 14.012857 2.628571 25.442857 6.900000 29.4 20.0 16.0 1 sj 1990 19 1990-05-07 0.169900 0.142175 0.162357 0.155486 22.82 298.211429 ... 77.368571 22.82 15.372857 2.371429 26.714286 6.371429 31.7 22.2 8.6 2 sj 1990 20 1990-05-14 0.032250 0.172967 0.157200 0.170843 34.54 298.781429 ... 82.052857 34.54 16.848571 2.300000 26.714286 6.485714 32.2 22.8 41.4 3 sj 1990 21 1990-05-21 0.128633 0.245067 0.227557 0.235886 15.36 298.987143 ... 80.337143 15.36 16.672857 2.428571 27.471429 6.771429 33.3 23.3 4.0 4 sj 1990 22 1990-05-28 0.196200 0.262200 0.251200 0.247340 7.52 299.518571 ... 80.460000 7.52 17.210000 3.014286 28.942857 9.371429 35.0 23.9 5.8"},{"location":"research/2022-8-2_dengu/#iii-exploratory-data-analysis-eda","title":"III. Exploratory data analysis (EDA)","text":"<p>It's important that we explore the structure of the data set to build a strategy for solving this problem. Let begin by exploring the time series of total cases, and it's histogram. The goal here is to examine relationships between predictors and the target, as well as determine how the time series evolved.</p> <p>III.A Examining the target variable; <code>total_cases</code></p> <p>Lets evaluate the time series and histogram of our target variable.</p> <pre><code>import matplotlib.pyplot as plt\ndef plot_feature_timeseries(data,feature):\ntime_series = data[['city','week_start_date',feature]]\ntime_series['week_start_date'] = pd.to_datetime(time_series['week_start_date'])\ntime_series.set_index('week_start_date',inplace=True)\nplt.figure(figsize=(18,8))\nplt.title('both cities',fontsize=20)\nplt.plot(time_series[time_series['city']=='sj'][feature],label='SJ',color='blue')\nplt.plot(time_series[time_series['city']=='iq'][feature],label='IQ',color='red')\nplt.legend(fontsize=20)\nplt.ylabel(feature,fontsize=20)\nplt.xticks(fontsize=20)\nplt.show()\nplot_feature_timeseries(df_train,'total_cases')\n</code></pre> <p></p> <p>We can see from the start that this is a cyclic time series count regression problem. It's possible that seasonal trends are at play, and it's important to determine if there is an evolution in values over time (stationarity).</p> <p>Our target variable is <code>total_cases</code>, and in any regression problem it's always good to evaluate the histogram of our target.</p> <pre><code>plt.figure(figsize=(18,9))\nplt.subplot(1,2,1)\niq_mean = round(df_train[df_train['city']=='iq']['total_cases'].mean(),2)\niq_var = round(df_train[df_train['city']=='iq']['total_cases'].var(),2)\nplt.title('Historgram of Total cases in city: iq\\n mean: {}, variance: {}'.format(iq_mean,iq_var),fontsize=20)\ndf_train[df_train['city']=='iq']['total_cases'].hist(bins=30)\nplt.subplot(1,2,2)\nsj_mean = round(df_train[df_train['city']=='sj']['total_cases'].mean(),2)\nsj_var = round(df_train[df_train['city']=='sj']['total_cases'].var(),2)\nplt.title('Histogram of Total cases in city: sj\\n mean: {}, variance: {}'.format(sj_mean,sj_var),fontsize=20)\ndf_train[df_train['city']=='sj']['total_cases'].hist(bins=30)\nplt.subplots_adjust(wspace=0.3)\nplt.show()\n</code></pre> <p></p> <p>III.B Data cleaning</p> <p>The data has some missing values, and due to the nature of being a time series across different cities, the correct approach to fill these data points would be to use the foward fill method.</p> <pre><code>iq = df_train[df_train['city']=='iq']\nsj = df_train[df_train['city']=='sj']\nsj.fillna(method='ffill', inplace=True)\niq.fillna(method='ffill', inplace=True)\ndf_train = pd.concat([sj,iq],axis=0)\ndf_train.head()\n</code></pre> <p>5 rows \u00d7 25 columns</p> city year weekofyear week_start_date ndvi_ne ndvi_nw ndvi_se ndvi_sw precipitation_amt_mm reanalysis_air_temp_k ... reanalysis_relative_humidity_percent reanalysis_sat_precip_amt_mm reanalysis_specific_humidity_g_per_kg reanalysis_tdtr_k station_avg_temp_c station_diur_temp_rng_c station_max_temp_c station_min_temp_c station_precip_mm total_cases 0 sj 1990 18 1990-04-30 0.122600 0.103725 0.198483 0.177617 12.42 297.572857 ... 73.365714 12.42 14.012857 2.628571 25.442857 6.900000 29.4 20.0 16.0 1 sj 1990 19 1990-05-07 0.169900 0.142175 0.162357 0.155486 22.82 298.211429 ... 77.368571 22.82 15.372857 2.371429 26.714286 6.371429 31.7 22.2 8.6 2 sj 1990 20 1990-05-14 0.032250 0.172967 0.157200 0.170843 34.54 298.781429 ... 82.052857 34.54 16.848571 2.300000 26.714286 6.485714 32.2 22.8 41.4 3 sj 1990 21 1990-05-21 0.128633 0.245067 0.227557 0.235886 15.36 298.987143 ... 80.337143 15.36 16.672857 2.428571 27.471429 6.771429 33.3 23.3 4.0 4 sj 1990 22 1990-05-28 0.196200 0.262200 0.251200 0.247340 7.52 299.518571 ... 80.460000 7.52 17.210000 3.014286 28.942857 9.371429 35.0 23.9 5.8 <p>III.C Time series and stationarity</p> <p>This appears to be a time series count-regression problem. In any time series it's important to determine if there is an existing trend in the time series - i.e. if it's stationary. Stationarity in time series indicates if the average is constant in time. To determine stationarity, we can use the augmented dickey fuller test. Thus we perform the augmented dickey fuller test on the time series for both cities.</p> <p>If it's determined that all columns are stationary, we can treat this as a typical count-regression problem.</p> <pre><code>from statsmodels.tsa.stattools import adfuller\ndef dickey_fuller_test(data):\ntest_statistic,p_value,lags_used,observations,critical_values,icbest = adfuller(data)\nmetric = 'test statistic: {}, p_value: {}'.format(test_statistic,p_value)\nfor key in sorted(critical_values.keys()):\nalpha = float(key.replace('%',''))/100\ncritical = float(critical_values[key])\nif test_statistic&lt;=critical and p_value&lt;=alpha:\nmetric2 = '{}:{}'.format(key,critical_values[key])\nreturn 'Stationary Series, Reject Null Hypothesis;\\n '+metric+'\\n '+metric2\nreturn 'Fail to reject null hypothesis, stationary series: '+metric\n</code></pre> <p>The data comes in different contexts: one time series for each city in the data studied. Hence we should run all the evaluations in the following contexts;</p> <ul> <li>City IQ</li> <li>City SJ</li> <li>Both cities, combined</li> </ul> <p>Now lets determine if the target, <code>total_cases</code>, is stationary.</p> <pre><code>time_series = df_train[['city','week_start_date','total_cases']]\ntime_series['week_start_date'] = pd.to_datetime(time_series['week_start_date'])\nfor city in ['sj','iq','both']:\nprint('City: {}'.format(city))\nif city=='both':\nprint(dickey_fuller_test(time_series['total_cases']))\nelse:\nprint(dickey_fuller_test(time_series[time_series['city']==city]['total_cases']))\nprint('\\n')\n</code></pre> <pre><code>City: sj\nStationary Series, Reject Null Hypothesis;\n test statistic: -6.650077901931189, p_value: 5.1473186737592894e-09\n 1%:-3.4374315551464734\n\n\nCity: iq\nStationary Series, Reject Null Hypothesis;\n test statistic: -6.085428681900057, p_value: 1.0672522948401663e-07\n 1%:-3.4431115411022146\n\n\nCity: both\nStationary Series, Reject Null Hypothesis;\n test statistic: -6.6232582356851655, p_value: 5.963285375798725e-09\n 1%:-3.434889827343955\n</code></pre> <p>Now we should run the test for stationarity across all the other features, in each city context.</p> <pre><code>features = [j for j in df_train.columns if j not in ['total_cases','week_start_date','city','year','weekofyear']]\nfor col in features:\n_ = df_train[['city','week_start_date',col]]\n_['week_start_date'] = pd.to_datetime(_['week_start_date'])\nfor city in ['sj','iq','both']:\nif city=='both':\ntest = dickey_fuller_test(_[col])\nelse:\ntest = dickey_fuller_test(_[_['city']==city][col])\nif 'Fail' in test:\nprint('Feature: {} in cities: {} is non-stationary'.format(col,city))\n</code></pre> <pre><code>Feature: station_diur_temp_rng_c in cities: both is non-stationary\n</code></pre> <p>We see the the only feature regarded as non-stationary is <code>station_diur_temp_rng_c</code>. Lets graphically see what may cause this.</p> <pre><code>plot_feature_timeseries(df_train,'station_diur_temp_rng_c')\n</code></pre> <p></p> <p>Clearly, this isn't do to a trend evolving over time, but only due to the feature's average value being different for each city. In our series of tests, we also evaluated the time series being stationary when evaluating on each city individually. Thus for all intensive purposes, we can regard this feature as infract stationary.</p> <p>III.D Correlations between features and target</p> <p>Even though this is not a gaussian based regression problem, where the errors are assumed to be normally distributed, we can still inquite on the pearson correlation between each feature and our target variable <code>total_cases</code>. A bar plot showing the correlation of each feature with the target, in the context of each city can help us.</p> <pre><code>import numpy as np\nimport seaborn as sns\nsj_corrs = {}\niq_corrs = {}\nfor col in df_train.columns:\nif col not in ['year','week_start_date','city','total_cases']:\n_ = df_train[df_train['city']=='sj'][[col,'total_cases']]\nsj = np.corrcoef(_[col],_['total_cases'])[0][1]\nsj_corrs[col]=[sj]\n__ = df_train[df_train['city']=='iq'][[col,'total_cases']]\niq = np.corrcoef(__[col],__['total_cases'])[0][1]\niq_corrs[col]=[iq]\nplt.figure(figsize=(12,12))\nplt.title('Pearson correlations for each feature with total_cases',fontsize=20)\nsns.barplot(x=0, y='index', data=pd.DataFrame(sj_corrs).transpose().reset_index(), color=\"b\",label='sj')\nsns.barplot(x=0, y='index', data=pd.DataFrame(iq_corrs).transpose().reset_index(), color=\"r\",label='iq')\nplt.legend(fontsize=20)\nplt.show()\n</code></pre> <p></p> <p>From this plot we can see that most features are weakly correlated with the target. Some features have different correlations for different cities. For example, <code>weekofyear</code> has almost no correlation in city iq, but very strong correlation with sj. Perhaps this is due to geographical positioning of the cities, and seasonal effects may be more extreme in some locations over others.</p> <p>III.E Correlations between features (heat map)</p> <pre><code>iq_features = df_train[df_train['city']=='iq'].drop(['week_start_date','year'],axis=1).corr()\nsj_features = df_train[df_train['city']=='sj'].drop(['week_start_date','year'],axis=1).corr()\nplt.figure(figsize=(10,20))\nplt.subplot(2,1,1)\nplt.title('Heat map for features in city IQ',fontsize=20)\nsns.heatmap(iq_features)\nplt.subplot(2,1,2)\nplt.title('Heat map for features in city SJ',fontsize=20)\nsns.heatmap(sj_features)\nplt.subplots_adjust(hspace=0.7)\nplt.show()\n</code></pre> <p></p> <p>A few examples;</p> <p>Correlations between different features range widely, between 0.997 (max) and -0.896 (min). Thus this may indicate colinearity between features that could be problematic for linear models.</p> <p>For city SJ; * <code>weekofyear</code> has negative correlation with <code>reanalysis_air_temp_k</code> of 0.904 * <code>reanalysis_dew_point_temp_k</code> has negative correlation with <code>reanalysis_tdtr_k</code> of -0.374</p> <p>For city IQ * <code>weekofyear</code> has negative correlation with <code>reanalysis_specific_humidity_g_per_kg</code> of 0.997 * <code>reanalysis_dew_point_temp_k</code> has positive correlation with <code>reanalysis_tdtr_kc</code> of -0.896</p> <p>III.F Establishing a baseline: performance of a constant model</p> <p>In a regression problem, one strategy to establish a baseline model is to ask the following questions;</p> <ul> <li>How accurate would our model be if we only predicted the mean?<ul> <li>Or if we only predicted median?</li> <li>Or if we only predicted zero?</li> </ul> </li> </ul> <p>What would the loss and performance metrics be for this? Using these metrics, we can conclude any model that performance worse than this is by definition no better than simply guessing the mean/median/zero for every row.</p> <p>We should answer these questions in the context of each city, and the cities combined.</p> <pre><code>from sklearn.metrics import mean_absolute_error\ndef scores_of_bad_models(df):\npredict_median = [np.median(df['total_cases']) for i in range(len(df))]\npredict_mean = [df['total_cases'].mean() for i in range(len(df))]\npredict_zero = [0 for i in range(len(df))]\npredict_mean_error = round(mean_absolute_error(df['total_cases'],predict_mean),2)\npredict_zero_error = round(mean_absolute_error(df['total_cases'],predict_zero),2)\npredict_median_error = round(mean_absolute_error(df['total_cases'],predict_median),2)\nreturn predict_mean_error,predict_zero_error,predict_median_error\ndatasets = [df_train,df_train[df_train['city']=='sj'],df_train[df_train['city']=='iq']]\nscores = {}\nfor i in range(3):\nif i==0:\nname = 'both'\nelif i==1:\nname = 'sj'\nelif i==2:\nname = 'iq'\nscores[name] = [*scores_of_bad_models(datasets[i])]\nscores = pd.DataFrame(scores).transpose()\nscores.columns = ['predict_mean','predict_zero','predict_median']\nscores['min'] = scores.apply(lambda x: x.min(),axis=1)\nscores.index.name='cities'\nscores\n</code></pre> cities predict_zero predict_median min both 23.00 24.68 19.88 sj 28.35 34.18 24.71 iq 6.68 7.57 6.05 <p>Thus for both cities combined, the predicting the median every day would yield a <code>mean_absolute_error</code> of 19.88. For city SJ it's 24.71 and IQ it's 6.05. Thus any useful model should have a mean absolute error lower than these values.</p> <p>III.G Selecting the right linear model</p> <p>This is a count regression problem, which means that at no time should we expect our model or measurements to be less than zero. Some other examples of similar problems are;</p> <ul> <li>Number of hurricanes per year</li> <li>Number of traffic accidents per month</li> <li>Customer visits to a website per day</li> </ul> <p>Hence to measure -5 negative cases of dengu fever on a given day really has no meaning. There are two linear model approaches to solving problems like this, each with different assumptions.</p> <ul> <li>Poisson Regression</li> <li>Negative binomial distribution</li> </ul> <p>Poisson regression assumes that the average count per day will equal the variance. If this assumption is invalidated by our data, negative binomial distribution will be the appropriate model to use.</p> <p>From the section above \"examining the target variable\", we know that;</p> <ul> <li> <p>City IQ:</p> <ul> <li>mean: 7.57 cases/day</li> <li>variance: 115.9</li> </ul> </li> <li> <p>City SJ:</p> <ul> <li>mean: 34.18 cases/day</li> <li>variance: 2640</li> </ul> </li> </ul> <p>Hence the variance far exceeds the mean, so Negative binomial distribution is most relevant. This is known as over dispersion.</p> <p>III.G1 Linear modeling: Predictability of each feature versus the target</p> <p>One technique in feature selection is to examine the predictability using each feature versus the target variable, using a linear model (negative binomial distribution, hyper parameters tuned). This gives us an estimate of the usefulness of each feature in predicting the target, and we can eliminate features with poor predictability. This technique is limited to cases where the relationships between variabels are linear, but does give some useful insight. This script is inspired by source [1].</p> <pre><code>from statsmodels.tools import eval_measures\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\ndef negative_binomial_one_feature(train,test,city,param_grid,feature):\nmodel_formula = \"total_cases ~ {}\".format(feature)\nbest_alpha = 0\nbest_score = 1000\nfor alpha in param_grid:\nmodel = smf.glm(formula=model_formula,data=train,family=sm.families.NegativeBinomial(alpha=alpha))\nresults = model.fit()\npredictions = results.predict(test).astype(int)\nscore = eval_measures.meanabs(predictions, test.total_cases)\nif score &lt; best_score:\nbest_alpha = alpha\nbest_score = score\nmetrics = {'city':city,'best_MAE':best_score,'alpha':best_alpha,'feature':feature}\nreturn metrics\ndef feature_tests(data,features,param_grid):\nmask = np.random.rand(len(data))&lt;0.8\ntrain = data[mask]\ntest = data[~mask]\nresults = []\ncities = ['sj','iq']\nfor feature in features:\nfor element in range(3):\ntr,te = train.copy(),test.copy()\nif element in [0,1]:\ncity = cities[element]\ntr = tr[tr['city']==city]\nte = te[te['city']==city]\nelse:\ncity = 'both'\ntr.drop('city',axis=1,inplace=True)\nte.drop('city',axis=1,inplace=True)\nresult = negative_binomial_one_feature(tr,te,city,param_grid,feature)\nresults.append(result)\nreturn pd.DataFrame(results)\nfeatures = [j for j in df_train.columns if j not in ['city','year','week_start_date',target]]\nparam_grid = np.arange(0.001,20,0.05)\nfeature_results = feature_tests(df_train,features,param_grid)\nfeature_results.head()\n</code></pre> feature city best_MAE alpha weekofyear sj 23.384181 5.701 weekofyear iq 6.741935 0.001 weekofyear both 19.374074 1.601 ndvi_ne sj 25.005650 0.001 ndvi_ne iq 6.741935 0.001 <p>Now we have a comparison of each the mean absolute error of each feature versus the target, where the linear model is fitted with an optimal hyper-parameter. From this we can look at the histogram of scores, and see how many features had a mean absolute error less than that of a constant model. Features that performed better than a constant model are probably useful, ones that didn't may not be.</p> <pre><code>plt.figure(figsize=(18,6))\nplt.subplot(1,3,1)\nplt.title('Feature scores for city: sj \\n min MAE for constant model: {}'.format(scores.loc['sj']['min']))\nfeature_results[feature_results['city']=='sj']['best_MAE'].hist(bins=len(features))\nplt.axvline(scores.loc['sj']['min'],label='median prediction error',color='red')\nplt.xlabel('MAE',fontsize=20)\nplt.ylabel('count',fontsize=20)\nplt.legend()\nplt.subplot(1,3,2)\nplt.title('Feature scores for city: iq \\n min MAE for constant model: {}'.format(scores.loc['iq']['min']))\nfeature_results[feature_results['city']=='iq']['best_MAE'].hist(bins=len(features))\nplt.axvline(scores.loc['iq']['min'],label='median prediction error',color='red')\nplt.subplot(1,3,3)\nplt.title('Feature scores for both cities combined \\n min MAE for constant model: {}'.format(scores.loc['both']['min']))\nfeature_results[feature_results['city']=='both']['best_MAE'].hist(bins=len(features))\nplt.axvline(scores.loc['both']['min'],label='median prediction error',color='red')\nplt.show()\n</code></pre> <p></p> <p>From the graphs above we can see that a negative binomial distribution on each feature versus the target usually doesn't perform any better than simply guessing mean/median/zero every day.</p> <p>Thus we can hypothesize; linear models probably have poor predictability on this data set.</p> <p>III.G2 Feature selection</p> <p>For  there are some features that had a MAE better than the minimum MAE provided by a constant model. Another hypothesis can be formed suggesting that these features may be useful and a linear model with them only may provide some good performance. Lets see what these features are.</p> <pre><code>min_scores = pd.DataFrame(scores['min']).reset_index()\nfeature_results_joined = feature_results.merge(min_scores,left_on='city',right_on='cities')[['best_MAE','cities','alpha','min','feature']]\nkey_features = feature_results_joined[feature_results_joined['best_MAE']&lt;=feature_results_joined['min']]\nkey_features_list = list(set(key_features['feature']))\nkey_features\n</code></pre> best_MAE cities alpha min feature 23.384181 sj 5.701 24.71 weekofyear 24.502825 sj 0.151 24.71 precipitation_amt_mm 24.531073 sj 0.001 24.71 reanalysis_air_temp_k 24.587571 sj 0.001 24.71 reanalysis_avg_temp_k 24.186441 sj 0.201 24.71 reanalysis_dew_point_temp_k 24.406780 sj 0.801 24.71 reanalysis_max_air_temp_k 24.451977 sj 0.001 24.71 reanalysis_relative_humidity_percent 24.502825 sj 0.151 24.71 reanalysis_sat_precip_amt_mm 24.141243 sj 0.001 24.71 reanalysis_specific_humidity_g_per_kg 24.525424 sj 0.001 24.71 station_min_temp_c 24.581921 sj 0.901 24.71 station_precip_mm 19.374074 both 1.601 19.88 weekofyear 18.822222 both 0.001 19.88 reanalysis_air_temp_k 19.803704 both 0.051 19.88 reanalysis_avg_temp_k 18.511111 both 9.701 19.88 reanalysis_min_air_temp_k 18.522222 both 1.101 19.88 reanalysis_tdtr_k 19.359259 both 0.001 19.88 station_diur_temp_rng_c 19.085185 both 1.151 19.88 station_min_temp_c <p>The data frame above provides the results where the negative binomial distribution on the feature versus <code>total_cases</code> performed better than guessing the median. These feature were;</p> <pre><code>key_features_list\n</code></pre> <pre><code>['reanalysis_max_air_temp_k',\n 'reanalysis_min_air_temp_k',\n 'reanalysis_avg_temp_k',\n 'reanalysis_air_temp_k',\n 'reanalysis_specific_humidity_g_per_kg',\n 'station_diur_temp_rng_c',\n 'reanalysis_dew_point_temp_k',\n 'station_min_temp_c',\n 'reanalysis_tdtr_k',\n 'weekofyear',\n 'station_precip_mm',\n 'precipitation_amt_mm',\n 'reanalysis_relative_humidity_percent',\n 'reanalysis_sat_precip_amt_mm']\n</code></pre>"},{"location":"research/2022-8-2_dengu/#iv-modelling","title":"IV Modelling","text":"<p>The strategy for modeling is to begin with simple linear models, evaluate their performance and then attempt non-linear ones such as random forest regression.</p> <p>IV.A Modelling: Negative Binomial distribution</p> <p>We previously established a baseline performance of what guessing the median value every day would be, and we also identified. Now lets run experiments using negative binomial distribution on;</p> <ul> <li>top features from the last step</li> <li>all the features</li> </ul> <pre><code>def negative_binomial_models(data,param_grid,*train_features,return_predictions=False):\nif 'city' in data.columns:\ndata['city']=data['city'].apply(lambda x: 1 if x=='sj' else 0)\nmask = np.random.rand(len(data))&lt;0.8\ntrain = data[mask]\ntest = data[~mask]\nmodel_formula = \"total_cases ~\"\nfor index,f in enumerate([j for j in train_features if j!='total_cases']):\nif index==0:\nmodel_formula = model_formula+\" {}\".format(f)\nelse:\nmodel_formula = model_formula+\" + {}\".format(f)\nbest_alpha = None\nbest_score = 1000\nfor alpha in param_grid:\nmodel = smf.glm(formula=model_formula,data=train,family=sm.families.NegativeBinomial(alpha=alpha))\nresults = model.fit()\npredictions = results.predict(test).astype(int)\nscore = eval_measures.meanabs(predictions, test.total_cases)\nif score &lt; best_score:\nbest_alpha = alpha\nbest_score = score\n# Step 3: refit on entire dataset\nbest_model = smf.glm(formula=model_formula,data=train,family=sm.families.NegativeBinomial(alpha=best_alpha))\nbest_results = best_model.fit()\npredictions_train = best_results.predict(train).astype(int)\npredictions_test = best_results.predict(test).astype(int)\nscore_train = eval_measures.meanabs(predictions_train, train.total_cases)\nscore_test = eval_measures.meanabs(predictions_test, test.total_cases)\nmetrics = {'MAE_test':score_train,'MAE_train':score_test,'alpha':best_alpha}\nreturn metrics\n</code></pre> <p>Tune and test negative binomial distribution on the key features observed before.</p> <pre><code>negative_binomial_models(df_train,param_grid,*key_features_list)\n</code></pre> <pre><code>{'MAE_test': 19.004198152812762,\n 'MAE_train': 18.724528301886792,\n 'alpha': 0.051000000000000004}\n</code></pre> <p>Tune and test negative binomial distribution on all the features.</p> <pre><code>all_features = [j for j in df_train.columns if j not in ['year','week_start_date',target]]\nnegative_binomial_models(df_train,param_grid,*all_features)\n</code></pre> <pre><code>{'MAE_test': 17.706837606837606,\n 'MAE_train': 21.377622377622377,\n 'alpha': 0.101}\n</code></pre> <p>From these results we can see that although the model's performance on both a train and test set indicates a reasonably good fit, but the MAE performs approximately the same as guessing the median. Thus its worth investigating a better performing model than negative binomial distribution.</p> <p>IV.B Modelling: Random Forest Regression</p> <p>Random forest regression is a good model for data sets that contain non-linearities between features. As a first experiment. It's also good to use a standard scalarizer of our data before putting it into the random forest model.</p> <pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport random\ndef preprocess_data(data):\nfor c in ['week_start_date','year']:\nif c in data.columns:\ndata.drop(c,axis=1,inplace=True)\ndata['city']=data['city'].apply(lambda x: 1 if x=='sj' else 0)\nif target in data.columns:\nX = data.drop(target,axis=1)\ny = data[target]\nelse:\nX = data\ny = None\nreturn X,y\ndef split_and_preprocess_data(data):\nX,y = preprocess_data(data)\nX_train, X_val, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)\nreturn X_train, X_val, y_train, y_val\ncriterion = [\"poisson\",\"mse\",\"mae\"]\nmax_depth = [None,20,25,30,35]\nmax_features = [\"sqrt\", \"log2\", None]\nnum_estimators = [10,50,100,150]\nbest_err = 1000\nbest_params = []\nX_train, X_val, y_train, y_val = split_and_preprocess_data(df_train.copy())\nfor c in criterion:\nfor d in max_depth:\nfor f in max_features:\nfor n in num_estimators:\nrf = RandomForestRegressor(n_estimators=n,criterion=c,max_depth=d,max_features=f)\npipe_rf = Pipeline([('scaler', StandardScaler()), ('rf',rf)])\npipe_rf.fit(X_train,y_train)\ny_pred = pipe_rf.predict(X_test)\ny_pred[y_pred==np.inf]=0\ny_pred[y_pred&lt;0]=0\nerr = mean_absolute_error(y_test,y_pred)\nif err&lt;best_err:\nbest_err = err\nif len(best_params)!=0:\nbest_params.pop()\nbest_params.append([c,d,f,n])\nprint(best_err)\nprint(best_params)\n</code></pre> <pre><code>16.54417808219178\n[['mse', 35, None, 10]]\n</code></pre> <p>IV.B1 Random Forest: Feature selection</p> <p>Our <code>MAE</code> (for this split) is 16.54. This is reasonably good and better than the negative binomial distribution. Using these hyperparameters, lets fit the model to take advance of random forest's feature selection.</p> <pre><code>c='mse'\nd=30\nf=None\nn=50\nrf = RandomForestRegressor(n_estimators=n,criterion=c,max_depth=d,max_features=f)\npipe_rf = Pipeline([('scaler', StandardScaler()), ('rf',rf)])\npipe_rf.fit(X_train,y_train)\nfor feature in zip(X_train.columns, rf.feature_importances_):\nprint(feature)\n</code></pre> <pre><code>('city', 0.00012123375843353865)\n('weekofyear', 0.0884120168882518)\n('ndvi_ne', 0.018704338094528178)\n('ndvi_nw', 0.02330064085826595)\n('ndvi_se', 0.3065804669674904)\n('ndvi_sw', 0.08197494560635799)\n('precipitation_amt_mm', 0.008394869826651807)\n('reanalysis_air_temp_k', 0.01718239611866463)\n('reanalysis_avg_temp_k', 0.019715048212381053)\n('reanalysis_dew_point_temp_k', 0.04598412742970319)\n('reanalysis_max_air_temp_k', 0.023837285355508442)\n('reanalysis_min_air_temp_k', 0.10103776364819778)\n('reanalysis_precip_amt_kg_per_m2', 0.032496353106185906)\n('reanalysis_relative_humidity_percent', 0.015229479597685027)\n('reanalysis_sat_precip_amt_mm', 0.005335078003527012)\n('reanalysis_specific_humidity_g_per_kg', 0.035410600175340604)\n('reanalysis_tdtr_k', 0.04086107229416256)\n('station_avg_temp_c', 0.02208406820316571)\n('station_diur_temp_rng_c', 0.016308194900887744)\n('station_max_temp_c', 0.04946073934079644)\n('station_min_temp_c', 0.014044667116736333)\n('station_precip_mm', 0.033524614497077845)\n</code></pre> <pre><code>X_train, X_val, y_train, y_val = split_and_preprocess_data(df_train.copy())\nrf = RandomForestRegressor(n_estimators=50,criterion='mse',max_depth=None,max_features=None)\npipe = Pipeline([('scaler', StandardScaler()), ('rf',rf)])\npipe.fit(X_train,y_train)\ny_pred_val = pipe.predict(X_val)\ny_pred_train = pipe.predict(X_train)\ny_pred_val[y_pred_val == np.inf] = 0\ny_pred_val[y_pred_val&lt;0]=0\ny_pred_train[y_pred_train == np.inf] = 0\ny_pred_train[y_pred_train&lt;0]=0\nerr_val = mean_absolute_error(y_val,y_pred_val)\nerr_train = mean_absolute_error(y_train,y_pred_train)\nprint(err_val)\nprint(err_train)\n</code></pre> <pre><code>17.56821917808219\n5.864226804123711\n</code></pre> <p>IV.B2 Random forest results * MAE validation set: 17.56 * MAE train set: 5.86</p> <p>Despite the model being overfit, lets plot this and see how the predictions compare for the train and validation sets.</p> <pre><code>val = pd.concat([X_val,y_val],axis=1)\ntrain = pd.concat([X_train,y_train],axis=1)\nval['predictions'] = y_pred_val\ntrain['predictions'] = y_pred_train\nindices = df_train[['week_start_date','total_cases','weekofyear','year','city']]\nval_result = val.join(indices,on=None,lsuffix='Left')[['total_cases','predictions','week_start_date','city']]#.set_index('week_start_date')\nval_result.sort_values(by='week_start_date',inplace=True)\nval_result['data_set'] = 'validation'\ntrain_result = train.join(indices,on=None,lsuffix='Left')[['total_cases','predictions','week_start_date','city']]\ntrain_result.sort_values(by='week_start_date',inplace=True)\ntrain_result['data_set'] = 'train'\npredictions = pd.concat([val_result,train_result],axis=0)\npredictions['week_start_date'] = pd.to_datetime(predictions['week_start_date'])\npredictions = predictions.sort_values(by='week_start_date').set_index('week_start_date')\ndef plot_results(df,city,label):\nplt.title('city: {}, {} set'.format(city,label))\nplt.plot(df[(df['city']==city)&amp;(df['data_set']==label)]['predictions'],label='predictions',color='blue')\nplt.plot(df[(df['city']==city)&amp;(df['data_set']==label)]['total_cases'],label='total_cases',color='red')\nplt.xticks(rotation=45)\nplt.legend()\nplt.figure(figsize=(20,20))\nplt.subplot(2,2,1)\nplot_results(predictions,'sj','train')\nplt.subplot(2,2,2)\nplot_results(predictions,'iq','train')\nplt.subplot(2,2,3)\nplot_results(predictions,'sj','validation')\nplt.subplot(2,2,4)\nplot_results(predictions,'iq','validation')\nplt.subplots_adjust(wspace=0.5,hspace=0.15)\nprint(\"Random Forest results, train and validation set\")\nplt.show()\n</code></pre> <pre><code>Random Forest results, train and validation set\n</code></pre> <p></p>"},{"location":"research/2022-8-2_dengu/#v-forecasting-on-the-test-data","title":"V: Forecasting on the test data","text":"<p>Finally, we use the hyperparameters from the best performing random forest reressor model to make forecasts on the test set.</p> <pre><code>X,y= preprocess_data(df_train.copy())\niq_test = df_test[df_test['city']=='iq'].copy()\nsj_test = df_test[df_test['city']=='sj'].copy()\nsj_test.fillna(method='ffill', inplace=True)\niq_test.fillna(method='ffill', inplace=True)\ndf_test = pd.concat([sj_test,iq_test],axis=0)\nX_test = df_test.copy()\nX_test.drop(['week_start_date','year'],axis=1,inplace=True)\nX_test['city'] = X_test['city'].apply(lambda x: 1 if x=='sj' else 0)\nrf = RandomForestRegressor(n_estimators=50,criterion='mse',max_depth=None,max_features=None)\npipe_final = Pipeline([('scaler', StandardScaler()), ('rf',rf)])\npipe_final.fit(X,y)\ny_pred_test = pipe_final.predict(X_test).astype(int)\nsubmission = pd.concat([df_test[['city','year','weekofyear']],pd.Series(y_pred_test)],axis=1)\nsubmission.columns = ['city','year','weekofyear','total_cases']\nsubmission.set_index(['city','year','weekofyear'],inplace=True)\nsubmission.to_csv('submission.csv')\n</code></pre>"},{"location":"research/2022-8-2_dengu/#vi-discussion-further-steps","title":"VI: Discussion &amp; Further steps","text":"<p>We can draw a few conclusions about the data from the investigation; * The time series of all data is stationary. * There are non-linear relationships between the predictors and between the predictors and the target.     * Collinearity is present between predictors * Linear models have poor performance * Ensemble tree techniques have issues with over fitting the data.</p> <p>Further Steps</p> <p>With more time, computational ability (this was done on a rather old macbook), more advanced modelling techniques can be integrated. The next step I would use would be neural networks.</p>"},{"location":"research/2022-8-2_dengu/#vii-sources","title":"VII: Sources","text":"<ul> <li> <ol> <li>https://www.drivendata.co/blog/dengue-benchmark/</li> </ol> </li> <li> <ol> <li>https://timeseriesreasoning.com/contents/poisson-regression-model/</li> </ol> </li> <li> <ol> <li>https://timeseriesreasoning.com/contents/negative-binomial-regression-model/</li> </ol> </li> <li> <ol> <li>http://www.southsudanmedicaljournal.com/archive/february-2012/dengue-fever.html</li> </ol> </li> </ul>"}]}